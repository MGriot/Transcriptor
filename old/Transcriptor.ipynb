{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the Hugging Face token from config.py\n",
    "try:\n",
    "    from config import HF_TOKEN\n",
    "except ImportError:\n",
    "    HF_TOKEN = None\n",
    "    logging.warning(\n",
    "        \"config.py not found or HF_TOKEN not defined. Diarization may not work.\"\n",
    "        \" Please create a config.py file with HF_TOKEN = 'YOUR_HUGGINGFACE_TOKEN'\"\n",
    "    )\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def diarize_audio(\n",
    "    audio_file_path: str,\n",
    "    max_speakers: Optional[int] = None,\n",
    "    min_speakers: Optional[int] = None,\n",
    ") -> Tuple[Pipeline, Dict]:\n",
    "    \"\"\"\n",
    "    Performs speaker diarization on an audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        max_speakers (int, optional): Maximum number of speakers. Defaults to None.\n",
    "        min_speakers (int, optional): Minimum number of speakers. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the diarization pipeline and the diarization output.\n",
    "    \"\"\"\n",
    "    # Check if a GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=HF_TOKEN,\n",
    "    )\n",
    "    pipeline.to(device)  # Move the pipeline to the selected device\n",
    "\n",
    "    # Load the audio file\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading audio file: {e}\")\n",
    "        raise  # Re-raise the exception to be handled by the caller\n",
    "\n",
    "    # Prepare the input for the pipeline\n",
    "    input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "\n",
    "    # Add speaker number hints if provided\n",
    "    if max_speakers is not None:\n",
    "        input_data[\"max_speakers\"] = max_speakers\n",
    "    if min_speakers is not None:\n",
    "        input_data[\"min_speakers\"] = min_speakers\n",
    "\n",
    "    # Run the diarization pipeline with the progress hook\n",
    "    with ProgressHook() as hook:\n",
    "        try:\n",
    "            diarization = pipeline(input_data, hook=hook)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during diarization: {e}\")\n",
    "            raise  # Re-raise the exception\n",
    "\n",
    "    logging.info(\"Diarization complete.\")\n",
    "    return pipeline, diarization  # Return both pipeline and diarization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: \n",
       "UserWarning: std(): degrees of freedom is &lt;= 0. Correction should be strictly less than the reduction factor (input\n",
       "numel divided by output numel). (Triggered internally at \n",
       "C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
       "  std = sequences.std(dim=-1, correction=1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: \n",
       "UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input\n",
       "numel divided by output numel). (Triggered internally at \n",
       "C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
       "  std = sequences.std(dim=-1, correction=1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Diarization complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker SPEAKER_00: 0.03 - 10.00\n",
      "Speaker SPEAKER_01: 11.42 - 12.82\n",
      "Speaker SPEAKER_00: 12.82 - 14.91\n",
      "Speaker SPEAKER_00: 16.94 - 25.63\n",
      "Speaker SPEAKER_01: 25.58 - 27.37\n",
      "Speaker SPEAKER_00: 27.37 - 28.97\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your audio file\n",
    "audio_file_path = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "\n",
    "# Optional: Specify the maximum and minimum number of speakers\n",
    "max_speakers = 2\n",
    "min_speakers = 1\n",
    "\n",
    "# Call the diarize_audio function\n",
    "pipeline, diarization = diarize_audio(audio_file_path, max_speakers, min_speakers)\n",
    "\n",
    "# Print the diarization result\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"Speaker {speaker}: {turn.start:.2f} - {turn.end:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from typing import Optional, Dict, Tuple\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Segment\n",
    "\n",
    "try:\n",
    "    from pydub import AudioSegment\n",
    "except ImportError:\n",
    "    print(\"pydub is not installed. Please install it using: pip install pydub\")\n",
    "    AudioSegment = None\n",
    "\n",
    "try:\n",
    "    import whisper_timestamped as whisper\n",
    "except ImportError:\n",
    "    print(\n",
    "        \"whisper-timestamped is not installed. Please install it using: pip install whisper-timestamped\"\n",
    "    )\n",
    "    whisper = None\n",
    "\n",
    "\n",
    "def diarize_audio(\n",
    "    audio_file_path: str,\n",
    "    max_speakers: Optional[int] = None,\n",
    "    min_speakers: Optional[int] = None,\n",
    ") -> Tuple[Pipeline, Dict]:\n",
    "    \"\"\"\n",
    "    Performs speaker diarization on an audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        max_speakers (int, optional): Maximum number of speakers. Defaults to None.\n",
    "        min_speakers (int, optional): Minimum number of speakers. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the diarization pipeline and the diarization output.\n",
    "    \"\"\"\n",
    "    import logging\n",
    "\n",
    "    # Check if a GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=\"YOUR_HUGGINGFACE_TOKEN\",  # Replace with your token\n",
    "    )\n",
    "    pipeline.to(device)  # Move the pipeline to the selected device\n",
    "\n",
    "    # Load the audio file\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading audio file: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Prepare the input for the pipeline\n",
    "    input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "\n",
    "    # Add speaker number hints if provided\n",
    "    if max_speakers is not None:\n",
    "        input_data[\"max_speakers\"] = max_speakers\n",
    "    if min_speakers is not None:\n",
    "        input_data[\"min_speakers\"] = min_speakers\n",
    "\n",
    "    # Run the diarization pipeline\n",
    "    try:\n",
    "        diarization = pipeline(input_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during diarization: {e}\")\n",
    "        raise\n",
    "\n",
    "    logging.info(\"Diarization complete.\")\n",
    "    return pipeline, diarization\n",
    "\n",
    "\n",
    "def chunk_audio(\n",
    "    audio_file_path: str, diarization: Dict, output_directory: str\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Chunks an audio file into segments based on the diarization output, saves them,\n",
    "    and returns a list of chunk information.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        diarization (Dict): Diarization output from the pyannote pipeline.\n",
    "        output_directory (str): Directory to save the chunked audio files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list where each element is a dictionary containing:\n",
    "            - file_path (str): Path to the saved audio chunk.\n",
    "            - speaker (str): Speaker label.\n",
    "            - start_time (float): Start time of the chunk in seconds.\n",
    "            - end_time (float): End time of the chunk in seconds.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Load the audio file using pydub\n",
    "    if AudioSegment is None:\n",
    "        print(\"Skipping audio chunking because pydub is not installed.\")\n",
    "        return []\n",
    "\n",
    "    audio = AudioSegment.from_file(audio_file_path)\n",
    "\n",
    "    chunk_info_list = []\n",
    "    chunk_number = 0  # Initialize chunk counter\n",
    "\n",
    "    # Iterate over each turn in the diarization output\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        start_time = turn.start\n",
    "        end_time = turn.end\n",
    "        start_time_ms = int(start_time * 1000)  # Convert seconds to milliseconds\n",
    "        end_time_ms = int(end_time * 1000)  # Convert seconds to milliseconds\n",
    "\n",
    "        chunk = audio[start_time_ms:end_time_ms]\n",
    "        chunk_number += 1  # Increment chunk counter for filename\n",
    "        output_file_path = os.path.join(\n",
    "            output_directory,\n",
    "            f\"chunk_{chunk_number}.mp3\",  # Use chunk number in filename\n",
    "        )\n",
    "        chunk.export(output_file_path, format=\"mp3\")\n",
    "\n",
    "        chunk_info = {\n",
    "            \"file_path\": output_file_path,\n",
    "            \"speaker\": speaker,\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "        }\n",
    "        chunk_info_list.append(chunk_info)\n",
    "\n",
    "    return chunk_info_list\n",
    "\n",
    "\n",
    "def transcribe_audio_chunk(audio_file_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Transcribes an audio file using whisper-timestamped.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        Dict: The transcription with word-level timestamps.  Returns empty dict on error.\n",
    "    \"\"\"\n",
    "    if whisper is None:\n",
    "        print(\"whisper-timestamped is not installed. Transcription is skipped.\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        # Load audio and model\n",
    "        model = whisper.load_model(\"base\")  # You can change the model size if needed\n",
    "        audio = whisper.load_audio(audio_file_path)\n",
    "\n",
    "        # Transcribe the audio file\n",
    "        result = whisper.transcribe(\n",
    "            model, audio, language=\"en\"\n",
    "        )  # You can change the language\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during transcription: {e}\")\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return {}  # Return empty dict on error\n",
    "\n",
    "\n",
    "def process_and_transcribe_chunks(chunk_info_list: List[Dict], output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Transcribes audio chunks using whisper-timestamped and saves the transcriptions\n",
    "    to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        chunk_info_list (List[Dict]): A list of dictionaries, where each dictionary\n",
    "            contains chunk information as returned by the chunk_audio function.\n",
    "        output_dir (str): The directory where the JSON file should be saved.\n",
    "    \"\"\"\n",
    "    if not chunk_info_list:\n",
    "        print(\"No chunks to transcribe.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    transcriptions = []\n",
    "    for chunk_info in chunk_info_list:\n",
    "        chunk_file_path = chunk_info[\"file_path\"]\n",
    "        speaker = chunk_info[\"speaker\"]\n",
    "        start_time = chunk_info[\"start_time\"]\n",
    "        end_time = chunk_info[\"end_time\"]\n",
    "\n",
    "        print(\n",
    "            f\"Transcribing chunk: {chunk_file_path}, Speaker: {speaker}, Start: {start_time:.2f}, End: {end_time:.2f}\"\n",
    "        )\n",
    "        transcription = transcribe_audio_chunk(chunk_file_path)  # returns {} on error\n",
    "\n",
    "        if transcription:  # only add if transcription was successful\n",
    "            transcription_data = {\n",
    "                \"speaker\": speaker,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"transcription\": transcription,\n",
    "            }\n",
    "            transcriptions.append(transcription_data)\n",
    "\n",
    "            # Print the transcription\n",
    "            print(f\"Transcription for {chunk_file_path}:\")\n",
    "            for segment in transcription[\"segments\"]:\n",
    "                print(\n",
    "                    f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\"\n",
    "                )\n",
    "\n",
    "    # Save all transcriptions to a JSON file\n",
    "    output_json_path = os.path.join(output_dir, \"transcriptions.json\")\n",
    "    try:\n",
    "        with open(output_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        print(f\"Transcriptions saved to {output_json_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving transcriptions to JSON: {e}\")\n",
    "        print(f\"Error saving transcriptions to JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using device: cpu\n",
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "INFO:root:Diarization complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "Transcribing chunk: chunks\\chunk_1.mp3, Speaker: SPEAKER_00, Start: 0.03, End: 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:04<00:00, 225.82frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_1.mp3:\n",
      "    [0.00 - 1.58]  Anyway, look where we're digressing the rules.\n",
      "    [1.68 - 4.32]  Oh, simple, Emma, you're about to face five questions\n",
      "    [4.32 - 5.30]  of increasing difficulty.\n",
      "    [5.32 - 6.60]  You must answer as quickly as possible.\n",
      "    [6.62 - 8.32]  If you get it correct, you move onto the next round.\n",
      "    [8.58 - 9.76]  Do you know what happens if you get it wrong?\n",
      "Transcribing chunk: chunks\\chunk_2.mp3, Speaker: SPEAKER_01, Start: 11.42, End: 12.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:01<00:00, 121.47frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_2.mp3:\n",
      "    [0.10 - 1.14]  and correction and embarrassment.\n",
      "Transcribing chunk: chunks\\chunk_3.mp3, Speaker: SPEAKER_00, Start: 12.82, End: 14.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:01<00:00, 140.74frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_3.mp3:\n",
      "    [0.00 - 1.86]  Do indeed round one.\n",
      "Transcribing chunk: chunks\\chunk_4.mp3, Speaker: SPEAKER_00, Start: 16.94, End: 25.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:02<00:00, 366.74frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_4.mp3:\n",
      "    [0.08 - 4.48]  Round 1 astronomers are saying that Saturn's rings are slowly disappearing.\n",
      "    [4.76 - 8.70]  They estimate we only have a few hundred million years left of them.\n",
      "Transcribing chunk: chunks\\chunk_5.mp3, Speaker: SPEAKER_01, Start: 25.58, End: 27.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 115.73frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_5.mp3:\n",
      "    [0.06 - 1.52]  I'll earn you a few hundred million.\n",
      "Transcribing chunk: chunks\\chunk_6.mp3, Speaker: SPEAKER_00, Start: 27.37, End: 28.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:01<00:00, 98.25frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_6.mp3:\n",
      "    [0.16 - 1.08]  But what I want to know?\n",
      "Transcriptions saved to chunks\\transcriptions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "audio_file = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "output_dir = \"chunks\"\n",
    "# Make sure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    pipeline, diarization_result = diarize_audio(audio_file)\n",
    "    chunk_info_list = chunk_audio(audio_file, diarization_result, output_dir)\n",
    "    print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "    process_and_transcribe_chunks(\n",
    "        chunk_info_list, output_dir\n",
    "    )  # added call to new function\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from typing import Optional, Dict, Tuple\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Segment\n",
    "\n",
    "try:\n",
    "    from pydub import AudioSegment\n",
    "except ImportError:\n",
    "    print(\"pydub is not installed. Please install it using: pip install pydub\")\n",
    "    AudioSegment = None\n",
    "\n",
    "try:\n",
    "    import whisper_timestamped as whisper\n",
    "except ImportError:\n",
    "    print(\n",
    "        \"whisper-timestamped is not installed. Please install it using: pip install whisper-timestamped\"\n",
    "    )\n",
    "    whisper = None\n",
    "\n",
    "\n",
    "def diarize_audio(\n",
    "    audio_file_path: str,\n",
    "    max_speakers: Optional[int] = None,\n",
    "    min_speakers: Optional[int] = None,\n",
    ") -> Tuple[Pipeline, Dict]:\n",
    "    \"\"\"\n",
    "    Performs speaker diarization on an audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        max_speakers (int, optional): Maximum number of speakers. Defaults to None.\n",
    "        min_speakers (int, optional): Minimum number of speakers. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the diarization pipeline and the diarization output.\n",
    "    \"\"\"\n",
    "    import logging\n",
    "\n",
    "    # Check if a GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=\"YOUR_HUGGINGFACE_TOKEN\",  # Replace with your token\n",
    "    )\n",
    "    pipeline.to(device)  # Move the pipeline to the selected device\n",
    "\n",
    "    # Load the audio file\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading audio file: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Prepare the input for the pipeline\n",
    "    input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "\n",
    "    # Add speaker number hints if provided\n",
    "    if max_speakers is not None:\n",
    "        input_data[\"max_speakers\"] = max_speakers\n",
    "    if min_speakers is not None:\n",
    "        input_data[\"min_speakers\"] = min_speakers\n",
    "\n",
    "    # Run the diarization pipeline\n",
    "    try:\n",
    "        diarization = pipeline(input_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during diarization: {e}\")\n",
    "        raise\n",
    "\n",
    "    logging.info(\"Diarization complete.\")\n",
    "    return pipeline, diarization\n",
    "\n",
    "\n",
    "def chunk_audio(\n",
    "    audio_file_path: str, diarization: Dict, output_directory: str\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Chunks an audio file into segments based on the diarization output, saves them,\n",
    "    and returns a list of chunk information.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        diarization (Dict): Diarization output from the pyannote pipeline.\n",
    "        output_directory (str): Directory to save the chunked audio files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list where each element is a dictionary containing:\n",
    "            - file_path (str): Path to the saved audio chunk.\n",
    "            - speaker (str): Speaker label.\n",
    "            - start_time (float): Start time of the chunk in seconds.\n",
    "            - end_time (float): End time of the chunk in seconds.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Load the audio file using pydub\n",
    "    if AudioSegment is None:\n",
    "        print(\"Skipping audio chunking because pydub is not installed.\")\n",
    "        return []\n",
    "\n",
    "    audio = AudioSegment.from_file(audio_file_path)\n",
    "\n",
    "    chunk_info_list = []\n",
    "    chunk_number = 0  # Initialize chunk counter\n",
    "\n",
    "    # Iterate over each turn in the diarization output\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        start_time = turn.start\n",
    "        end_time = turn.end\n",
    "        start_time_ms = int(start_time * 1000)  # Convert seconds to milliseconds\n",
    "        end_time_ms = int(end_time * 1000)  # Convert seconds to milliseconds\n",
    "\n",
    "        chunk = audio[start_time_ms:end_time_ms]\n",
    "        chunk_number += 1  # Increment chunk counter for filename\n",
    "        output_file_path = os.path.join(\n",
    "            output_directory,\n",
    "            f\"chunk_{chunk_number}.mp3\",  # Use chunk number in filename\n",
    "        )\n",
    "        chunk.export(output_file_path, format=\"mp3\")\n",
    "\n",
    "        chunk_info = {\n",
    "            \"file_path\": output_file_path,\n",
    "            \"speaker\": speaker,\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "        }\n",
    "        chunk_info_list.append(chunk_info)\n",
    "\n",
    "    return chunk_info_list\n",
    "\n",
    "\n",
    "def transcribe_audio_chunk(audio_file_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Transcribes an audio file using whisper-timestamped.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        Dict: The transcription with word-level timestamps.  Returns empty dict on error.\n",
    "    \"\"\"\n",
    "    if whisper is None:\n",
    "        print(\"whisper-timestamped is not installed. Transcription is skipped.\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        # Load audio and model\n",
    "        model = whisper.load_model(\"base\")  # You can change the model size if needed\n",
    "        audio = whisper.load_audio(audio_file_path)\n",
    "\n",
    "        # Transcribe the audio file\n",
    "        result = whisper.transcribe(\n",
    "            model, audio, language=\"en\"\n",
    "        )  # You can change the language\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during transcription: {e}\")\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return {}  # Return empty dict on error\n",
    "\n",
    "\n",
    "def process_and_transcribe_chunks(chunk_info_list: List[Dict], output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Transcribes audio chunks using whisper-timestamped and saves the transcriptions\n",
    "    to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        chunk_info_list (List[Dict]): A list of dictionaries, where each dictionary\n",
    "            contains chunk information as returned by the chunk_audio function.\n",
    "        output_dir (str): The directory where the JSON file should be saved.\n",
    "    \"\"\"\n",
    "    if not chunk_info_list:\n",
    "        print(\"No chunks to transcribe.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    transcriptions = []\n",
    "    for chunk_info in chunk_info_list:\n",
    "        chunk_file_path = chunk_info[\"file_path\"]\n",
    "        speaker = chunk_info[\"speaker\"]\n",
    "        start_time = chunk_info[\"start_time\"]\n",
    "        end_time = chunk_info[\"end_time\"]\n",
    "\n",
    "        print(\n",
    "            f\"Transcribing chunk: {chunk_file_path}, Speaker: {speaker}, Start: {start_time:.2f}, End: {end_time:.2f}\"\n",
    "        )\n",
    "        transcription = transcribe_audio_chunk(chunk_file_path)  # returns {} on error\n",
    "\n",
    "        if transcription:  # only add if transcription was successful\n",
    "            transcription_data = {\n",
    "                \"speaker\": speaker,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"transcription\": transcription,\n",
    "            }\n",
    "            transcriptions.append(transcription_data)\n",
    "\n",
    "            # Print the transcription\n",
    "            print(f\"Transcription for {chunk_file_path}:\")\n",
    "            for segment in transcription[\"segments\"]:\n",
    "                print(\n",
    "                    f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\"\n",
    "                )\n",
    "\n",
    "    # Save all transcriptions to a JSON file\n",
    "    output_json_path = os.path.join(output_dir, \"transcriptions.json\")\n",
    "    try:\n",
    "        with open(output_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        print(f\"Transcriptions saved to {output_json_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving transcriptions to JSON: {e}\")\n",
    "        print(f\"Error saving transcriptions to JSON: {e}\")\n",
    "\n",
    "\n",
    "def clean_transcription(transcriptions_json_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Cleans the transcription data from a JSON file, merging text from the same speaker\n",
    "    into paragraphs.  The output format is:  \"SpeakerName [start_time - end_time]: text\"\n",
    "\n",
    "    Args:\n",
    "        transcriptions_json_path (str): Path to the JSON file containing the transcription data\n",
    "            generated by process_and_transcribe_chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings, where each string represents a paragraph\n",
    "        in the format \"SpeakerName [start_time - end_time]: text\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(transcriptions_json_path, \"r\") as f:\n",
    "            transcriptions = json.load(f)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading transcriptions JSON: {e}\")\n",
    "        print(f\"Error loading transcriptions JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "    cleaned_transcriptions = []\n",
    "    current_speaker = None\n",
    "    current_text = \"\"\n",
    "    current_start_time = None\n",
    "    current_end_time = None\n",
    "\n",
    "    for chunk in transcriptions:\n",
    "        speaker = chunk[\"speaker\"]\n",
    "        transcription = chunk[\"transcription\"]\n",
    "        chunk_start_time = chunk[\"start_time\"]  # Use chunk start/end times\n",
    "        chunk_end_time = chunk[\"end_time\"]\n",
    "\n",
    "        if not transcription or not transcription[\"segments\"]:\n",
    "            continue  # Skip empty transcriptions\n",
    "\n",
    "        for segment in transcription[\"segments\"]:\n",
    "            text = segment[\"text\"]\n",
    "\n",
    "            if current_speaker != speaker:\n",
    "                if current_speaker is not None:\n",
    "                    cleaned_transcriptions.append(\n",
    "                        f\"{current_speaker} [{current_start_time:.2f} - {current_end_time:.2f}]: {current_text}\"\n",
    "                    )\n",
    "                current_speaker = speaker\n",
    "                current_text = text\n",
    "                current_start_time = chunk_start_time  # from chunk\n",
    "                current_end_time = chunk_end_time  # from chunk\n",
    "            else:\n",
    "                current_text += \" \" + text\n",
    "                current_end_time = chunk_end_time  # from chunk\n",
    "\n",
    "    # Add the last paragraph\n",
    "    if current_speaker is not None:\n",
    "        cleaned_transcriptions.append(\n",
    "            f\"{current_speaker} [{current_start_time:.2f} - {current_end_time:.2f}]: {current_text}\"\n",
    "        )\n",
    "\n",
    "    return cleaned_transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using device: cpu\n",
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "INFO:root:Diarization complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "Transcribing chunk: chunks\\chunk_1.mp3, Speaker: SPEAKER_00, Start: 0.03, End: 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:03<00:00, 272.55frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_1.mp3:\n",
      "    [0.00 - 1.58]  Anyway, look where we're digressing the rules.\n",
      "    [1.68 - 4.32]  Oh, simple, Emma, you're about to face five questions\n",
      "    [4.32 - 5.30]  of increasing difficulty.\n",
      "    [5.32 - 6.60]  You must answer as quickly as possible.\n",
      "    [6.62 - 8.32]  If you get it correct, you move onto the next round.\n",
      "    [8.58 - 9.76]  Do you know what happens if you get it wrong?\n",
      "Transcribing chunk: chunks\\chunk_2.mp3, Speaker: SPEAKER_01, Start: 11.42, End: 12.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:01<00:00, 79.05frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_2.mp3:\n",
      "    [0.10 - 1.14]  and correction and embarrassment.\n",
      "Transcribing chunk: chunks\\chunk_3.mp3, Speaker: SPEAKER_00, Start: 12.82, End: 14.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:01<00:00, 155.39frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_3.mp3:\n",
      "    [0.00 - 1.86]  Do indeed round one.\n",
      "Transcribing chunk: chunks\\chunk_4.mp3, Speaker: SPEAKER_00, Start: 16.94, End: 25.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:01<00:00, 466.88frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_4.mp3:\n",
      "    [0.08 - 4.48]  Round 1 astronomers are saying that Saturn's rings are slowly disappearing.\n",
      "    [4.76 - 8.70]  They estimate we only have a few hundred million years left of them.\n",
      "Transcribing chunk: chunks\\chunk_5.mp3, Speaker: SPEAKER_01, Start: 25.58, End: 27.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 124.91frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_5.mp3:\n",
      "    [0.06 - 1.52]  I'll earn you a few hundred million.\n",
      "Transcribing chunk: chunks\\chunk_6.mp3, Speaker: SPEAKER_00, Start: 27.37, End: 28.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:01<00:00, 132.10frames/s]\n",
      "ERROR:root:An error occurred: string indices must be integers, not 'str'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_6.mp3:\n",
      "    [0.16 - 1.08]  But what I want to know?\n",
      "Transcriptions saved to chunks\\transcriptions.json\n",
      "\n",
      "Cleaned Transcriptions:\n",
      "An error occurred: string indices must be integers, not 'str'\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "audio_file = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "output_dir = \"chunks\"\n",
    "# Make sure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    pipeline, diarization_result = diarize_audio(audio_file)\n",
    "    chunk_info_list = chunk_audio(audio_file, diarization_result, output_dir)\n",
    "    print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "    process_and_transcribe_chunks(chunk_info_list, output_dir) # added call to new function\n",
    "\n",
    "    transcriptions_json = os.path.join(output_dir, \"transcriptions.json\")\n",
    "    cleaned_transcriptions = clean_transcription(transcriptions_json)\n",
    "\n",
    "    print(\"\\nCleaned Transcriptions:\")\n",
    "    for paragraph in cleaned_transcriptions:\n",
    "        print(f\"Speaker: {paragraph['speaker']}\")\n",
    "        print(f\"Time: {paragraph['start_time']:.2f} - {paragraph['end_time']:.2f}\")\n",
    "        print(f\"Text: {paragraph['text']}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from typing import Optional, Dict, Tuple\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Segment\n",
    "\n",
    "try:\n",
    "    from pydub import AudioSegment\n",
    "except ImportError:\n",
    "    print(\"pydub is not installed. Please install it using: pip install pydub\")\n",
    "    AudioSegment = None\n",
    "\n",
    "try:\n",
    "    import whisper_timestamped as whisper\n",
    "except ImportError:\n",
    "    print(\n",
    "        \"whisper-timestamped is not installed. Please install it using: pip install whisper-timestamped\"\n",
    "    )\n",
    "    whisper = None\n",
    "\n",
    "\n",
    "def diarize_audio(\n",
    "    audio_file_path: str,\n",
    "    max_speakers: Optional[int] = None,\n",
    "    min_speakers: Optional[int] = None,\n",
    ") -> Tuple[Pipeline, Dict]:\n",
    "    \"\"\"\n",
    "    Performs speaker diarization on an audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        max_speakers (int, optional): Maximum number of speakers. Defaults to None.\n",
    "        min_speakers (int, optional): Minimum number of speakers. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the diarization pipeline and the diarization output.\n",
    "    \"\"\"\n",
    "    import logging\n",
    "\n",
    "    # Check if a GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=\"YOUR_HUGGINGFACE_TOKEN\",  # Replace with your token\n",
    "    )\n",
    "    pipeline.to(device)  # Move the pipeline to the selected device\n",
    "\n",
    "    # Load the audio file\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading audio file: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Prepare the input for the pipeline\n",
    "    input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "\n",
    "    # Add speaker number hints if provided\n",
    "    if max_speakers is not None:\n",
    "        input_data[\"max_speakers\"] = max_speakers\n",
    "    if min_speakers is not None:\n",
    "        input_data[\"min_speakers\"] = min_speakers\n",
    "\n",
    "    # Run the diarization pipeline\n",
    "    try:\n",
    "        diarization = pipeline(input_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during diarization: {e}\")\n",
    "        raise\n",
    "\n",
    "    logging.info(\"Diarization complete.\")\n",
    "    return pipeline, diarization\n",
    "\n",
    "\n",
    "def chunk_audio(\n",
    "    audio_file_path: str, diarization: Dict, output_directory: str\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Chunks an audio file into segments based on the diarization output, saves them,\n",
    "    and returns a list of chunk information.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        diarization (Dict): Diarization output from the pyannote pipeline.\n",
    "        output_directory (str): Directory to save the chunked audio files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list where each element is a dictionary containing:\n",
    "            - file_path (str): Path to the saved audio chunk.\n",
    "            - speaker (str): Speaker label.\n",
    "            - start_time (float): Start time of the chunk in seconds.\n",
    "            - end_time (float): End time of the chunk in seconds.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Load the audio file using pydub\n",
    "    if AudioSegment is None:\n",
    "        print(\"Skipping audio chunking because pydub is not installed.\")\n",
    "        return []\n",
    "\n",
    "    audio = AudioSegment.from_file(audio_file_path)\n",
    "\n",
    "    chunk_info_list = []\n",
    "    chunk_number = 0  # Initialize chunk counter\n",
    "\n",
    "    # Iterate over each turn in the diarization output\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        start_time = turn.start\n",
    "        end_time = turn.end\n",
    "        start_time_ms = int(start_time * 1000)  # Convert seconds to milliseconds\n",
    "        end_time_ms = int(end_time * 1000)  # Convert seconds to milliseconds\n",
    "\n",
    "        chunk = audio[start_time_ms:end_time_ms]\n",
    "        chunk_number += 1  # Increment chunk counter for filename\n",
    "        output_file_path = os.path.join(\n",
    "            output_directory,\n",
    "            f\"chunk_{chunk_number}.mp3\",  # Use chunk number in filename\n",
    "        )\n",
    "        chunk.export(output_file_path, format=\"mp3\")\n",
    "\n",
    "        chunk_info = {\n",
    "            \"file_path\": output_file_path,\n",
    "            \"speaker\": speaker,\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "        }\n",
    "        chunk_info_list.append(chunk_info)\n",
    "\n",
    "    return chunk_info_list\n",
    "\n",
    "\n",
    "def transcribe_audio_chunk(audio_file_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Transcribes an audio file using whisper-timestamped.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        Dict: The transcription with word-level timestamps.  Returns empty dict on error.\n",
    "    \"\"\"\n",
    "    if whisper is None:\n",
    "        print(\"whisper-timestamped is not installed. Transcription is skipped.\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        # Load audio and model\n",
    "        model = whisper.load_model(\"base\")  # You can change the model size if needed\n",
    "        audio = whisper.load_audio(audio_file_path)\n",
    "\n",
    "        # Transcribe the audio file\n",
    "        result = whisper.transcribe(\n",
    "            model, audio, language=\"en\"\n",
    "        )  # You can change the language\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during transcription: {e}\")\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return {}  # Return empty dict on error\n",
    "\n",
    "\n",
    "def process_and_transcribe_chunks(chunk_info_list: List[Dict], output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Transcribes audio chunks using whisper-timestamped and saves the transcriptions\n",
    "    to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        chunk_info_list (List[Dict]): A list of dictionaries, where each dictionary\n",
    "            contains chunk information as returned by the chunk_audio function.\n",
    "        output_dir (str): The directory where the JSON file should be saved.\n",
    "    \"\"\"\n",
    "    if not chunk_info_list:\n",
    "        print(\"No chunks to transcribe.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    transcriptions = []\n",
    "    for chunk_info in chunk_info_list:\n",
    "        chunk_file_path = chunk_info[\"file_path\"]\n",
    "        speaker = chunk_info[\"speaker\"]\n",
    "        start_time = chunk_info[\"start_time\"]\n",
    "        end_time = chunk_info[\"end_time\"]\n",
    "\n",
    "        print(\n",
    "            f\"Transcribing chunk: {chunk_file_path}, Speaker: {speaker}, Start: {start_time:.2f}, End: {end_time:.2f}\"\n",
    "        )\n",
    "        transcription = transcribe_audio_chunk(chunk_file_path)  # returns {} on error\n",
    "\n",
    "        if transcription:  # only add if transcription was successful\n",
    "            transcription_data = {\n",
    "                \"speaker\": speaker,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"transcription\": transcription,\n",
    "            }\n",
    "            transcriptions.append(transcription_data)\n",
    "\n",
    "            # Print the transcription\n",
    "            print(f\"Transcription for {chunk_file_path}:\")\n",
    "            for segment in transcription[\"segments\"]:\n",
    "                print(\n",
    "                    f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\"\n",
    "                )\n",
    "\n",
    "    # Save all transcriptions to a JSON file\n",
    "    output_json_path = os.path.join(output_dir, \"transcriptions.json\")\n",
    "    try:\n",
    "        with open(output_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        print(f\"Transcriptions saved to {output_json_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving transcriptions to JSON: {e}\")\n",
    "        print(f\"Error saving transcriptions to JSON: {e}\")\n",
    "\n",
    "\n",
    "def clean_transcription(transcriptions_json_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Cleans the transcription data from a JSON file, merging text from the same speaker\n",
    "    into paragraphs.  The output format is:  \"SpeakerName [start_time - end_time]: text\"\n",
    "\n",
    "    Args:\n",
    "        transcriptions_json_path (str): Path to the JSON file containing the transcription data\n",
    "            generated by process_and_transcribe_chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings, where each string represents a paragraph\n",
    "        in the format \"SpeakerName [start_time - end_time]: text\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(transcriptions_json_path, \"r\") as f:\n",
    "            transcriptions = json.load(f)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading transcriptions JSON: {e}\")\n",
    "        print(f\"Error loading transcriptions JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "    cleaned_transcriptions = []\n",
    "    current_speaker = None\n",
    "    current_text = \"\"\n",
    "    current_start_time = None\n",
    "    current_end_time = None\n",
    "\n",
    "    for chunk in transcriptions:\n",
    "        speaker = chunk[\"speaker\"]\n",
    "        # transcription = chunk[\"transcription\"] # Removed unused variable\n",
    "        chunk_start_time = chunk[\"start_time\"]  # Use chunk start/end times\n",
    "        chunk_end_time = chunk[\"end_time\"]\n",
    "\n",
    "        # if not transcription or not transcription[\"segments\"]: # Removed check for transcription\n",
    "        #     continue  # Skip empty transcriptions\n",
    "        if current_speaker != speaker:\n",
    "            if current_speaker is not None:\n",
    "                cleaned_transcriptions.append(\n",
    "                    f\"{current_speaker} [{current_start_time:.2f} - {current_end_time:.2f}]: {current_text}\"\n",
    "                )\n",
    "            current_speaker = speaker\n",
    "            current_text = \"\"  # Reset text for new speaker.\n",
    "            current_start_time = chunk_start_time  # from chunk\n",
    "            current_end_time = chunk_end_time  # from chunk\n",
    "        else:\n",
    "            #  The original error occurred because the code was trying to access\n",
    "            #  the 'text' key within the 'chunk' dictionary.  The 'chunk' dictionary\n",
    "            #  does NOT contain a 'text' key.  The 'text' comes from the 'segment'\n",
    "            #  in the inner loop, which is not used here.\n",
    "            #  Instead,  we should accumulate the text from the segments within\n",
    "            #  the same speaker.  But, we don't have the segments here.\n",
    "            #  The simplest fix is to just pass the entire chunk.\n",
    "            #  We will reconstruct the text.\n",
    "            # current_text += \" \" + chunk  # This line caused the error\n",
    "            # The corrected way is to accumulate the text.\n",
    "            # There is no 'text' in chunk, the text is in the transcription segments.\n",
    "            # But we don't have the segments here, so we will reconstruct the text later.\n",
    "            current_end_time = chunk_end_time  # from chunk\n",
    "\n",
    "        # Add the text from the chunk.\n",
    "        transcription = chunk.get(\n",
    "            \"transcription\", None\n",
    "        )  # Get transcription, None if missing.\n",
    "        if transcription and transcription[\"segments\"]:\n",
    "            for segment in transcription[\"segments\"]:\n",
    "                current_text += \" \" + segment[\"text\"]\n",
    "\n",
    "    # Add the last paragraph\n",
    "    if current_speaker is not None:\n",
    "        cleaned_transcriptions.append(\n",
    "            f\"{current_speaker} [{current_start_time:.2f} - {current_end_time:.2f}]: {current_text}\"\n",
    "        )\n",
    "\n",
    "    return cleaned_transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using device: cpu\n",
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "INFO:root:Diarization complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "Transcribing chunk: chunks\\chunk_1.mp3, Speaker: SPEAKER_00, Start: 0.03, End: 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:03<00:00, 273.04frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_1.mp3:\n",
      "    [0.00 - 1.58]  Anyway, look where we're digressing the rules.\n",
      "    [1.68 - 4.32]  Oh, simple, Emma, you're about to face five questions\n",
      "    [4.32 - 5.30]  of increasing difficulty.\n",
      "    [5.32 - 6.60]  You must answer as quickly as possible.\n",
      "    [6.62 - 8.32]  If you get it correct, you move onto the next round.\n",
      "    [8.58 - 9.76]  Do you know what happens if you get it wrong?\n",
      "Transcribing chunk: chunks\\chunk_2.mp3, Speaker: SPEAKER_01, Start: 11.42, End: 12.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:00<00:00, 140.70frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_2.mp3:\n",
      "    [0.10 - 1.14]  and correction and embarrassment.\n",
      "Transcribing chunk: chunks\\chunk_3.mp3, Speaker: SPEAKER_00, Start: 12.82, End: 14.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:01<00:00, 173.37frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_3.mp3:\n",
      "    [0.00 - 1.86]  Do indeed round one.\n",
      "Transcribing chunk: chunks\\chunk_4.mp3, Speaker: SPEAKER_00, Start: 16.94, End: 25.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:01<00:00, 439.77frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_4.mp3:\n",
      "    [0.08 - 4.48]  Round 1 astronomers are saying that Saturn's rings are slowly disappearing.\n",
      "    [4.76 - 8.70]  They estimate we only have a few hundred million years left of them.\n",
      "Transcribing chunk: chunks\\chunk_5.mp3, Speaker: SPEAKER_01, Start: 25.58, End: 27.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 155.12frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_5.mp3:\n",
      "    [0.06 - 1.52]  I'll earn you a few hundred million.\n",
      "Transcribing chunk: chunks\\chunk_6.mp3, Speaker: SPEAKER_00, Start: 27.37, End: 28.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:01<00:00, 136.58frames/s]\n",
      "ERROR:root:An error occurred: string indices must be integers, not 'str'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_6.mp3:\n",
      "    [0.16 - 1.08]  But what I want to know?\n",
      "Transcriptions saved to chunks\\transcriptions.json\n",
      "\n",
      "Cleaned Transcriptions:\n",
      "An error occurred: string indices must be integers, not 'str'\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "audio_file = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "output_dir = \"chunks\"\n",
    "# Make sure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    pipeline, diarization_result = diarize_audio(audio_file)\n",
    "    chunk_info_list = chunk_audio(audio_file, diarization_result, output_dir)\n",
    "    print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "    process_and_transcribe_chunks(\n",
    "        chunk_info_list, output_dir\n",
    "    )  # added call to new function\n",
    "\n",
    "    transcriptions_json = os.path.join(output_dir, \"transcriptions.json\")\n",
    "    cleaned_transcriptions = clean_transcription(transcriptions_json)\n",
    "\n",
    "    print(\"\\nCleaned Transcriptions:\")\n",
    "    for paragraph in cleaned_transcriptions:\n",
    "        print(f\"Speaker: {paragraph['speaker']}\")\n",
    "        print(f\"Time: {paragraph['start_time']:.2f} - {paragraph['end_time']:.2f}\")\n",
    "        print(f\"Text: {paragraph['text']}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\nSPEAKER_00 [0.03 - 10.00]:   Anyway, look where we're digressing the rules.  Oh, simple, Emma, you're about to face five questions  of increasing difficulty.  You must answer as quickly as possible.  If you get it correct, you move onto the next round.  Do you know what happens if you get it wrong?\", '\\nSPEAKER_01 [11.42 - 12.82]:   and correction and embarrassment.', \"\\nSPEAKER_00 [12.82 - 25.63]:   Do indeed round one.  Round 1 astronomers are saying that Saturn's rings are slowly disappearing.  They estimate we only have a few hundred million years left of them.\", \"\\nSPEAKER_01 [25.58 - 27.37]:   I'll earn you a few hundred million.\", 'SPEAKER_00 [27.37 - 28.97]:   But what I want to know?']\n"
     ]
    }
   ],
   "source": [
    "def clean_transcription(transcriptions_json_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Cleans the transcription data from a JSON file, merging text from the same speaker\n",
    "    into paragraphs.  The output format is:  \"SpeakerName [start_time - end_time]: text\"\n",
    "\n",
    "    Args:\n",
    "        transcriptions_json_path (str): Path to the JSON file containing the transcription data\n",
    "            generated by process_and_transcribe_chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings, where each string represents a paragraph\n",
    "        in the format \"SpeakerName [start_time - end_time]: text\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(transcriptions_json_path, \"r\") as f:\n",
    "            transcriptions = json.load(f)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading transcriptions JSON: {e}\")\n",
    "        print(f\"Error loading transcriptions JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "    cleaned_transcriptions = []\n",
    "    current_speaker = None\n",
    "    current_text = \"\"\n",
    "    current_start_time = None\n",
    "    current_end_time = None\n",
    "\n",
    "    for chunk in transcriptions:\n",
    "        speaker = chunk[\"speaker\"]\n",
    "        # transcription = chunk[\"transcription\"] # Removed unused variable\n",
    "        chunk_start_time = chunk[\"start_time\"]  # Use chunk start/end times\n",
    "        chunk_end_time = chunk[\"end_time\"]\n",
    "\n",
    "        # if not transcription or not [\"segments\"]: # Removed check for transcription\n",
    "        #     continue  # Skip empty transcriptions\n",
    "        if current_speaker != speaker:\n",
    "            if current_speaker is not None:\n",
    "                cleaned_transcriptions.append(\n",
    "                    f\"\\n{current_speaker} [{current_start_time:.2f} - {current_end_time:.2f}]: {current_text}\"  # Add newline\n",
    "                )\n",
    "            current_speaker = speaker\n",
    "            current_text = \"\"  # Reset text for new speaker.\n",
    "            current_start_time = chunk_start_time  # from chunk\n",
    "            current_end_time = chunk_end_time  # from chunk\n",
    "        else:\n",
    "            #  The original error occurred because the code was trying to access\n",
    "            #  the 'text' key within the 'chunk' dictionary.  The 'chunk' dictionary\n",
    "            #  does NOT contain a 'text' key.  The 'text' comes from the 'segment'\n",
    "            #  in the inner loop, which is not used here.\n",
    "            #  Instead,  we should accumulate the text from the segments within\n",
    "            #  the same speaker.  But, we don't have the segments here.\n",
    "            #  The simplest fix is to just pass the entire chunk.\n",
    "            #  We will reconstruct the text.\n",
    "            # current_text += \" \" + chunk  # This line caused the error\n",
    "            # The corrected way is to accumulate the text.\n",
    "            # There is no 'text' in chunk, the text is in the transcription segments.\n",
    "            # But we don't have the segments here, so we will reconstruct the text later.\n",
    "            current_end_time = chunk_end_time  # from chunk\n",
    "\n",
    "        # Add the text from the chunk.\n",
    "        transcription = chunk.get(\n",
    "            \"transcription\", None\n",
    "        )  # Get transcription, None if missing.\n",
    "        if transcription and transcription[\"segments\"]:\n",
    "            for segment in transcription[\"segments\"]:\n",
    "                current_text += \" \" + segment[\"text\"]\n",
    "\n",
    "    # Add the last paragraph\n",
    "    if current_speaker is not None:\n",
    "        cleaned_transcriptions.append(\n",
    "            f\"{current_speaker} [{current_start_time:.2f} - {current_end_time:.2f}]: {current_text}\"\n",
    "        )\n",
    "\n",
    "    return cleaned_transcriptions\n",
    "\n",
    "\n",
    "transcriptions_json = os.path.join(output_dir, \"transcriptions.json\")\n",
    "cleaned_transcriptions = clean_transcription(transcriptions_json)\n",
    "print(cleaned_transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from typing import Optional, Dict, Tuple\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Segment\n",
    "\n",
    "try:\n",
    "    from pydub import AudioSegment\n",
    "except ImportError:\n",
    "    print(\"pydub is not installed. Please install it using: pip install pydub\")\n",
    "    AudioSegment = None\n",
    "\n",
    "try:\n",
    "    import whisper_timestamped as whisper\n",
    "except ImportError:\n",
    "    print(\"whisper-timestamped is not installed. Please install it using: pip install whisper-timestamped\")\n",
    "    whisper = None\n",
    "\n",
    "\n",
    "def diarize_audio(\n",
    "    audio_file_path: str,\n",
    "    max_speakers: Optional[int] = None,\n",
    "    min_speakers: Optional[int] = None,\n",
    ") -> Tuple[Pipeline, Dict]:\n",
    "    \"\"\"\n",
    "    Performs speaker diarization on an audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        max_speakers (int, optional): Maximum number of speakers. Defaults to None.\n",
    "        min_speakers (int, optional): Minimum number of speakers. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the diarization pipeline and the diarization output.\n",
    "    \"\"\"\n",
    "    import logging\n",
    "\n",
    "    # Check if a GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=\"YOUR_HUGGINGFACE_TOKEN\",  # Replace with your token\n",
    "    )\n",
    "    pipeline.to(device)  # Move the pipeline to the selected device\n",
    "\n",
    "    # Load the audio file\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading audio file: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Prepare the input for the pipeline\n",
    "    input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "\n",
    "    # Add speaker number hints if provided\n",
    "    if max_speakers is not None:\n",
    "        input_data[\"max_speakers\"] = max_speakers\n",
    "    if min_speakers is not None:\n",
    "        input_data[\"min_speakers\"] = min_speakers\n",
    "\n",
    "    # Run the diarization pipeline\n",
    "    try:\n",
    "        diarization = pipeline(input_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during diarization: {e}\")\n",
    "        raise\n",
    "\n",
    "    logging.info(\"Diarization complete.\")\n",
    "    return pipeline, diarization\n",
    "\n",
    "\n",
    "\n",
    "def chunk_audio(audio_file_path: str, diarization: Dict, output_directory: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Chunks an audio file into segments based on the diarization output, saves them,\n",
    "    and returns a list of chunk information.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        diarization (Dict): Diarization output from the pyannote pipeline.\n",
    "        output_directory (str): Directory to save the chunked audio files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list where each element is a dictionary containing:\n",
    "            - file_path (str): Path to the saved audio chunk.\n",
    "            - speaker (str): Speaker label.\n",
    "            - start_time (float): Start time of the chunk in seconds.\n",
    "            - end_time (float): End time of the chunk in seconds.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Load the audio file using pydub\n",
    "    if AudioSegment is None:\n",
    "        print(\"Skipping audio chunking because pydub is not installed.\")\n",
    "        return []\n",
    "\n",
    "    audio = AudioSegment.from_file(audio_file_path)\n",
    "\n",
    "    chunk_info_list = []\n",
    "    chunk_number = 0  # Initialize chunk counter\n",
    "\n",
    "    # Iterate over each turn in the diarization output\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        start_time = turn.start\n",
    "        end_time = turn.end\n",
    "        start_time_ms = int(start_time * 1000)  # Convert seconds to milliseconds\n",
    "        end_time_ms = int(end_time * 1000)  # Convert seconds to milliseconds\n",
    "\n",
    "        chunk = audio[start_time_ms:end_time_ms]\n",
    "        chunk_number += 1  # Increment chunk counter for filename\n",
    "        output_file_path = os.path.join(\n",
    "            output_directory, f\"chunk_{chunk_number}.mp3\"  # Use chunk number in filename\n",
    "        )\n",
    "        chunk.export(output_file_path, format=\"mp3\")\n",
    "\n",
    "        chunk_info = {\n",
    "            \"file_path\": output_file_path,\n",
    "            \"speaker\": speaker,\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "        }\n",
    "        chunk_info_list.append(chunk_info)\n",
    "\n",
    "    return chunk_info_list\n",
    "\n",
    "\n",
    "\n",
    "def transcribe_audio_chunk(audio_file_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Transcribes an audio file using whisper-timestamped.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        Dict: The transcription with word-level timestamps.  Returns empty dict on error.\n",
    "    \"\"\"\n",
    "    if whisper is None:\n",
    "        print(\"whisper-timestamped is not installed. Transcription is skipped.\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        # Load audio and model\n",
    "        model = whisper.load_model(\"base\")  # You can change the model size if needed\n",
    "        audio = whisper.load_audio(audio_file_path)\n",
    "\n",
    "        # Transcribe the audio file\n",
    "        result = whisper.transcribe(model, audio, language=\"en\")  # You can change the language\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during transcription: {e}\")\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return {}  # Return empty dict on error\n",
    "\n",
    "\n",
    "\n",
    "def process_and_transcribe_chunks(chunk_info_list: List[Dict], output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Transcribes audio chunks using whisper-timestamped and saves the transcriptions\n",
    "    to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        chunk_info_list (List[Dict]): A list of dictionaries, where each dictionary\n",
    "            contains chunk information as returned by the chunk_audio function.\n",
    "        output_dir (str): The directory where the JSON file should be saved.\n",
    "    \"\"\"\n",
    "    if not chunk_info_list:\n",
    "        print(\"No chunks to transcribe.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    transcriptions = []\n",
    "    for chunk_info in chunk_info_list:\n",
    "        chunk_file_path = chunk_info[\"file_path\"]\n",
    "        speaker = chunk_info[\"speaker\"]\n",
    "        start_time = chunk_info[\"start_time\"]\n",
    "        end_time = chunk_info[\"end_time\"]\n",
    "\n",
    "        print(f\"Transcribing chunk: {chunk_file_path}, Speaker: {speaker}, Start: {start_time:.2f}, End: {end_time:.2f}\")\n",
    "        transcription = transcribe_audio_chunk(chunk_file_path) # returns {} on error\n",
    "\n",
    "        if transcription: # only add if transcription was successful\n",
    "            transcription_data = {\n",
    "                \"speaker\": speaker,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"transcription\": transcription,\n",
    "            }\n",
    "            transcriptions.append(transcription_data)\n",
    "\n",
    "            # Print the transcription\n",
    "            print(f\"Transcription for {chunk_file_path}:\")\n",
    "            for segment in transcription[\"segments\"]:\n",
    "                print(f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "    # Save all transcriptions to a JSON file\n",
    "    output_json_path = os.path.join(output_dir, \"transcriptions.json\")\n",
    "    try:\n",
    "        with open(output_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        print(f\"Transcriptions saved to {output_json_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving transcriptions to JSON: {e}\")\n",
    "        print(f\"Error saving transcriptions to JSON: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_transcription(transcriptions_json_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Cleans the transcription data from a JSON file, merging text from the same speaker\n",
    "    into paragraphs.  The output format is:  \"SpeakerName [start_time - end_time]: text\"\n",
    "\n",
    "    Args:\n",
    "        transcriptions_json_path (str): Path to the JSON file containing the transcription data\n",
    "            generated by process_and_transcribe_chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings, where each string represents a paragraph\n",
    "        in the format \"SpeakerName [start_time - end_time]: text\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(transcriptions_json_path, \"r\") as f:\n",
    "            transcriptions = json.load(f)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading transcriptions JSON: {e}\")\n",
    "        print(f\"Error loading transcriptions JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "    cleaned_transcriptions = []\n",
    "    current_speaker = None\n",
    "    current_text = \"\"\n",
    "    current_start_time = None\n",
    "    current_end_time = None\n",
    "\n",
    "    for chunk in transcriptions:\n",
    "        speaker = chunk[\"speaker\"]\n",
    "        # transcription = chunk[\"transcription\"] # Removed unused variable\n",
    "        chunk_start_time = chunk[\"start_time\"]  # Use chunk start/end times\n",
    "        chunk_end_time = chunk[\"end_time\"]\n",
    "\n",
    "        # if not transcription or not [\"segments\"]: # Removed check for transcription\n",
    "        #     continue  # Skip empty transcriptions\n",
    "        if current_speaker != speaker:\n",
    "            if current_speaker is not None:\n",
    "                cleaned_transcriptions.append(\n",
    "                    f\"\\n{current_speaker} [{current_start_time:.2f} - {current_end_time:.2f}]: {current_text}\"  # Add newline\n",
    "                )\n",
    "            current_speaker = speaker\n",
    "            current_text = \"\"  # Reset text for new speaker.\n",
    "            current_start_time = chunk_start_time  # from chunk\n",
    "            current_end_time = chunk_end_time  # from chunk\n",
    "        else:\n",
    "            #  The original error occurred because the code was trying to access\n",
    "            #  the 'text' key within the 'chunk' dictionary.  The 'chunk' dictionary\n",
    "            #  does NOT contain a 'text' key.  The 'text' comes from the 'segment'\n",
    "            #  in the inner loop, which is not used here.\n",
    "            #  Instead,  we should accumulate the text from the segments within\n",
    "            #  the same speaker.  But, we don't have the segments here.\n",
    "            #  The simplest fix is to just pass the entire chunk.\n",
    "            #  We will reconstruct the text.\n",
    "            # current_text += \" \" + chunk  # This line caused the error\n",
    "            # The corrected way is to accumulate the text.\n",
    "            # There is no 'text' in chunk, the text is in the transcription segments.\n",
    "            # But we don't have the segments here, so we will reconstruct the text later.\n",
    "            current_end_time = chunk_end_time  # from chunk\n",
    "\n",
    "        # Add the text from the chunk.\n",
    "        transcription = chunk.get(\"transcription\", None)  # Get transcription, None if missing.\n",
    "        if transcription and transcription[\"segments\"]:\n",
    "            for segment in transcription[\"segments\"]:\n",
    "                current_text += \" \" + segment[\"text\"]\n",
    "\n",
    "    # Add the last paragraph\n",
    "    if current_speaker is not None:\n",
    "        cleaned_transcriptions.append(\n",
    "            f\"{current_speaker} [{current_start_time:.2f} - {current_end_time:.2f}]: {current_text}\"\n",
    "        )\n",
    "\n",
    "    return cleaned_transcriptions\n",
    "\n",
    "\n",
    "def save_transcription_to_file(transcription_text: List[str], audio_file_path: str, output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the cleaned transcription text to a file.  The filename is derived from the\n",
    "    original audio file name.\n",
    "\n",
    "    Args:\n",
    "        transcription_text (List[str]):  A list of strings, where each string represents a paragraph\n",
    "            in the format \"SpeakerName [start_time - end_time]: text\".  This is the output\n",
    "            from the clean_transcription function.\n",
    "        audio_file_path (str):  The path to the original audio file.  This is used to derive\n",
    "            the output filename.\n",
    "        output_dir (str): The directory where the text file should be saved.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Get the base filename of the audio file.\n",
    "    audio_filename_base = os.path.splitext(os.path.basename(audio_file_path))[0]\n",
    "    output_file_path = os.path.join(output_dir, f\"{audio_filename_base}.txt\")\n",
    "\n",
    "    try:\n",
    "        with open(output_file_path, \"w\") as f:\n",
    "            for line in transcription_text:\n",
    "                f.write(line + \"\\n\")  # Write each line, adding a newline.\n",
    "        print(f\"Transcription saved to {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving transcription to file: {e}\")\n",
    "        print(f\"Error saving transcription to file: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using device: cpu\n",
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "INFO:root:Diarization complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "Transcribing chunk: chunks\\chunk_1.mp3, Speaker: SPEAKER_00, Start: 0.03, End: 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:03<00:00, 261.97frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_1.mp3:\n",
      "    [0.00 - 1.58]  Anyway, look where we're digressing the rules.\n",
      "    [1.68 - 4.32]  Oh, simple, Emma, you're about to face five questions\n",
      "    [4.32 - 5.30]  of increasing difficulty.\n",
      "    [5.32 - 6.60]  You must answer as quickly as possible.\n",
      "    [6.62 - 8.32]  If you get it correct, you move onto the next round.\n",
      "    [8.58 - 9.76]  Do you know what happens if you get it wrong?\n",
      "Transcribing chunk: chunks\\chunk_2.mp3, Speaker: SPEAKER_01, Start: 11.42, End: 12.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:01<00:00, 135.66frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_2.mp3:\n",
      "    [0.10 - 1.14]  and correction and embarrassment.\n",
      "Transcribing chunk: chunks\\chunk_3.mp3, Speaker: SPEAKER_00, Start: 12.82, End: 14.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:01<00:00, 187.77frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_3.mp3:\n",
      "    [0.00 - 1.86]  Do indeed round one.\n",
      "Transcribing chunk: chunks\\chunk_4.mp3, Speaker: SPEAKER_00, Start: 16.94, End: 25.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:01<00:00, 442.11frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_4.mp3:\n",
      "    [0.08 - 4.48]  Round 1 astronomers are saying that Saturn's rings are slowly disappearing.\n",
      "    [4.76 - 8.70]  They estimate we only have a few hundred million years left of them.\n",
      "Transcribing chunk: chunks\\chunk_5.mp3, Speaker: SPEAKER_01, Start: 25.58, End: 27.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 140.71frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_5.mp3:\n",
      "    [0.06 - 1.52]  I'll earn you a few hundred million.\n",
      "Transcribing chunk: chunks\\chunk_6.mp3, Speaker: SPEAKER_00, Start: 27.37, End: 28.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:02<00:00, 47.95frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_6.mp3:\n",
      "    [0.16 - 1.08]  But what I want to know?\n",
      "Transcriptions saved to chunks\\transcriptions.json\n",
      "\n",
      "Cleaned Transcriptions:\n",
      "\n",
      "SPEAKER_00 [0.03 - 10.00]:   Anyway, look where we're digressing the rules.  Oh, simple, Emma, you're about to face five questions  of increasing difficulty.  You must answer as quickly as possible.  If you get it correct, you move onto the next round.  Do you know what happens if you get it wrong?\n",
      "\n",
      "SPEAKER_01 [11.42 - 12.82]:   and correction and embarrassment.\n",
      "\n",
      "SPEAKER_00 [12.82 - 25.63]:   Do indeed round one.  Round 1 astronomers are saying that Saturn's rings are slowly disappearing.  They estimate we only have a few hundred million years left of them.\n",
      "\n",
      "SPEAKER_01 [25.58 - 27.37]:   I'll earn you a few hundred million.\n",
      "SPEAKER_00 [27.37 - 28.97]:   But what I want to know?\n",
      "Transcription saved to chunks\\test.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "audio_file = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "output_dir = \"chunks\"\n",
    "# Make sure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    pipeline, diarization_result = diarize_audio(audio_file)\n",
    "    chunk_info_list = chunk_audio(audio_file, diarization_result, output_dir)\n",
    "    print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "    process_and_transcribe_chunks(chunk_info_list, output_dir)\n",
    "\n",
    "    transcriptions_json = os.path.join(output_dir, \"transcriptions.json\")\n",
    "    cleaned_transcriptions = clean_transcription(transcriptions_json)\n",
    "\n",
    "    print(\"\\nCleaned Transcriptions:\")\n",
    "    for paragraph in cleaned_transcriptions:\n",
    "        print(paragraph)\n",
    "\n",
    "    save_transcription_to_file(\n",
    "        cleaned_transcriptions, audio_file, output_dir\n",
    "    )  # Save to text file\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thoerial the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "INFO:root:Diarization complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "Speaker names saved to chunks\\speaker_names.json\n",
      "Transcribing chunk: chunks\\chunk_1.mp3, Speaker: SPEAKER_00, Start: 0.03, End: 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:03<00:00, 273.25frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_1.mp3:\n",
      "    [0.00 - 1.58]  Anyway, look where we're digressing the rules.\n",
      "    [1.68 - 4.32]  Oh, simple, Emma, you're about to face five questions\n",
      "    [4.32 - 5.30]  of increasing difficulty.\n",
      "    [5.32 - 6.60]  You must answer as quickly as possible.\n",
      "    [6.62 - 8.32]  If you get it correct, you move onto the next round.\n",
      "    [8.58 - 9.76]  Do you know what happens if you get it wrong?\n",
      "Transcribing chunk: chunks\\chunk_2.mp3, Speaker: SPEAKER_01, Start: 11.42, End: 12.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:01<00:00, 135.27frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_2.mp3:\n",
      "    [0.10 - 1.14]  and correction and embarrassment.\n",
      "Transcribing chunk: chunks\\chunk_3.mp3, Speaker: SPEAKER_00, Start: 12.82, End: 14.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:01<00:00, 206.29frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_3.mp3:\n",
      "    [0.00 - 1.86]  Do indeed round one.\n",
      "Transcribing chunk: chunks\\chunk_4.mp3, Speaker: SPEAKER_00, Start: 16.94, End: 25.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:01<00:00, 474.21frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_4.mp3:\n",
      "    [0.08 - 4.48]  Round 1 astronomers are saying that Saturn's rings are slowly disappearing.\n",
      "    [4.76 - 8.70]  They estimate we only have a few hundred million years left of them.\n",
      "Transcribing chunk: chunks\\chunk_5.mp3, Speaker: SPEAKER_01, Start: 25.58, End: 27.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 154.51frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_5.mp3:\n",
      "    [0.06 - 1.52]  I'll earn you a few hundred million.\n",
      "Transcribing chunk: chunks\\chunk_6.mp3, Speaker: SPEAKER_00, Start: 27.37, End: 28.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:01<00:00, 122.63frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_6.mp3:\n",
      "    [0.16 - 1.08]  But what I want to know?\n",
      "Transcriptions saved to chunks\\transcriptions.json\n",
      "\n",
      "Cleaned Transcriptions:\n",
      "\n",
      "Speaker 1 [0.03 - 10.00]:   Anyway, look where we're digressing the rules.  Oh, simple, Emma, you're about to face five questions  of increasing difficulty.  You must answer as quickly as possible.  If you get it correct, you move onto the next round.  Do you know what happens if you get it wrong?\n",
      "\n",
      "Speaker 2 [11.42 - 12.82]:   and correction and embarrassment.\n",
      "\n",
      "Speaker 1 [12.82 - 14.91]:   Do indeed round one.  Round 1 astronomers are saying that Saturn's rings are slowly disappearing.  They estimate we only have a few hundred million years left of them.\n",
      "\n",
      "Speaker 2 [25.58 - 27.37]:   I'll earn you a few hundred million.\n",
      "Speaker 1 [27.37 - 28.97]:   But what I want to know?\n",
      "Transcription saved to chunks\\test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from typing import Optional, Dict, Tuple\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Segment\n",
    "from pyannote.core import Annotation\n",
    "\n",
    "try:\n",
    "    from pydub import AudioSegment\n",
    "except ImportError:\n",
    "    print(\"pydub is not installed. Please install it using: pip install pydub\")\n",
    "    AudioSegment = None\n",
    "\n",
    "try:\n",
    "    import whisper_timestamped as whisper\n",
    "except ImportError:\n",
    "    print(\n",
    "        \"whisper-timestamped is not installed. Please install it using: pip install whisper-timestamped\"\n",
    "    )\n",
    "    whisper = None\n",
    "\n",
    "\n",
    "def diarize_audio(\n",
    "    audio_file_path: str,\n",
    "    max_speakers: Optional[int] = None,\n",
    "    min_speakers: Optional[int] = None,\n",
    ") -> Tuple[Pipeline, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Performs speaker diarization on an audio file.  Also creates a speaker names mapping.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        max_speakers (int, optional): Maximum number of speakers. Defaults to None.\n",
    "        min_speakers (int, optional): Minimum number of speakers. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the diarization pipeline, the diarization output,\n",
    "               and a dictionary of speaker names.\n",
    "    \"\"\"\n",
    "    import logging\n",
    "\n",
    "    # Check if a GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=\"YOUR_HUGGINGFACE_TOKEN\",  # Replace with your token\n",
    "    )\n",
    "    pipeline.to(device)  # Move the pipeline to the selected device\n",
    "\n",
    "    # Load the audio file\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading audio file: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Prepare the input for the pipeline\n",
    "    input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "\n",
    "    # Add speaker number hints if provided\n",
    "    if max_speakers is not None:\n",
    "        input_data[\"max_speakers\"] = max_speakers\n",
    "    if min_speakers is not None:\n",
    "        input_data[\"min_speakers\"] = min_speakers\n",
    "\n",
    "    # Run the diarization pipeline\n",
    "    try:\n",
    "        diarization = pipeline(input_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during diarization: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Create speaker names dictionary.  Default is Speaker 1, Speaker 2, etc.\n",
    "    speaker_names = {}\n",
    "    if isinstance(diarization, Annotation):\n",
    "        # Iterate over the segments and their corresponding labels.\n",
    "        for segment, track_name, label in diarization.itertracks(yield_label=True):\n",
    "            if label not in speaker_names:\n",
    "                speaker_names[label] = f\"Speaker {len(speaker_names) + 1}\"\n",
    "\n",
    "    logging.info(\"Diarization complete.\")\n",
    "    return pipeline, diarization, speaker_names\n",
    "\n",
    "\n",
    "def chunk_audio(\n",
    "    audio_file_path: str, diarization: Dict, output_directory: str\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Chunks an audio file into segments based on the diarization output, saves them,\n",
    "    and returns a list of chunk information.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        diarization (Dict): Diarization output from the pyannote pipeline.\n",
    "        output_directory (str): Directory to save the chunked audio files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list where each element is a dictionary containing:\n",
    "            - file_path (str): Path to the saved audio chunk.\n",
    "            - speaker (str): Speaker label.\n",
    "            - start_time (float): Start time of the chunk in seconds.\n",
    "            - end_time (float): End time of the chunk in seconds.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Load the audio file using pydub\n",
    "    if AudioSegment is None:\n",
    "        print(\"Skipping audio chunking because pydub is not installed.\")\n",
    "        return []\n",
    "\n",
    "    audio = AudioSegment.from_file(audio_file_path)\n",
    "\n",
    "    chunk_info_list = []\n",
    "    chunk_number = 0  # Initialize chunk counter\n",
    "\n",
    "    # Iterate over each turn in the diarization output\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        start_time = turn.start\n",
    "        end_time = turn.end\n",
    "        start_time_ms = int(start_time * 1000)  # Convert seconds to milliseconds\n",
    "        end_time_ms = int(end_time * 1000)  # Convert seconds to milliseconds\n",
    "\n",
    "        chunk = audio[start_time_ms:end_time_ms]\n",
    "        chunk_number += 1  # Increment chunk counter for filename\n",
    "        output_file_path = os.path.join(\n",
    "            output_directory,\n",
    "            f\"chunk_{chunk_number}.mp3\",  # Use chunk number in filename\n",
    "        )\n",
    "        chunk.export(output_file_path, format=\"mp3\")\n",
    "\n",
    "        chunk_info = {\n",
    "            \"file_path\": output_file_path,\n",
    "            \"speaker\": speaker,\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "        }\n",
    "        chunk_info_list.append(chunk_info)\n",
    "\n",
    "    return chunk_info_list\n",
    "\n",
    "\n",
    "def transcribe_audio_chunk(audio_file_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Transcribes an audio file using whisper-timestamped.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        Dict: The transcription with word-level timestamps.  Returns empty dict on error.\n",
    "    \"\"\"\n",
    "    if whisper is None:\n",
    "        print(\"whisper-timestamped is not installed. Transcription is skipped.\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        # Load audio and model\n",
    "        model = whisper.load_model(\"base\")  # You can change the model size if needed\n",
    "        audio = whisper.load_audio(audio_file_path)\n",
    "\n",
    "        # Transcribe the audio file\n",
    "        result = whisper.transcribe(\n",
    "            model, audio, language=\"en\"\n",
    "        )  # You can change the language\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during transcription: {e}\")\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return {}  # Return empty dict on error\n",
    "\n",
    "\n",
    "def process_and_transcribe_chunks(chunk_info_list: List[Dict], output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Transcribes audio chunks using whisper-timestamped and saves the transcriptions\n",
    "    to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        chunk_info_list (List[Dict]): A list of dictionaries, where each dictionary\n",
    "            contains chunk information as returned by the chunk_audio function.\n",
    "        output_dir (str): The directory where the JSON file should be saved.\n",
    "    \"\"\"\n",
    "    if not chunk_info_list:\n",
    "        print(\"No chunks to transcribe.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    transcriptions = []\n",
    "    for chunk_info in chunk_info_list:\n",
    "        chunk_file_path = chunk_info[\"file_path\"]\n",
    "        speaker = chunk_info[\"speaker\"]\n",
    "        start_time = chunk_info[\"start_time\"]\n",
    "        end_time = chunk_info[\"end_time\"]\n",
    "\n",
    "        print(\n",
    "            f\"Transcribing chunk: {chunk_file_path}, Speaker: {speaker}, Start: {start_time:.2f}, End: {end_time:.2f}\"\n",
    "        )\n",
    "        transcription = transcribe_audio_chunk(chunk_file_path)  # returns {} on error\n",
    "\n",
    "        if transcription:  # only add if transcription was successful\n",
    "            transcription_data = {\n",
    "                \"speaker\": speaker,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"transcription\": transcription,\n",
    "            }\n",
    "            transcriptions.append(transcription_data)\n",
    "\n",
    "            # Print the transcription\n",
    "            print(f\"Transcription for {chunk_file_path}:\")\n",
    "            for segment in transcription[\"segments\"]:\n",
    "                print(\n",
    "                    f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\"\n",
    "                )\n",
    "\n",
    "    # Save all transcriptions to a JSON file\n",
    "    output_json_path = os.path.join(output_dir, \"transcriptions.json\")\n",
    "    try:\n",
    "        with open(output_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        print(f\"Transcriptions saved to {output_json_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving transcriptions to JSON: {e}\")\n",
    "        print(f\"Error saving transcriptions to JSON: {e}\")\n",
    "\n",
    "\n",
    "def clean_transcription(\n",
    "    transcriptions_json_path: str, speaker_names: Dict[str, str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Cleans the transcription data from a JSON file, merging text from the same speaker\n",
    "    into paragraphs.  The output format is:  \"SpeakerName [start_time - end_time]: text\"\n",
    "\n",
    "    Args:\n",
    "        transcriptions_json_path (str): Path to the JSON file containing the transcription data\n",
    "            generated by process_and_transcribe_chunks.\n",
    "        speaker_names (Dict[str, str]): A dictionary mapping speaker labels (e.g., \"SPEAKER_01\")\n",
    "            to speaker names (e.g., \"John Doe\").\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings, where each string represents a paragraph\n",
    "        in the format \"SpeakerName [start_time - end_time]: text\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(transcriptions_json_path, \"r\") as f:\n",
    "            transcriptions = json.load(f)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading transcriptions JSON: {e}\")\n",
    "        print(f\"Error loading transcriptions JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "    cleaned_transcriptions = []\n",
    "    current_speaker = None\n",
    "    current_text = \"\"\n",
    "    current_start_time = None\n",
    "    current_end_time = None\n",
    "\n",
    "    for chunk in transcriptions:\n",
    "        speaker = chunk[\"speaker\"]\n",
    "        chunk_start_time = chunk[\"start_time\"]\n",
    "        chunk_end_time = chunk[\"end_time\"]\n",
    "\n",
    "        if current_speaker != speaker:\n",
    "            if current_speaker is not None:\n",
    "                # Use the speaker name from the dictionary, or the original if not found.\n",
    "                speaker_name = speaker_names.get(current_speaker, current_speaker)\n",
    "                cleaned_transcriptions.append(\n",
    "                    f\"\\n{speaker_name} [{current_start_time:.2f} - {current_end_time:.2f}]: {current_text}\"\n",
    "                )\n",
    "            current_speaker = speaker\n",
    "            current_text = \"\"\n",
    "            current_start_time = chunk_start_time\n",
    "            current_end_time = chunk_end_time\n",
    "\n",
    "        transcription = chunk.get(\"transcription\", None)\n",
    "        if transcription and transcription[\"segments\"]:\n",
    "            for segment in transcription[\"segments\"]:\n",
    "                current_text += \" \" + segment[\"text\"]\n",
    "\n",
    "    if current_speaker is not None:\n",
    "        speaker_name = speaker_names.get(current_speaker, current_speaker)\n",
    "        cleaned_transcriptions.append(\n",
    "            f\"{speaker_name} [{current_start_time:.2f} - {current_end_time:.2f}]: {current_text}\"\n",
    "        )\n",
    "\n",
    "    return cleaned_transcriptions\n",
    "\n",
    "\n",
    "def save_transcription_to_file(\n",
    "    transcription_text: List[str], audio_file_path: str, output_dir: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Saves the cleaned transcription text to a file.  The filename is derived from the\n",
    "    original audio file name.\n",
    "\n",
    "    Args:\n",
    "        transcription_text (List[str]):  A list of strings, where each string represents a paragraph\n",
    "            in the format \"SpeakerName [start_time - end_time]: text\".  This is the output\n",
    "            from the clean_transcription function.\n",
    "        audio_file_path (str):  The path to the original audio file.  This is used to derive\n",
    "            the output filename.\n",
    "        output_dir (str): The directory where the text file should be saved.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Get the base filename of the audio file.\n",
    "    audio_filename_base = os.path.splitext(os.path.basename(audio_file_path))[0]\n",
    "    output_file_path = os.path.join(output_dir, f\"{audio_filename_base}.txt\")\n",
    "\n",
    "    try:\n",
    "        with open(output_file_path, \"w\") as f:\n",
    "            for line in transcription_text:\n",
    "                f.write(line + \"\\n\")  # Write each line, adding a newline.\n",
    "        print(f\"Transcription saved to {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving transcription to file: {e}\")\n",
    "        print(f\"Error saving transcription to file: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    audio_file = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "    output_dir = \"chunks\"\n",
    "    # Make sure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Set up basic logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    try:\n",
    "        pipeline, diarization_result, speaker_names = diarize_audio(\n",
    "            audio_file\n",
    "        )  # Get speaker_names\n",
    "        chunk_info_list = chunk_audio(audio_file, diarization_result, output_dir)\n",
    "        print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "        # Save speaker names to a JSON file\n",
    "        speaker_names_path = os.path.join(output_dir, \"speaker_names.json\")\n",
    "        with open(speaker_names_path, \"w\") as f:\n",
    "            json.dump(speaker_names, f, indent=4)\n",
    "        print(f\"Speaker names saved to {speaker_names_path}\")\n",
    "\n",
    "        # You can modify speaker_names here if needed\n",
    "        speaker_names[\"SPEAKER_01\"] = \"Moderator\"\n",
    "        speaker_names[\"SPEAKER_02\"] = \"Participant 1\"\n",
    "\n",
    "        process_and_transcribe_chunks(chunk_info_list, output_dir)\n",
    "\n",
    "        transcriptions_json = os.path.join(output_dir, \"transcriptions.json\")\n",
    "        cleaned_transcriptions = clean_transcription(\n",
    "            transcriptions_json, speaker_names\n",
    "        )  # Pass speaker_names\n",
    "        print(\"\\nCleaned Transcriptions:\")\n",
    "        for paragraph in cleaned_transcriptions:\n",
    "            print(paragraph)\n",
    "\n",
    "        save_transcription_to_file(\n",
    "            cleaned_transcriptions, audio_file, output_dir\n",
    "        )  # Save to text file\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Annotation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m         logging.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading audio file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_diarization\u001b[39m(audio_file_path: \u001b[38;5;28mstr\u001b[39m, max_speakers: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, min_speakers: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> Tuple[Pipeline, \u001b[43mAnnotation\u001b[49m, Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Performs speaker diarization.\"\"\"\u001b[39;00m\n\u001b[32m     38\u001b[39m     device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Annotation' is not defined"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import whisper_timestamped as whisper\n",
    "\n",
    "# Load the Hugging Face token from config.py\n",
    "try:\n",
    "    from config import HF_TOKEN\n",
    "except ImportError:\n",
    "    HF_TOKEN = None\n",
    "    logging.warning(\n",
    "        \"config.py not found or HF_TOKEN not defined. Diarization may not work.\"\n",
    "        \" Please create a config.py file with HF_TOKEN = 'YOUR_HUGGINGFACE_TOKEN'\"\n",
    "    )\n",
    "WHISPER_MODEL = \"base\"\n",
    "#TRANSCRIPTION_LANGUAGE = \"en\"\n",
    "DEBUG_MODE = False  # Set to False to disable debug printing\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def load_audio(audio_file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\"Loads an audio file using torchaudio.\"\"\"\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "        return waveform, sample_rate\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading audio file: {e}\")\n",
    "        raise\n",
    "\n",
    "def run_diarization(audio_file_path: str, max_speakers: Optional[int] = None, min_speakers: Optional[int] = None) -> Tuple[Pipeline, Annotation, Dict[str, str]]:\n",
    "    \"\"\"Performs speaker diarization.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\", use_auth_token=HF_TOKEN\n",
    "    ).to(device)\n",
    "    waveform, sample_rate = load_audio(audio_file_path)\n",
    "    input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "    if max_speakers: input_data[\"max_speakers\"] = max_speakers\n",
    "    if min_speakers: input_data[\"min_speakers\"] = min_speakers\n",
    "    diarization = pipeline(input_data)\n",
    "    speaker_names = {label: f\"Speaker {i + 1}\" for i, label in enumerate(sorted(set(segment.label for segment in diarization.segments)))}\n",
    "    return pipeline, diarization, speaker_names\n",
    "\n",
    "def chunk_audio(audio_file_path: str, diarization: Annotation, output_directory: str) -> List[Dict]:\n",
    "    \"\"\"Chunks audio based on diarization.\"\"\"\n",
    "    if not os.path.exists(output_directory): os.makedirs(output_directory)\n",
    "    audio = AudioSegment.from_file(audio_file_path)\n",
    "    chunks = []\n",
    "    for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True), 1):\n",
    "        start_ms, end_ms = int(turn.start * 1000), int(turn.end * 1000)\n",
    "        chunk_path = os.path.join(output_directory, f\"chunk_{i}.mp3\")\n",
    "        audio[start_ms:end_ms].export(chunk_path, format=\"mp3\")\n",
    "        chunks.append({\"file_path\": chunk_path, \"speaker\": speaker, \"start_time\": turn.start, \"end_time\": turn.end})\n",
    "    return chunks\n",
    "\n",
    "def transcribe_chunk(audio_file_path: str) -> Dict:\n",
    "    \"\"\"Transcribes an audio chunk.\"\"\"\n",
    "    try:\n",
    "        model = whisper.load_model(WHISPER_MODEL)\n",
    "        audio = whisper.load_audio(audio_file_path)\n",
    "        return whisper.transcribe(model, audio, language=TRANSCRIPTION_LANGUAGE)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Transcription error: {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_transcriptions(chunks: List[Dict], output_dir: str) -> List[Dict]:\n",
    "    \"\"\"Processes and saves transcriptions.\"\"\"\n",
    "    transcriptions = []\n",
    "    for chunk in chunks:\n",
    "        logging.info(f\"Transcribing {chunk['file_path']}\")\n",
    "        transcription = transcribe_chunk(chunk[\"file_path\"])\n",
    "        if transcription:\n",
    "            transcriptions.append({**chunk, \"transcription\": transcription})\n",
    "            if DEBUG_MODE: # only print if debug mode is on\n",
    "                print(f\"Transcription for {chunk['file_path']}:\")\n",
    "                for segment in transcription[\"segments\"]:\n",
    "                    print(f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "    with open(os.path.join(output_dir, \"transcriptions.json\"), \"w\") as f:\n",
    "        json.dump(transcriptions, f, indent=4)\n",
    "    return transcriptions\n",
    "\n",
    "def clean_and_save_transcriptions(transcriptions: List[Dict], speaker_names: Dict[str, str], audio_file_path: str, output_dir: str):\n",
    "    \"\"\"Cleans and saves transcriptions to a text file.\"\"\"\n",
    "    cleaned = []\n",
    "    current_speaker, current_text, current_start, current_end = None, \"\", None, None\n",
    "    for chunk in transcriptions:\n",
    "        if current_speaker != chunk[\"speaker\"]:\n",
    "            if current_speaker:\n",
    "                cleaned.append(f\"\\n{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "            current_speaker, current_text = chunk[\"speaker\"], \"\"\n",
    "            current_start, current_end = chunk[\"start_time\"], chunk[\"end_time\"]\n",
    "        if chunk[\"transcription\"] and chunk[\"transcription\"][\"segments\"]:\n",
    "            current_text += \" \".join(seg[\"text\"] for seg in chunk[\"transcription\"][\"segments\"])\n",
    "    if current_speaker:\n",
    "        cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "    output_file = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(audio_file_path))[0]}.txt\")\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(cleaned))\n",
    "\n",
    "    if DEBUG_MODE: # only print the cleaned transcriptions if debug mode is on\n",
    "        print(\"\\nCleaned Transcriptions:\")\n",
    "        for paragraph in cleaned:\n",
    "            print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using device: cpu\n",
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "INFO:root:Diarization complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "Speaker names saved to chunks\\speaker_names.json\n",
      "Transcribing chunk: chunks\\chunk_1.mp3, Speaker: SPEAKER_00, Start: 0.03, End: 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:03<00:00, 288.60frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_1.mp3:\n",
      "    [0.00 - 1.58]  Anyway, look where we're digressing the rules.\n",
      "    [1.68 - 4.32]  Oh, simple, Emma, you're about to face five questions\n",
      "    [4.32 - 5.30]  of increasing difficulty.\n",
      "    [5.32 - 6.60]  You must answer as quickly as possible.\n",
      "    [6.62 - 8.32]  If you get it correct, you move onto the next round.\n",
      "    [8.58 - 9.76]  Do you know what happens if you get it wrong?\n",
      "Transcribing chunk: chunks\\chunk_2.mp3, Speaker: SPEAKER_01, Start: 11.42, End: 12.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:01<00:00, 137.32frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_2.mp3:\n",
      "    [0.10 - 1.14]  and correction and embarrassment.\n",
      "Transcribing chunk: chunks\\chunk_3.mp3, Speaker: SPEAKER_00, Start: 12.82, End: 14.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:00<00:00, 223.29frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_3.mp3:\n",
      "    [0.00 - 1.86]  Do indeed round one.\n",
      "Transcribing chunk: chunks\\chunk_4.mp3, Speaker: SPEAKER_00, Start: 16.94, End: 25.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:02<00:00, 393.47frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_4.mp3:\n",
      "    [0.08 - 4.48]  Round 1 astronomers are saying that Saturn's rings are slowly disappearing.\n",
      "    [4.76 - 8.70]  They estimate we only have a few hundred million years left of them.\n",
      "Transcribing chunk: chunks\\chunk_5.mp3, Speaker: SPEAKER_01, Start: 25.58, End: 27.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 159.92frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_5.mp3:\n",
      "    [0.06 - 1.52]  I'll earn you a few hundred million.\n",
      "Transcribing chunk: chunks\\chunk_6.mp3, Speaker: SPEAKER_00, Start: 27.37, End: 28.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:01<00:00, 127.65frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for chunks\\chunk_6.mp3:\n",
      "    [0.16 - 1.08]  But what I want to know?\n",
      "Transcriptions saved to chunks\\transcriptions.json\n",
      "\n",
      "Cleaned Transcriptions:\n",
      "\n",
      "Speaker 1 [0.03 - 10.00]:   Anyway, look where we're digressing the rules.  Oh, simple, Emma, you're about to face five questions  of increasing difficulty.  You must answer as quickly as possible.  If you get it correct, you move onto the next round.  Do you know what happens if you get it wrong?\n",
      "\n",
      "Moderator [11.42 - 12.82]:   and correction and embarrassment.\n",
      "\n",
      "Speaker 1 [12.82 - 14.91]:   Do indeed round one.  Round 1 astronomers are saying that Saturn's rings are slowly disappearing.  They estimate we only have a few hundred million years left of them.\n",
      "\n",
      "Moderator [25.58 - 27.37]:   I'll earn you a few hundred million.\n",
      "Speaker 1 [27.37 - 28.97]:   But what I want to know?\n",
      "Transcription saved to chunks\\test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "audio_file = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "output_dir = \"chunks\"\n",
    "# Make sure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    pipeline, diarization_result, speaker_names = diarize_audio(\n",
    "        audio_file\n",
    "    )  # Get speaker_names\n",
    "    chunk_info_list = chunk_audio(audio_file, diarization_result, output_dir)\n",
    "    print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "    # Save speaker names to a JSON file\n",
    "    speaker_names_path = os.path.join(output_dir, \"speaker_names.json\")\n",
    "    with open(speaker_names_path, \"w\") as f:\n",
    "        json.dump(speaker_names, f, indent=4)\n",
    "    print(f\"Speaker names saved to {speaker_names_path}\")\n",
    "\n",
    "    # You can modify speaker_names here if needed\n",
    "    speaker_names[\"SPEAKER_01\"] = \"Moderator\"\n",
    "    speaker_names[\"SPEAKER_02\"] = \"Participant 1\"\n",
    "\n",
    "    process_and_transcribe_chunks(chunk_info_list, output_dir)\n",
    "\n",
    "    transcriptions_json = os.path.join(output_dir, \"transcriptions.json\")\n",
    "    cleaned_transcriptions = clean_transcription(\n",
    "        transcriptions_json, speaker_names\n",
    "    )  # Pass speaker_names\n",
    "    print(\"\\nCleaned Transcriptions:\")\n",
    "    for paragraph in cleaned_transcriptions:\n",
    "        print(paragraph)\n",
    "\n",
    "    save_transcription_to_file(\n",
    "        cleaned_transcriptions, audio_file, output_dir\n",
    "    )  # Save to text file\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 18:26:36,349 - INFO - Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using automatic language detection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "2025-03-23 18:27:10,294 - INFO - Transcribing chunks\\chunk_1.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "Speaker names saved to chunks\\speaker_names.json\n",
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:03<00:00, 316.61frames/s]\n",
      "2025-03-23 18:27:15,779 - INFO - Transcribing chunks\\chunk_2.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:00<00:00, 148.70frames/s]\n",
      "2025-03-23 18:27:18,679 - INFO - Transcribing chunks\\chunk_3.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:01<00:00, 172.15frames/s]\n",
      "2025-03-23 18:27:21,890 - INFO - Transcribing chunks\\chunk_4.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:01<00:00, 448.40frames/s]\n",
      "2025-03-23 18:27:25,951 - INFO - Transcribing chunks\\chunk_5.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:02<00:00, 61.94frames/s]\n",
      "2025-03-23 18:27:31,068 - INFO - Transcribing chunks\\chunk_6.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:01<00:00, 140.95frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Transcriptions:\n",
      "\n",
      "Speaker 1 [0.03 - 10.00]:  Anyway, look where we're digressing the rules.  Oh, simple, Emma, you're about to face five questions  of increasing difficulty.  You must answer as quickly as possible.  If you get it correct, you move onto the next round.  Do you know what happens if you get it wrong?\n",
      "\n",
      "Moderator [11.42 - 12.82]:  and correction and embarrassment.\n",
      "\n",
      "Speaker 1 [12.82 - 14.91]:  Do indeed round one. Round 1 astronomers are saying that Saturn's rings are slowly disappearing.  They estimate we only have a few hundred million years left of them.\n",
      "\n",
      "Moderator [25.58 - 27.37]:  I'll earn you a few hundred million.\n",
      "Speaker 1 [27.37 - 28.97]:  But what I want to know?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Annotation\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import whisper_timestamped as whisper\n",
    "\n",
    "# Load the Hugging Face token from config.py\n",
    "try:\n",
    "    from config import HF_TOKEN\n",
    "except ImportError:\n",
    "    HF_TOKEN = None\n",
    "    logging.warning(\n",
    "        \"config.py not found or HF_TOKEN not defined. Diarization may not work.\"\n",
    "        \" Please create a config.py file with HF_TOKEN = 'YOUR_HUGGINGFACE_TOKEN'\"\n",
    "    )\n",
    "WHISPER_MODEL = \"base\"\n",
    "# TRANSCRIPTION_LANGUAGE = \"en\" # Removed hardcoded language\n",
    "DEBUG_MODE = False  # Set to False to disable debug printing\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def load_audio(audio_file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\"Loads an audio file using torchaudio.\"\"\"\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "        return waveform, sample_rate\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading audio file: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def run_diarization(\n",
    "    audio_file_path: str,\n",
    "    max_speakers: Optional[int] = None,\n",
    "    min_speakers: Optional[int] = None,\n",
    ") -> Tuple[Pipeline, Annotation, Dict[str, str]]:\n",
    "    \"\"\"Performs speaker diarization.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\", use_auth_token=HF_TOKEN\n",
    "    ).to(device)\n",
    "    waveform, sample_rate = load_audio(audio_file_path)\n",
    "    input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "    if max_speakers:\n",
    "        input_data[\"max_speakers\"] = max_speakers\n",
    "    if min_speakers:\n",
    "        input_data[\"min_speakers\"] = min_speakers\n",
    "    diarization = pipeline(input_data)\n",
    "    speaker_labels = set()\n",
    "    for segment, _, label in diarization.itertracks(yield_label=True):\n",
    "        speaker_labels.add(label)\n",
    "    speaker_names = {\n",
    "        label: f\"Speaker {i + 1}\" for i, label in enumerate(sorted(speaker_labels))\n",
    "    }\n",
    "    return pipeline, diarization, speaker_names\n",
    "\n",
    "\n",
    "def chunk_audio(audio_file_path: str, diarization: Annotation, output_directory: str) -> List[Dict]:\n",
    "    \"\"\"Chunks audio based on diarization.\"\"\"\n",
    "    if not os.path.exists(output_directory): os.makedirs(output_directory)\n",
    "    audio = AudioSegment.from_file(audio_file_path)\n",
    "    chunks = []\n",
    "    for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True), 1):\n",
    "        start_ms, end_ms = int(turn.start * 1000), int(turn.end * 1000)\n",
    "        chunk_path = os.path.join(output_directory, f\"chunk_{i}.mp3\")\n",
    "        audio[start_ms:end_ms].export(chunk_path, format=\"mp3\")\n",
    "        chunks.append({\"file_path\": chunk_path, \"speaker\": speaker, \"start_time\": turn.start, \"end_time\": turn.end})\n",
    "    return chunks\n",
    "\n",
    "def transcribe_chunk(audio_file_path: str, language: Optional[str] = None) -> Dict:\n",
    "    \"\"\"Transcribes an audio chunk.\"\"\"\n",
    "    try:\n",
    "        model = whisper.load_model(WHISPER_MODEL)\n",
    "        audio = whisper.load_audio(audio_file_path)\n",
    "        return whisper.transcribe(model, audio, language=language)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Transcription error: {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_and_transcribe_chunks(chunks: List[Dict], output_dir: str, language: Optional[str] = None) -> List[Dict]:\n",
    "    \"\"\"Processes and saves transcriptions.\"\"\"\n",
    "    transcriptions = []\n",
    "    for chunk in chunks:\n",
    "        logging.info(f\"Transcribing {chunk['file_path']}\")\n",
    "        transcription = transcribe_chunk(chunk[\"file_path\"], language=language)\n",
    "        if transcription:\n",
    "            transcriptions.append({**chunk, \"transcription\": transcription})\n",
    "            if DEBUG_MODE: # only print if debug mode is on\n",
    "                print(f\"Transcription for {chunk['file_path']}:\")\n",
    "                for segment in transcription[\"segments\"]:\n",
    "                    print(f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "    with open(os.path.join(output_dir, \"transcriptions.json\"), \"w\") as f:\n",
    "        json.dump(transcriptions, f, indent=4)\n",
    "    return transcriptions\n",
    "\n",
    "def clean_transcription(transcriptions_json: str, speaker_names: Dict[str, str]) -> List[str]:\n",
    "    \"\"\"Cleans the transcription JSON to a readable format.\"\"\"\n",
    "    with open(transcriptions_json, 'r') as f:\n",
    "        transcriptions = json.load(f)\n",
    "\n",
    "    cleaned =[]\n",
    "    current_speaker, current_text, current_start, current_end = None, \"\", None, None\n",
    "    for chunk in transcriptions:\n",
    "        if current_speaker != chunk[\"speaker\"]:\n",
    "            if current_speaker:\n",
    "                cleaned.append(f\"\\n{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "            current_speaker, current_text = chunk[\"speaker\"], \"\"\n",
    "            current_start, current_end = chunk[\"start_time\"], chunk[\"end_time\"]\n",
    "        if chunk[\"transcription\"] and chunk[\"transcription\"][\"segments\"]:\n",
    "            current_text += \" \".join(seg[\"text\"] for seg in chunk[\"transcription\"][\"segments\"])\n",
    "    if current_speaker:\n",
    "        cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "    return cleaned\n",
    "\n",
    "def save_transcription_to_file(cleaned_transcriptions: List[str], audio_file_path: str, output_dir: str):\n",
    "    \"\"\"Saves the cleaned transcription to a text file.\"\"\"\n",
    "    output_file = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(audio_file_path))[0]}.txt\")\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(cleaned_transcriptions))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "    output_dir = \"chunks\"\n",
    "\n",
    "    # Make sure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Set up basic logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # Ask the user if they want to specify a language\n",
    "    specify_language = input(\"Do you want to specify a language for transcription? (yes/no): \").lower()\n",
    "    transcription_language = None\n",
    "    if specify_language == \"yes\":\n",
    "        transcription_language = input(\"Enter the language code (e.g., en, fr, es), or leave blank for auto-detection: \").strip()\n",
    "        if not transcription_language:\n",
    "            transcription_language = None # Explicitly set to None if user leaves it blank\n",
    "    elif specify_language == \"no\":\n",
    "        print(\"Using automatic language detection.\")\n",
    "    else:\n",
    "        print(\"Invalid input. Using automatic language detection.\")\n",
    "\n",
    "    try:\n",
    "        pipeline, diarization_result, speaker_names = run_diarization(\n",
    "            audio_file\n",
    "        )  # Get speaker_names\n",
    "        chunk_info_list = chunk_audio(audio_file, diarization_result, output_dir)\n",
    "        print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "        # Save speaker names to a JSON file\n",
    "        speaker_names_path = os.path.join(output_dir, \"speaker_names.json\")\n",
    "        with open(speaker_names_path, \"w\") as f:\n",
    "            json.dump(speaker_names, f, indent=4)\n",
    "        print(f\"Speaker names saved to {speaker_names_path}\")\n",
    "\n",
    "        # You can modify speaker_names here if needed\n",
    "        speaker_names[\"SPEAKER_01\"] = \"Moderator\"\n",
    "        speaker_names[\"SPEAKER_02\"] = \"Participant 1\"\n",
    "\n",
    "        process_and_transcribe_chunks(chunk_info_list, output_dir, language=transcription_language)\n",
    "\n",
    "        transcriptions_json = os.path.join(output_dir, \"transcriptions.json\")\n",
    "        cleaned_transcriptions = clean_transcription(\n",
    "            transcriptions_json, speaker_names\n",
    "        )  # Pass speaker_names\n",
    "        print(\"\\nCleaned Transcriptions:\")\n",
    "        for paragraph in cleaned_transcriptions:\n",
    "            print(paragraph)\n",
    "\n",
    "        save_transcription_to_file(\n",
    "            cleaned_transcriptions, audio_file, output_dir\n",
    "        )  # Save to text file\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 18:38:10,444 - INFO - Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using automatic language detection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "2025-03-23 18:38:38,894 - INFO - Transcribing chunks\\chunk_1.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "Speaker names saved to chunks\\speaker_names.json\n",
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:03<00:00, 300.75frames/s]\n",
      "2025-03-23 18:38:44,593 - INFO - Transcribing chunks\\chunk_2.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:00<00:00, 140.28frames/s]\n",
      "2025-03-23 18:38:47,603 - INFO - Transcribing chunks\\chunk_3.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:01<00:00, 201.33frames/s]\n",
      "2025-03-23 18:38:50,962 - INFO - Transcribing chunks\\chunk_4.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:01<00:00, 510.26frames/s]\n",
      "2025-03-23 18:38:54,607 - INFO - Transcribing chunks\\chunk_5.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 150.02frames/s]\n",
      "2025-03-23 18:38:57,857 - INFO - Transcribing chunks\\chunk_6.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:01<00:00, 113.89frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Transcriptions:\n",
      "\n",
      "Speaker 1 [0.03 - 10.00]:  Anyway, look where we're digressing the rules.  Oh, simple, Emma, you're about to face five questions  of increasing difficulty.  You must answer as quickly as possible.  If you get it correct, you move onto the next round.  Do you know what happens if you get it wrong?\n",
      "\n",
      "Moderator [11.42 - 12.82]:  and correction and embarrassment.\n",
      "\n",
      "Speaker 1 [12.82 - 14.91]:  Do indeed round one. Round 1 astronomers are saying that Saturn's rings are slowly disappearing.  They estimate we only have a few hundred million years left of them.\n",
      "\n",
      "Moderator [25.58 - 27.37]:  I'll earn you a few hundred million.\n",
      "Speaker 1 [27.37 - 28.97]:  But what I want to know?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Annotation\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import whisper_timestamped as whisper\n",
    "\n",
    "class AudioTranscriber:\n",
    "    def __init__(self, audio_file: str, output_dir: str = \"chunks\", hf_token: Optional[str] = None):\n",
    "        self.audio_file = audio_file\n",
    "        self.output_dir = output_dir\n",
    "        self.hf_token = hf_token\n",
    "        self.whisper_model = \"base\"\n",
    "        self.debug_mode = False\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def load_audio(self, audio_file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Loads an audio file using torchaudio.\"\"\"\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "            return waveform, sample_rate\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading audio file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_diarization(\n",
    "        self,\n",
    "        max_speakers: Optional[int] = None,\n",
    "        min_speakers: Optional[int] = None,\n",
    "    ) -> Tuple[Pipeline, Annotation, Dict[str, str]]:\n",
    "        \"\"\"Performs speaker diarization.\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(f\"Using device: {device}\")\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\", use_auth_token=self.hf_token\n",
    "        ).to(device)\n",
    "        waveform, sample_rate = self.load_audio(self.audio_file)\n",
    "        input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "        if max_speakers:\n",
    "            input_data[\"max_speakers\"] = max_speakers\n",
    "        if min_speakers:\n",
    "            input_data[\"min_speakers\"] = min_speakers\n",
    "        diarization = pipeline(input_data)\n",
    "        speaker_labels = set()\n",
    "        for segment, _, label in diarization.itertracks(yield_label=True):\n",
    "            speaker_labels.add(label)\n",
    "        speaker_names = {\n",
    "            label: f\"Speaker {i + 1}\" for i, label in enumerate(sorted(speaker_labels))\n",
    "        }\n",
    "        return pipeline, diarization, speaker_names\n",
    "\n",
    "    def chunk_audio(self, diarization: Annotation) -> List[Dict]:\n",
    "        \"\"\"Chunks audio based on diarization.\"\"\"\n",
    "        audio = AudioSegment.from_file(self.audio_file)\n",
    "        chunks =[]\n",
    "        for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True), 1):\n",
    "            start_ms, end_ms = int(turn.start * 1000), int(turn.end * 1000)\n",
    "            chunk_path = os.path.join(self.output_dir, f\"chunk_{i}.mp3\")\n",
    "            audio[start_ms:end_ms].export(chunk_path, format=\"mp3\")\n",
    "            chunks.append({\"file_path\": chunk_path, \"speaker\": speaker, \"start_time\": turn.start, \"end_time\": turn.end})\n",
    "        return chunks\n",
    "\n",
    "    def transcribe_chunk(self, audio_file_path: str, language: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Transcribes an audio chunk.\"\"\"\n",
    "        try:\n",
    "            model = whisper.load_model(self.whisper_model)\n",
    "            audio = whisper.load_audio(audio_file_path)\n",
    "            return whisper.transcribe(model, audio, language=language)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Transcription error: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def process_and_transcribe_chunks(self, chunks: List[Dict], language: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"Processes and saves transcriptions.\"\"\"\n",
    "        transcriptions =[]\n",
    "        for chunk in chunks:\n",
    "            logging.info(f\"Transcribing {chunk['file_path']}\")\n",
    "            transcription = self.transcribe_chunk(chunk[\"file_path\"], language=language)\n",
    "            if transcription:\n",
    "                transcriptions.append({**chunk, \"transcription\": transcription})\n",
    "                if self.debug_mode: # only print if debug mode is on\n",
    "                    print(f\"Transcription for {chunk['file_path']}:\")\n",
    "                    for segment in transcription[\"segments\"]:\n",
    "                        print(f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "        transcriptions_json_path = os.path.join(self.output_dir, \"transcriptions.json\")\n",
    "        with open(transcriptions_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        return transcriptions\n",
    "\n",
    "    def clean_transcription(self, transcriptions_json: str, speaker_names: Dict[str, str]) -> List[str]:\n",
    "        \"\"\"Cleans the transcription JSON to a readable format.\"\"\"\n",
    "        with open(transcriptions_json, 'r') as f:\n",
    "            transcriptions = json.load(f)\n",
    "\n",
    "        cleaned =[]\n",
    "        current_speaker, current_text, current_start, current_end = None, \"\", None, None\n",
    "        for chunk in transcriptions:\n",
    "            if current_speaker != chunk[\"speaker\"]:\n",
    "                if current_speaker:\n",
    "                    cleaned.append(f\"\\n{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "                current_speaker, current_text = chunk[\"speaker\"], \"\"\n",
    "                current_start, current_end = chunk[\"start_time\"], chunk[\"end_time\"]\n",
    "            if chunk[\"transcription\"] and chunk[\"transcription\"][\"segments\"]:\n",
    "                current_text += \" \".join(seg[\"text\"] for seg in chunk[\"transcription\"][\"segments\"])\n",
    "        if current_speaker:\n",
    "            cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "        return cleaned\n",
    "\n",
    "    def save_transcription_to_file(self, cleaned_transcriptions: List[str]):\n",
    "        \"\"\"Saves the cleaned transcription to a text file.\"\"\"\n",
    "        output_file = os.path.join(self.output_dir, f\"{os.path.splitext(os.path.basename(self.audio_file))[0]}.txt\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(cleaned_transcriptions))\n",
    "\n",
    "    def process_audio(self, language: Optional[str] = None):\n",
    "        \"\"\"Orchestrates the audio processing pipeline.\"\"\"\n",
    "        try:\n",
    "            pipeline, diarization_result, speaker_names = self.run_diarization()\n",
    "            chunk_info_list = self.chunk_audio(diarization_result)\n",
    "            print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "            speaker_names_path = os.path.join(self.output_dir, \"speaker_names.json\")\n",
    "            with open(speaker_names_path, \"w\") as f:\n",
    "                json.dump(speaker_names, f, indent=4)\n",
    "            print(f\"Speaker names saved to {speaker_names_path}\")\n",
    "\n",
    "            # You can modify speaker_names here if needed\n",
    "            speaker_names[\"SPEAKER_01\"] = \"Moderator\"\n",
    "            speaker_names[\"SPEAKER_02\"] = \"Participant 1\"\n",
    "\n",
    "            self.process_and_transcribe_chunks(chunk_info_list, language=language)\n",
    "\n",
    "            transcriptions_json_path = os.path.join(self.output_dir, \"transcriptions.json\")\n",
    "            cleaned_transcriptions = self.clean_transcription(transcriptions_json_path, speaker_names)\n",
    "            print(\"\\nCleaned Transcriptions:\")\n",
    "            for paragraph in cleaned_transcriptions:\n",
    "                print(paragraph)\n",
    "\n",
    "            self.save_transcription_to_file(cleaned_transcriptions)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred: {e}\")\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "    output_dir = \"chunks\"\n",
    "\n",
    "    # Load HF_TOKEN if config.py exists\n",
    "    hf_token = None\n",
    "    try:\n",
    "        from config import HF_TOKEN as config_token\n",
    "        hf_token = config_token\n",
    "    except ImportError:\n",
    "        logging.warning(\n",
    "            \"config.py not found or HF_TOKEN not defined. Diarization may not work.\"\n",
    "            \" Please create a config.py file with HF_TOKEN = 'YOUR_HUGGINGFACE_TOKEN'\"\n",
    "        )\n",
    "\n",
    "    transcriber = AudioTranscriber(audio_file, output_dir, hf_token)\n",
    "\n",
    "    # Ask the user if they want to specify a language\n",
    "    specify_language = input(\"Do you want to specify a language for transcription? (yes/no): \").lower()\n",
    "    transcription_language = None\n",
    "    if specify_language == \"yes\":\n",
    "        transcription_language = input(\"Enter the language code (e.g., en, fr, es), or leave blank for auto-detection: \").strip()\n",
    "        if not transcription_language:\n",
    "            transcription_language = None # Explicitly set to None if user leaves it blank\n",
    "    elif specify_language == \"no\":\n",
    "        print(\"Using automatic language detection.\")\n",
    "    else:\n",
    "        print(\"Invalid input. Using automatic language detection.\")\n",
    "\n",
    "    transcriber.process_audio(language=transcription_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 19:59:20,944 - INFO - Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using automatic language detection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "2025-03-23 19:59:51,202 - INFO - Transcribing chunks\\chunk_1.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "Speaker names saved to chunks\\speaker_names.json\n",
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:03<00:00, 294.38frames/s]\n",
      "2025-03-23 19:59:56,951 - INFO - Transcribing chunks\\chunk_2.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:01<00:00, 122.74frames/s]\n",
      "2025-03-23 20:00:00,084 - INFO - Transcribing chunks\\chunk_3.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:01<00:00, 118.62frames/s]\n",
      "2025-03-23 20:00:04,073 - INFO - Transcribing chunks\\chunk_4.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:02<00:00, 332.94frames/s]\n",
      "2025-03-23 20:00:09,485 - INFO - Transcribing chunks\\chunk_5.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 139.06frames/s]\n",
      "2025-03-23 20:00:12,930 - INFO - Transcribing chunks\\chunk_6.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:01<00:00, 118.85frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Transcriptions:\n",
      "\n",
      "Speaker 1 [0.03 - 10.00]:  Anyway, look where we're digressing the rules.  Oh, simple, Emma, you're about to face five questions  of increasing difficulty.  You must answer as quickly as possible.  If you get it correct, you move onto the next round.  Do you know what happens if you get it wrong?\n",
      "\n",
      "Moderator [11.42 - 12.82]:  and correction and embarrassment.\n",
      "\n",
      "Speaker 1 [12.82 - 14.91]:  Do indeed round one. Round 1 astronomers are saying that Saturn's rings are slowly disappearing.  They estimate we only have a few hundred million years left of them.\n",
      "\n",
      "Moderator [25.58 - 27.37]:  I'll earn you a few hundred million.\n",
      "Speaker 1 [27.37 - 28.97]:  But what I want to know?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Annotation\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import whisper_timestamped as whisper\n",
    "\n",
    "class AudioTranscriber:\n",
    "    def __init__(self, audio_file: str, output_dir: str = \"chunks\", hf_token: Optional[str] = None, skip_diarization: bool = False):\n",
    "        self.audio_file = audio_file\n",
    "        self.output_dir = output_dir\n",
    "        self.hf_token = hf_token\n",
    "        self.whisper_model = \"base\"\n",
    "        self.debug_mode = False\n",
    "        self.skip_diarization = skip_diarization\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def load_audio(self, audio_file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Loads an audio file using torchaudio.\"\"\"\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "            return waveform, sample_rate\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading audio file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_diarization(\n",
    "        self,\n",
    "        max_speakers: Optional[int] = None,\n",
    "        min_speakers: Optional[int] = None,\n",
    "    ) -> Tuple[Pipeline, Annotation, Dict[str, str]]:\n",
    "        \"\"\"Performs speaker diarization.\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(f\"Using device: {device}\")\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\", use_auth_token=self.hf_token\n",
    "        ).to(device)\n",
    "        waveform, sample_rate = self.load_audio(self.audio_file)\n",
    "        input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "        if max_speakers:\n",
    "            input_data[\"max_speakers\"] = max_speakers\n",
    "        if min_speakers:\n",
    "            input_data[\"min_speakers\"] = min_speakers\n",
    "        diarization = pipeline(input_data)\n",
    "        speaker_labels = set()\n",
    "        for segment, _, label in diarization.itertracks(yield_label=True):\n",
    "            speaker_labels.add(label)\n",
    "        speaker_names = {\n",
    "            label: f\"Speaker {i + 1}\" for i, label in enumerate(sorted(speaker_labels))\n",
    "        }\n",
    "        return pipeline, diarization, speaker_names\n",
    "\n",
    "    def chunk_audio(self, diarization: Annotation) -> List[Dict]:\n",
    "        \"\"\"Chunks audio based on diarization.\"\"\"\n",
    "        audio = AudioSegment.from_file(self.audio_file)\n",
    "        chunks =[]\n",
    "        for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True), 1):\n",
    "            start_ms, end_ms = int(turn.start * 1000), int(turn.end * 1000)\n",
    "            chunk_path = os.path.join(self.output_dir, f\"chunk_{i}.mp3\")\n",
    "            audio[start_ms:end_ms].export(chunk_path, format=\"mp3\")\n",
    "            chunks.append({\"file_path\": chunk_path, \"speaker\": speaker, \"start_time\": turn.start, \"end_time\": turn.end})\n",
    "        return chunks\n",
    "\n",
    "    def transcribe_chunk(self, audio_file_path: str, language: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Transcribes an audio chunk.\"\"\"\n",
    "        try:\n",
    "            model = whisper.load_model(self.whisper_model)\n",
    "            audio = whisper.load_audio(audio_file_path)\n",
    "            return whisper.transcribe(model, audio, language=language)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Transcription error: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def process_and_transcribe_chunks(self, chunks: List[Dict], language: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"Processes and saves transcriptions for individual chunks.\"\"\"\n",
    "        transcriptions =[]\n",
    "        for chunk in chunks:\n",
    "            logging.info(f\"Transcribing {chunk['file_path']}\")\n",
    "            transcription = self.transcribe_chunk(chunk[\"file_path\"], language=language)\n",
    "            if transcription:\n",
    "                transcriptions.append({**chunk, \"transcription\": transcription})\n",
    "                if self.debug_mode: # only print if debug mode is on\n",
    "                    print(f\"Transcription for {chunk['file_path']}:\")\n",
    "                    for segment in transcription[\"segments\"]:\n",
    "                        print(f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "        transcriptions_json_path = os.path.join(self.output_dir, \"transcriptions.json\")\n",
    "        with open(transcriptions_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        return transcriptions\n",
    "\n",
    "    def transcribe_whole_audio(self, language: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Transcribes the entire audio file without diarization.\"\"\"\n",
    "        logging.info(f\"Transcribing the entire audio file: {self.audio_file}\")\n",
    "        return self.transcribe_chunk(self.audio_file, language=language)\n",
    "\n",
    "    def clean_transcription(self, transcriptions_json: str, speaker_names: Dict[str, str]) -> List[str]:\n",
    "        \"\"\"Cleans the transcription JSON to a readable format (for diarized audio).\"\"\"\n",
    "        with open(transcriptions_json, 'r') as f:\n",
    "            transcriptions = json.load(f)\n",
    "\n",
    "        cleaned =[]\n",
    "        current_speaker, current_text, current_start, current_end = None, \"\", None, None\n",
    "        for chunk in transcriptions:\n",
    "            if current_speaker != chunk[\"speaker\"]:\n",
    "                if current_speaker:\n",
    "                    cleaned.append(f\"\\n{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "                current_speaker, current_text = chunk[\"speaker\"], \"\"\n",
    "                current_start, current_end = chunk[\"start_time\"], chunk[\"end_time\"]\n",
    "            if chunk[\"transcription\"] and chunk[\"transcription\"][\"segments\"]:\n",
    "                current_text += \" \".join(seg[\"text\"] for seg in chunk[\"transcription\"][\"segments\"])\n",
    "        if current_speaker:\n",
    "            cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "        return cleaned\n",
    "\n",
    "    def clean_whole_transcription(self, whole_transcription: Dict) -> List[str]:\n",
    "        \"\"\"Cleans the whole transcription output to a readable format (without diarization).\"\"\"\n",
    "        cleaned =[]\n",
    "        if whole_transcription and whole_transcription.get(\"segments\"):\n",
    "            for segment in whole_transcription[\"segments\"]:\n",
    "                cleaned.append(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "        return cleaned\n",
    "\n",
    "    def save_transcription_to_file(self, cleaned_transcriptions: List[str], filename=\"transcription.txt\"):\n",
    "        \"\"\"Saves the cleaned transcription to a text file.\"\"\"\n",
    "        output_file = os.path.join(self.output_dir, f\"{os.path.splitext(os.path.basename(self.audio_file))[0]}_{filename}\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(cleaned_transcriptions))\n",
    "\n",
    "    def process_audio(self, language: Optional[str] = None):\n",
    "        \"\"\"Orchestrates the audio processing pipeline.\"\"\"\n",
    "        try:\n",
    "            if self.skip_diarization:\n",
    "                print(\"Skipping diarization and transcribing the whole audio file.\")\n",
    "                whole_transcription = self.transcribe_whole_audio(language=language)\n",
    "                if whole_transcription:\n",
    "                    cleaned_transcriptions = self.clean_whole_transcription(whole_transcription)\n",
    "                    print(\"\\nTranscription:\")\n",
    "                    for line in cleaned_transcriptions:\n",
    "                        print(line)\n",
    "                    self.save_transcription_to_file(cleaned_transcriptions, filename=\"whole_transcription.txt\")\n",
    "                else:\n",
    "                    print(\"Transcription failed.\")\n",
    "            else:\n",
    "                pipeline, diarization_result, speaker_names = self.run_diarization()\n",
    "                chunk_info_list = self.chunk_audio(diarization_result)\n",
    "                print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "                speaker_names_path = os.path.join(self.output_dir, \"speaker_names.json\")\n",
    "                with open(speaker_names_path, \"w\") as f:\n",
    "                    json.dump(speaker_names, f, indent=4)\n",
    "                print(f\"Speaker names saved to {speaker_names_path}\")\n",
    "\n",
    "                # You can modify speaker_names here if needed\n",
    "                speaker_names[\"SPEAKER_01\"] = \"Moderator\"\n",
    "                speaker_names[\"SPEAKER_02\"] = \"Participant 1\"\n",
    "\n",
    "                self.process_and_transcribe_chunks(chunk_info_list, language=language)\n",
    "\n",
    "                transcriptions_json_path = os.path.join(self.output_dir, \"transcriptions.json\")\n",
    "                cleaned_transcriptions = self.clean_transcription(transcriptions_json_path, speaker_names)\n",
    "                print(\"\\nCleaned Transcriptions:\")\n",
    "                for paragraph in cleaned_transcriptions:\n",
    "                    print(paragraph)\n",
    "\n",
    "                self.save_transcription_to_file(cleaned_transcriptions, filename=\"diarized_transcription.txt\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred: {e}\")\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "    output_dir = \"chunks\"\n",
    "\n",
    "    # Load HF_TOKEN if config.py exists\n",
    "    hf_token = None\n",
    "    try:\n",
    "        from config import HF_TOKEN as config_token\n",
    "        hf_token = config_token\n",
    "    except ImportError:\n",
    "        logging.warning(\n",
    "            \"config.py not found or HF_TOKEN not defined. Diarization may not work.\"\n",
    "            \" Please create a config.py file with HF_TOKEN = 'YOUR_HUGGINGFACE_TOKEN'\"\n",
    "        )\n",
    "\n",
    "    # Ask the user if they want to skip diarization\n",
    "    skip_diarization_input = input(\"Do you want to skip speaker diarization and transcribe the whole audio directly? (yes/no): \").lower()\n",
    "    skip_diarization = skip_diarization_input == \"yes\"\n",
    "\n",
    "    transcriber = AudioTranscriber(audio_file, output_dir, hf_token, skip_diarization=skip_diarization)\n",
    "\n",
    "    # Ask the user if they want to specify a language\n",
    "    specify_language = input(\"Do you want to specify a language for transcription? (yes/no): \").lower()\n",
    "    transcription_language = None\n",
    "    if specify_language == \"yes\":\n",
    "        transcription_language = input(\"Enter the language code (e.g., en, fr, es), or leave blank for auto-detection: \").strip()\n",
    "        if not transcription_language:\n",
    "            transcription_language = None # Explicitly set to None if user leaves it blank\n",
    "    elif specify_language == \"no\":\n",
    "        print(\"Using automatic language detection.\")\n",
    "    else:\n",
    "        print(\"Invalid input. Using automatic language detection.\")\n",
    "\n",
    "    transcriber.process_audio(language=transcription_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:16:07,674 - INFO - Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using automatic language detection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:16:09,511 - INFO - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "2025-03-23 21:16:09,511 - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1007: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "2025-03-23 21:16:35,124 - INFO - Transcribing chunks\\chunk_1.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "Speaker names saved to chunks\\speaker_names.json\n",
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:03<00:00, 284.36frames/s]\n",
      "2025-03-23 21:16:41,114 - INFO - Transcribing chunks\\chunk_2.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:02<00:00, 53.29frames/s]\n",
      "2025-03-23 21:16:46,131 - INFO - Transcribing chunks\\chunk_3.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:01<00:00, 190.17frames/s]\n",
      "2025-03-23 21:16:49,268 - INFO - Transcribing chunks\\chunk_4.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:01<00:00, 434.81frames/s]\n",
      "2025-03-23 21:16:53,477 - INFO - Transcribing chunks\\chunk_5.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 161.09frames/s]\n",
      "2025-03-23 21:16:56,608 - INFO - Transcribing chunks\\chunk_6.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:01<00:00, 120.87frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Transcriptions:\n",
      "Speaker 1 [0.03 - 10.00]:  Anyway, look where we're digressing the rules.  Oh, simple, Emma, you're about to face five questions  of increasing difficulty.  You must answer as quickly as possible.  If you get it correct, you move onto the next round.  Do you know what happens if you get it wrong?\n",
      "Moderator [11.42 - 12.82]:  and correction and embarrassment.\n",
      "Speaker 1 [12.82 - 14.91]:  Do indeed round one. Round 1 astronomers are saying that Saturn's rings are slowly disappearing.  They estimate we only have a few hundred million years left of them.\n",
      "Moderator [25.58 - 27.37]:  I'll earn you a few hundred million.\n",
      "Speaker 1 [27.37 - 28.97]:  But what I want to know?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Annotation\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import whisper_timestamped as whisper\n",
    "\n",
    "class AudioTranscriber:\n",
    "    def __init__(self, audio_file: str, output_dir: str = \"chunks\", hf_token: Optional[str] = None, skip_diarization: bool = False):\n",
    "        self.audio_file = audio_file\n",
    "        self.output_dir = output_dir\n",
    "        self.hf_token = hf_token\n",
    "        self.whisper_model = \"base\"\n",
    "        self.debug_mode = False\n",
    "        self.skip_diarization = skip_diarization\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def load_audio(self, audio_file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Loads an audio file using torchaudio.\"\"\"\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "            return waveform, sample_rate\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading audio file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_diarization(\n",
    "        self,\n",
    "        max_speakers: Optional[int] = None,\n",
    "        min_speakers: Optional[int] = None,\n",
    "    ) -> Tuple[Pipeline, Annotation, Dict[str, str]]:\n",
    "        \"\"\"Performs speaker diarization.\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(f\"Using device: {device}\")\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\", use_auth_token=self.hf_token\n",
    "        ).to(device)\n",
    "        waveform, sample_rate = self.load_audio(self.audio_file)\n",
    "        input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "        if max_speakers:\n",
    "            input_data[\"max_speakers\"] = max_speakers\n",
    "        if min_speakers:\n",
    "            input_data[\"min_speakers\"] = min_speakers\n",
    "        diarization = pipeline(input_data)\n",
    "        speaker_labels = set()\n",
    "        for segment, _, label in diarization.itertracks(yield_label=True):\n",
    "            speaker_labels.add(label)\n",
    "        speaker_names = {\n",
    "            label: f\"Speaker {i + 1}\" for i, label in enumerate(sorted(speaker_labels))\n",
    "        }\n",
    "        return pipeline, diarization, speaker_names\n",
    "\n",
    "    def chunk_audio(self, diarization: Annotation) -> List[Dict]:\n",
    "        \"\"\"Chunks audio based on diarization.\"\"\"\n",
    "        audio = AudioSegment.from_file(self.audio_file)\n",
    "        chunks = []\n",
    "        for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True), 1):\n",
    "            start_ms, end_ms = int(turn.start * 1000), int(turn.end * 1000)\n",
    "            chunk_path = os.path.join(self.output_dir, f\"chunk_{i}.mp3\")\n",
    "            audio[start_ms:end_ms].export(chunk_path, format=\"mp3\")\n",
    "            chunks.append({\"file_path\": chunk_path, \"speaker\": speaker, \"start_time\": turn.start, \"end_time\": turn.end})\n",
    "        return chunks\n",
    "\n",
    "    def transcribe_chunk(self, audio_file_path: str, language: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Transcribes an audio chunk.\"\"\"\n",
    "        try:\n",
    "            model = whisper.load_model(self.whisper_model)\n",
    "            audio = whisper.load_audio(audio_file_path)\n",
    "            return whisper.transcribe(model, audio, language=language)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Transcription error: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def process_and_transcribe_chunks(self, chunks: List[Dict], language: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"Processes and saves transcriptions for individual chunks.\"\"\"\n",
    "        transcriptions = []\n",
    "        for chunk in chunks:\n",
    "            logging.info(f\"Transcribing {chunk['file_path']}\")\n",
    "            transcription = self.transcribe_chunk(chunk[\"file_path\"], language=language)\n",
    "            if transcription:\n",
    "                transcriptions.append({**chunk, \"transcription\": transcription})\n",
    "                if self.debug_mode: # only print if debug mode is on\n",
    "                    print(f\"Transcription for {chunk['file_path']}:\")\n",
    "                    for segment in transcription[\"segments\"]:\n",
    "                        print(f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "        transcriptions_json_path = os.path.join(self.output_dir, \"transcriptions.json\")\n",
    "        with open(transcriptions_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        return transcriptions\n",
    "\n",
    "    def transcribe_whole_audio(self, language: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Transcribes the entire audio file without diarization.\"\"\"\n",
    "        logging.info(f\"Transcribing the entire audio file: {self.audio_file}\")\n",
    "        return self.transcribe_chunk(self.audio_file, language=language)\n",
    "\n",
    "    def clean_transcription(self, transcriptions_json: str, speaker_names: Dict[str, str]) -> List[str]:\n",
    "        \"\"\"Cleans the transcription JSON to a readable format (for diarized audio).\"\"\"\n",
    "        with open(transcriptions_json, 'r') as f:\n",
    "            transcriptions = json.load(f)\n",
    "\n",
    "        cleaned = []\n",
    "        current_speaker, current_text, current_start, current_end = None, \"\", None, None\n",
    "        for i, chunk in enumerate(transcriptions):\n",
    "            if current_speaker != chunk[\"speaker\"]:\n",
    "                if current_speaker is not None:\n",
    "                    cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "                    if i < len(transcriptions) and transcriptions[i][\"speaker\"] != transcriptions[i-1][\"speaker\"]:\n",
    "                        pass # Avoid adding extra blank line here, handle it before the next speaker\n",
    "                current_speaker, current_text = chunk[\"speaker\"], \"\"\n",
    "                current_start, current_end = chunk[\"start_time\"], chunk[\"end_time\"]\n",
    "                if i > 0 and transcriptions[i][\"speaker\"] != transcriptions[i-1][\"speaker\"]:\n",
    "                    cleaned.append(\"\") # Add a blank line before a new speaker (after the first)\n",
    "            if chunk[\"transcription\"] and chunk[\"transcription\"][\"segments\"]:\n",
    "                current_text += \" \".join(seg[\"text\"] for seg in chunk[\"transcription\"][\"segments\"])\n",
    "        if current_speaker:\n",
    "            cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "        return [line for line in cleaned if line.strip() != \"\"]\n",
    "\n",
    "    def clean_whole_transcription(self, whole_transcription: Dict) -> List[str]:\n",
    "        \"\"\"Cleans the whole transcription output to a readable format (without diarization).\"\"\"\n",
    "        cleaned = []\n",
    "        if whole_transcription and whole_transcription.get(\"segments\"):\n",
    "            for segment in whole_transcription[\"segments\"]:\n",
    "                cleaned.append(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "        return cleaned\n",
    "\n",
    "    def save_transcription_to_file(self, cleaned_transcriptions: List[str], filename=\"transcription.txt\"):\n",
    "        \"\"\"Saves the cleaned transcription to a text file.\"\"\"\n",
    "        output_file = os.path.join(self.output_dir, f\"{os.path.splitext(os.path.basename(self.audio_file))[0]}_{filename}\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(cleaned_transcriptions))\n",
    "\n",
    "    def process_audio(self, language: Optional[str] = None):\n",
    "        \"\"\"Orchestrates the audio processing pipeline.\"\"\"\n",
    "        try:\n",
    "            if self.skip_diarization:\n",
    "                print(\"Skipping diarization and transcribing the whole audio file.\")\n",
    "                whole_transcription = self.transcribe_whole_audio(language=language)\n",
    "                if whole_transcription:\n",
    "                    cleaned_transcriptions = self.clean_whole_transcription(whole_transcription)\n",
    "                    print(\"\\nTranscription:\")\n",
    "                    for line in cleaned_transcriptions:\n",
    "                        print(line)\n",
    "                    self.save_transcription_to_file(cleaned_transcriptions, filename=\"whole_transcription.txt\")\n",
    "                else:\n",
    "                    print(\"Transcription failed.\")\n",
    "            else:\n",
    "                pipeline, diarization_result, speaker_names = self.run_diarization()\n",
    "                chunk_info_list = self.chunk_audio(diarization_result)\n",
    "                print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "                speaker_names_path = os.path.join(self.output_dir, \"speaker_names.json\")\n",
    "                with open(speaker_names_path, \"w\") as f:\n",
    "                    json.dump(speaker_names, f, indent=4)\n",
    "                print(f\"Speaker names saved to {speaker_names_path}\")\n",
    "\n",
    "                # You can modify speaker_names here if needed\n",
    "                speaker_names[\"SPEAKER_01\"] = \"Moderator\"\n",
    "                speaker_names[\"SPEAKER_02\"] = \"Participant 1\"\n",
    "\n",
    "                self.process_and_transcribe_chunks(chunk_info_list, language=language)\n",
    "\n",
    "                transcriptions_json_path = os.path.join(self.output_dir, \"transcriptions.json\")\n",
    "                cleaned_transcriptions = self.clean_transcription(transcriptions_json_path, speaker_names)\n",
    "                print(\"Cleaned Transcriptions:\")\n",
    "                for paragraph in cleaned_transcriptions:\n",
    "                    print(paragraph)\n",
    "\n",
    "                self.save_transcription_to_file(cleaned_transcriptions, filename=\"diarized_transcription.txt\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred: {e}\")\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "    output_dir = \"chunks\"\n",
    "\n",
    "    # Load HF_TOKEN if config.py exists\n",
    "    hf_token = None\n",
    "    try:\n",
    "        from config import HF_TOKEN as config_token\n",
    "        hf_token = config_token\n",
    "    except ImportError:\n",
    "        logging.warning(\n",
    "            \"config.py not found or HF_TOKEN not defined. Diarization may not work.\"\n",
    "            \" Please create a config.py file with HF_TOKEN = 'YOUR_HUGGINGFACE_TOKEN'\"\n",
    "        )\n",
    "\n",
    "    # Ask the user if they want to skip diarization\n",
    "    skip_diarization_input = input(\"Do you want to skip speaker diarization and transcribe the whole audio directly? (yes/no): \").lower()\n",
    "    skip_diarization = skip_diarization_input == \"yes\"\n",
    "\n",
    "    transcriber = AudioTranscriber(audio_file, output_dir, hf_token, skip_diarization=skip_diarization)\n",
    "\n",
    "    # Ask the user if they want to specify a language\n",
    "    specify_language = input(\"Do you want to specify a language for transcription? (yes/no): \").lower()\n",
    "    transcription_language = None\n",
    "    if specify_language == \"yes\":\n",
    "        transcription_language = input(\"Enter the language code (e.g., en, fr, es), or leave blank for auto-detection: \").strip()\n",
    "        if not transcription_language:\n",
    "            transcription_language = None # Explicitly set to None if user leaves it blank\n",
    "    elif specify_language == \"no\":\n",
    "        print(\"Using automatic language detection.\")\n",
    "    else:\n",
    "        print(\"Invalid input. Using automatic language detection.\")\n",
    "\n",
    "    transcriber.process_audio(language=transcription_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n",
      "\n",
      "Available Whisper models: tiny, base, small, medium, large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:26:08,398 - INFO - Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using automatic language detection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:26:10,170 - INFO - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "2025-03-23 21:26:10,171 - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1007: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "2025-03-23 21:26:36,673 - INFO - Transcribing chunks\\chunk_1.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "Speaker names saved to chunks\\speaker_names.json\n",
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:03<00:00, 259.89frames/s]\n",
      "2025-03-23 21:26:43,126 - INFO - Transcribing chunks\\chunk_2.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:01<00:00, 135.97frames/s]\n",
      "2025-03-23 21:26:46,279 - INFO - Transcribing chunks\\chunk_3.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:01<00:00, 153.39frames/s]\n",
      "2025-03-23 21:26:49,841 - INFO - Transcribing chunks\\chunk_4.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:01<00:00, 439.31frames/s]\n",
      "2025-03-23 21:26:54,027 - INFO - Transcribing chunks\\chunk_5.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 139.01frames/s]\n",
      "2025-03-23 21:26:57,494 - INFO - Transcribing chunks\\chunk_6.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:01<00:00, 118.67frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Transcriptions:\n",
      "Speaker 1 [0.03 - 10.00]:  Anyway, look where we're digressing the rules.  Oh, simple, Emma, you're about to face five questions  of increasing difficulty.  You must answer as quickly as possible.  If you get it correct, you move onto the next round.  Do you know what happens if you get it wrong?\n",
      "Moderator [11.42 - 12.82]:  and correction and embarrassment.\n",
      "Speaker 1 [12.82 - 14.91]:  Do indeed round one. Round 1 astronomers are saying that Saturn's rings are slowly disappearing.  They estimate we only have a few hundred million years left of them.\n",
      "Moderator [25.58 - 27.37]:  I'll earn you a few hundred million.\n",
      "Speaker 1 [27.37 - 28.97]:  But what I want to know?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Annotation\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import whisper_timestamped as whisper\n",
    "\n",
    "class AudioTranscriber:\n",
    "    def __init__(self, audio_file: str, output_dir: str = \"chunks\", hf_token: Optional[str] = None, skip_diarization: bool = False, whisper_model: str = \"base\"):\n",
    "        self.audio_file = audio_file\n",
    "        self.output_dir = output_dir\n",
    "        self.hf_token = hf_token\n",
    "        self.whisper_model = whisper_model\n",
    "        self.debug_mode = False\n",
    "        self.skip_diarization = skip_diarization\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def load_audio(self, audio_file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Loads an audio file using torchaudio.\"\"\"\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "            return waveform, sample_rate\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading audio file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_diarization(\n",
    "        self,\n",
    "        max_speakers: Optional[int] = None,\n",
    "        min_speakers: Optional[int] = None,\n",
    "    ) -> Tuple[Pipeline, Annotation, Dict[str, str]]:\n",
    "        \"\"\"Performs speaker diarization.\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(f\"Using device: {device}\")\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\", use_auth_token=self.hf_token\n",
    "        ).to(device)\n",
    "        waveform, sample_rate = self.load_audio(self.audio_file)\n",
    "        input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "        if max_speakers:\n",
    "            input_data[\"max_speakers\"] = max_speakers\n",
    "        if min_speakers:\n",
    "            input_data[\"min_speakers\"] = min_speakers\n",
    "        diarization = pipeline(input_data)\n",
    "        speaker_labels = set()\n",
    "        for segment, _, label in diarization.itertracks(yield_label=True):\n",
    "            speaker_labels.add(label)\n",
    "        speaker_names = {\n",
    "            label: f\"Speaker {i + 1}\" for i, label in enumerate(sorted(speaker_labels))\n",
    "        }\n",
    "        return pipeline, diarization, speaker_names\n",
    "\n",
    "    def chunk_audio(self, diarization: Annotation) -> List[Dict]:\n",
    "        \"\"\"Chunks audio based on diarization.\"\"\"\n",
    "        audio = AudioSegment.from_file(self.audio_file)\n",
    "        chunks = []\n",
    "        for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True), 1):\n",
    "            start_ms, end_ms = int(turn.start * 1000), int(turn.end * 1000)\n",
    "            chunk_path = os.path.join(self.output_dir, f\"chunk_{i}.mp3\")\n",
    "            audio[start_ms:end_ms].export(chunk_path, format=\"mp3\")\n",
    "            chunks.append({\"file_path\": chunk_path, \"speaker\": speaker, \"start_time\": turn.start, \"end_time\": turn.end})\n",
    "        return chunks\n",
    "\n",
    "    def transcribe_chunk(self, audio_file_path: str, language: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Transcribes an audio chunk.\"\"\"\n",
    "        try:\n",
    "            model = whisper.load_model(self.whisper_model)\n",
    "            audio = whisper.load_audio(audio_file_path)\n",
    "            return whisper.transcribe(model, audio, language=language)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Transcription error: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def process_and_transcribe_chunks(self, chunks: List[Dict], language: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"Processes and saves transcriptions for individual chunks.\"\"\"\n",
    "        transcriptions = []\n",
    "        for chunk in chunks:\n",
    "            logging.info(f\"Transcribing {chunk['file_path']}\")\n",
    "            transcription = self.transcribe_chunk(chunk[\"file_path\"], language=language)\n",
    "            if transcription:\n",
    "                transcriptions.append({**chunk, \"transcription\": transcription})\n",
    "                if self.debug_mode: # only print if debug mode is on\n",
    "                    print(f\"Transcription for {chunk['file_path']}:\")\n",
    "                    for segment in transcription[\"segments\"]:\n",
    "                        print(f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "        transcriptions_json_path = os.path.join(self.output_dir, \"transcriptions.json\")\n",
    "        with open(transcriptions_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        return transcriptions\n",
    "\n",
    "    def transcribe_whole_audio(self, language: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Transcribes the entire audio file without diarization.\"\"\"\n",
    "        logging.info(f\"Transcribing the entire audio file: {self.audio_file}\")\n",
    "        return self.transcribe_chunk(self.audio_file, language=language)\n",
    "\n",
    "    def clean_transcription(self, transcriptions_json: str, speaker_names: Dict[str, str]) -> List[str]:\n",
    "        \"\"\"Cleans the transcription JSON to a readable format (for diarized audio).\"\"\"\n",
    "        with open(transcriptions_json, 'r') as f:\n",
    "            transcriptions = json.load(f)\n",
    "\n",
    "        cleaned = []\n",
    "        current_speaker, current_text, current_start, current_end = None, \"\", None, None\n",
    "        for i, chunk in enumerate(transcriptions):\n",
    "            if current_speaker != chunk[\"speaker\"]:\n",
    "                if current_speaker is not None:\n",
    "                    cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "                current_speaker, current_text = chunk[\"speaker\"], \"\"\n",
    "                current_start, current_end = chunk[\"start_time\"], chunk[\"end_time\"]\n",
    "                if i > 0 and transcriptions[i][\"speaker\"] != transcriptions[i-1][\"speaker\"]:\n",
    "                    cleaned.append(\"\") # Add a blank line before a new speaker (after the first)\n",
    "            if chunk[\"transcription\"] and chunk[\"transcription\"][\"segments\"]:\n",
    "                current_text += \" \".join(seg[\"text\"] for seg in chunk[\"transcription\"][\"segments\"])\n",
    "        if current_speaker:\n",
    "            cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "        return [line for line in cleaned if line.strip() != \"\"]\n",
    "\n",
    "    def clean_whole_transcription(self, whole_transcription: Dict) -> List[str]:\n",
    "        \"\"\"Cleans the whole transcription output to a readable format (without diarization).\"\"\"\n",
    "        cleaned = []\n",
    "        if whole_transcription and whole_transcription.get(\"segments\"):\n",
    "            for segment in whole_transcription[\"segments\"]:\n",
    "                cleaned.append(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "        return cleaned\n",
    "\n",
    "    def save_transcription_to_file(self, cleaned_transcriptions: List[str], filename=\"transcription.txt\"):\n",
    "        \"\"\"Saves the cleaned transcription to a text file.\"\"\"\n",
    "        output_file = os.path.join(self.output_dir, f\"{os.path.splitext(os.path.basename(self.audio_file))[0]}_{filename}\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(cleaned_transcriptions))\n",
    "\n",
    "    def process_audio(self, language: Optional[str] = None, min_speakers: Optional[int] = None, max_speakers: Optional[int] = None):\n",
    "        \"\"\"Orchestrates the audio processing pipeline.\"\"\"\n",
    "        try:\n",
    "            if self.skip_diarization:\n",
    "                print(\"Skipping diarization and transcribing the whole audio file.\")\n",
    "                whole_transcription = self.transcribe_whole_audio(language=language)\n",
    "                if whole_transcription:\n",
    "                    cleaned_transcriptions = self.clean_whole_transcription(whole_transcription)\n",
    "                    print(\"\\nTranscription:\")\n",
    "                    for line in cleaned_transcriptions:\n",
    "                        print(line)\n",
    "                    self.save_transcription_to_file(cleaned_transcriptions, filename=\"whole_transcription.txt\")\n",
    "                else:\n",
    "                    print(\"Transcription failed.\")\n",
    "            else:\n",
    "                pipeline, diarization_result, speaker_names = self.run_diarization(max_speakers=max_speakers, min_speakers=min_speakers)\n",
    "                chunk_info_list = self.chunk_audio(diarization_result)\n",
    "                print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "                speaker_names_path = os.path.join(self.output_dir, \"speaker_names.json\")\n",
    "                with open(speaker_names_path, \"w\") as f:\n",
    "                    json.dump(speaker_names, f, indent=4)\n",
    "                print(f\"Speaker names saved to {speaker_names_path}\")\n",
    "\n",
    "                # You can modify speaker_names here if needed\n",
    "                speaker_names[\"SPEAKER_01\"] = \"Moderator\"\n",
    "                speaker_names[\"SPEAKER_02\"] = \"Participant 1\"\n",
    "\n",
    "                self.process_and_transcribe_chunks(chunk_info_list, language=language)\n",
    "\n",
    "                transcriptions_json_path = os.path.join(self.output_dir, \"transcriptions.json\")\n",
    "                cleaned_transcriptions = self.clean_transcription(transcriptions_json_path, speaker_names)\n",
    "                print(\"Cleaned Transcriptions:\")\n",
    "                for paragraph in cleaned_transcriptions:\n",
    "                    print(paragraph)\n",
    "\n",
    "                self.save_transcription_to_file(cleaned_transcriptions, filename=\"diarized_transcription.txt\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred: {e}\")\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "    output_dir = \"chunks\"\n",
    "\n",
    "    # Load HF_TOKEN if config.py exists\n",
    "    hf_token = None\n",
    "    try:\n",
    "        from config import HF_TOKEN as config_token\n",
    "        hf_token = config_token\n",
    "    except ImportError:\n",
    "        logging.warning(\n",
    "            \"config.py not found or HF_TOKEN not defined. Diarization may not work.\"\n",
    "            \" Please create a config.py file with HF_TOKEN = 'YOUR_HUGGINGFACE_TOKEN'\"\n",
    "        )\n",
    "\n",
    "    # Ask the user if they want to skip diarization\n",
    "    skip_diarization_input = input(\"Do you want to skip speaker diarization and transcribe the whole audio directly? (yes/no): \").lower()\n",
    "    skip_diarization = skip_diarization_input == \"yes\"\n",
    "\n",
    "    whisper_model_choice = \"base\"\n",
    "    if not skip_diarization:\n",
    "        # Ask for min/max speakers\n",
    "        specify_speakers = input(\"Do you want to specify the minimum and maximum number of speakers? (yes/no): \").lower()\n",
    "        min_speakers = None\n",
    "        max_speakers = None\n",
    "        if specify_speakers == \"yes\":\n",
    "            try:\n",
    "                min_speakers = int(input(\"Enter the minimum number of speakers (optional, leave blank for auto): \") or None)\n",
    "                max_speakers = int(input(\"Enter the maximum number of speakers (optional, leave blank for auto): \") or None)\n",
    "            except ValueError:\n",
    "                print(\"Invalid input for the number of speakers. Using default settings.\")\n",
    "\n",
    "        # Ask for Whisper model\n",
    "        print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "        chosen_model = input(\"Choose a Whisper model for transcription (default: base): \").lower().strip()\n",
    "        if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "            whisper_model_choice = chosen_model\n",
    "        elif chosen_model:\n",
    "            print(f\"Invalid Whisper model '{chosen_model}'. Using default model 'base'.\")\n",
    "        else:\n",
    "            print(\"Using default Whisper model 'base'.\")\n",
    "    else:\n",
    "        # Ask for Whisper model even if diarization is skipped\n",
    "        print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "        chosen_model = input(\"Choose a Whisper model for transcription (default: base): \").lower().strip()\n",
    "        if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "            whisper_model_choice = chosen_model\n",
    "        elif chosen_model:\n",
    "            print(f\"Invalid Whisper model '{chosen_model}'. Using default model 'base'.\")\n",
    "        else:\n",
    "            print(\"Using default Whisper model 'base'.\")\n",
    "\n",
    "    transcriber = AudioTranscriber(audio_file, output_dir, hf_token, skip_diarization=skip_diarization, whisper_model=whisper_model_choice)\n",
    "\n",
    "    # Ask the user if they want to specify a language\n",
    "    specify_language = input(\"Do you want to specify a language for transcription? (yes/no): \").lower()\n",
    "    transcription_language = None\n",
    "    if specify_language == \"yes\":\n",
    "        transcription_language = input(\"Enter the language code (e.g., en, fr, es), or leave blank for auto-detection: \").strip()\n",
    "        if not transcription_language:\n",
    "            transcription_language = None # Explicitly set to None if user leaves it blank\n",
    "    elif specify_language == \"no\":\n",
    "        print(\"Using automatic language detection.\")\n",
    "    else:\n",
    "        print(\"Invalid input. Using automatic language detection.\")\n",
    "\n",
    "    if not skip_diarization:\n",
    "        transcriber.process_audio(language=transcription_language, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "    else:\n",
    "        transcriber.process_audio(language=transcription_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n",
      "\n",
      "Available Whisper models: tiny, base, small, medium, large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:31:44,202 - INFO - Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using automatic language detection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:31:46,218 - INFO - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "2025-03-23 21:31:46,218 - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1007: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "\n",
      "Detected 2 speakers:\n",
      "Chunk 1: Speaker 'SPEAKER_00' [0.03 - 10.00]\n",
      "Chunk 2: Speaker 'SPEAKER_01' [11.42 - 12.82]\n",
      "Chunk 3: Speaker 'SPEAKER_00' [12.82 - 14.91]\n",
      "Chunk 4: Speaker 'SPEAKER_00' [16.94 - 25.63]\n",
      "Chunk 5: Speaker 'SPEAKER_01' [25.58 - 27.37]\n",
      "Chunk 6: Speaker 'SPEAKER_00' [27.37 - 28.97]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:32:20,340 - INFO - Transcribing chunks\\chunk_1.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker names saved to chunks\\speaker_names.json\n",
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:03<00:00, 277.55frames/s]\n",
      "2025-03-23 21:32:26,586 - INFO - Transcribing chunks\\chunk_2.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:02<00:00, 48.70frames/s]\n",
      "2025-03-23 21:32:31,767 - INFO - Transcribing chunks\\chunk_3.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:01<00:00, 169.77frames/s]\n",
      "2025-03-23 21:32:35,941 - INFO - Transcribing chunks\\chunk_4.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:01<00:00, 459.99frames/s]\n",
      "2025-03-23 21:32:40,429 - INFO - Transcribing chunks\\chunk_5.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 116.41frames/s]\n",
      "2025-03-23 21:32:44,045 - INFO - Transcribing chunks\\chunk_6.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:01<00:00, 110.94frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Transcriptions:\n",
      "Speaker 1 [0.03 - 10.00]:  Anyway, look where we're digressing the rules.  Oh, simple, Emma, you're about to face five questions  of increasing difficulty.  You must answer as quickly as possible.  If you get it correct, you move onto the next round.  Do you know what happens if you get it wrong?\n",
      "Speaker 2 [11.42 - 12.82]:  and correction and embarrassment.\n",
      "Speaker 1 [12.82 - 14.91]:  Do indeed round one. Round 1 astronomers are saying that Saturn's rings are slowly disappearing.  They estimate we only have a few hundred million years left of them.\n",
      "Speaker 2 [25.58 - 27.37]:  I'll earn you a few hundred million.\n",
      "Speaker 1 [27.37 - 28.97]:  But what I want to know?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Annotation\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import whisper_timestamped as whisper\n",
    "\n",
    "class AudioTranscriber:\n",
    "    def __init__(self, audio_file: str, output_dir: str = \"chunks\", hf_token: Optional[str] = None, skip_diarization: bool = False, whisper_model: str = \"base\"):\n",
    "        self.audio_file = audio_file\n",
    "        self.output_dir = output_dir\n",
    "        self.hf_token = hf_token\n",
    "        self.whisper_model = whisper_model\n",
    "        self.debug_mode = False\n",
    "        self.skip_diarization = skip_diarization\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def load_audio(self, audio_file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Loads an audio file using torchaudio.\"\"\"\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "            return waveform, sample_rate\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading audio file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_diarization(\n",
    "        self,\n",
    "        max_speakers: Optional[int] = None,\n",
    "        min_speakers: Optional[int] = None,\n",
    "    ) -> Tuple[Pipeline, Annotation, Dict[str, str]]:\n",
    "        \"\"\"Performs speaker diarization.\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(f\"Using device: {device}\")\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\", use_auth_token=self.hf_token\n",
    "        ).to(device)\n",
    "        waveform, sample_rate = self.load_audio(self.audio_file)\n",
    "        input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "        if max_speakers:\n",
    "            input_data[\"max_speakers\"] = max_speakers\n",
    "        if min_speakers:\n",
    "            input_data[\"min_speakers\"] = min_speakers\n",
    "        diarization = pipeline(input_data)\n",
    "        speaker_labels = set()\n",
    "        for segment, _, label in diarization.itertracks(yield_label=True):\n",
    "            speaker_labels.add(label)\n",
    "        speaker_names = {\n",
    "            label: f\"Speaker {i + 1}\" for i, label in enumerate(sorted(speaker_labels))\n",
    "        }\n",
    "        return pipeline, diarization, speaker_names\n",
    "\n",
    "    def chunk_audio(self, diarization: Annotation) -> List[Dict]:\n",
    "        \"\"\"Chunks audio based on diarization.\"\"\"\n",
    "        audio = AudioSegment.from_file(self.audio_file)\n",
    "        chunks = []\n",
    "        for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True), 1):\n",
    "            start_ms, end_ms = int(turn.start * 1000), int(turn.end * 1000)\n",
    "            chunk_path = os.path.join(self.output_dir, f\"chunk_{i}.mp3\")\n",
    "            audio[start_ms:end_ms].export(chunk_path, format=\"mp3\")\n",
    "            chunks.append({\"file_path\": chunk_path, \"speaker\": speaker, \"start_time\": turn.start, \"end_time\": turn.end})\n",
    "        return chunks\n",
    "\n",
    "    def transcribe_chunk(self, audio_file_path: str, language: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Transcribes an audio chunk.\"\"\"\n",
    "        try:\n",
    "            model = whisper.load_model(self.whisper_model)\n",
    "            audio = whisper.load_audio(audio_file_path)\n",
    "            return whisper.transcribe(model, audio, language=language)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Transcription error: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def process_and_transcribe_chunks(self, chunks: List[Dict], language: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"Processes and saves transcriptions for individual chunks.\"\"\"\n",
    "        transcriptions = []\n",
    "        for chunk in chunks:\n",
    "            logging.info(f\"Transcribing {chunk['file_path']}\")\n",
    "            transcription = self.transcribe_chunk(chunk[\"file_path\"], language=language)\n",
    "            if transcription:\n",
    "                transcriptions.append({**chunk, \"transcription\": transcription})\n",
    "                if self.debug_mode: # only print if debug mode is on\n",
    "                    print(f\"Transcription for {chunk['file_path']}:\")\n",
    "                    for segment in transcription[\"segments\"]:\n",
    "                        print(f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "        transcriptions_json_path = os.path.join(self.output_dir, \"transcriptions.json\")\n",
    "        with open(transcriptions_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        return transcriptions\n",
    "\n",
    "    def transcribe_whole_audio(self, language: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Transcribes the entire audio file without diarization.\"\"\"\n",
    "        logging.info(f\"Transcribing the entire audio file: {self.audio_file}\")\n",
    "        return self.transcribe_chunk(self.audio_file, language=language)\n",
    "\n",
    "    def clean_transcription(self, transcriptions_json: str, speaker_names: Dict[str, str]) -> List[str]:\n",
    "        \"\"\"Cleans the transcription JSON to a readable format (for diarized audio).\"\"\"\n",
    "        with open(transcriptions_json, 'r') as f:\n",
    "            transcriptions = json.load(f)\n",
    "\n",
    "        cleaned = []\n",
    "        current_speaker, current_text, current_start, current_end = None, \"\", None, None\n",
    "        for i, chunk in enumerate(transcriptions):\n",
    "            if current_speaker != chunk[\"speaker\"]:\n",
    "                if current_speaker is not None:\n",
    "                    cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "                current_speaker, current_text = chunk[\"speaker\"], \"\"\n",
    "                current_start, current_end = chunk[\"start_time\"], chunk[\"end_time\"]\n",
    "                if i > 0 and transcriptions[i][\"speaker\"] != transcriptions[i-1][\"speaker\"]:\n",
    "                    cleaned.append(\"\") # Add a blank line before a new speaker (after the first)\n",
    "            if chunk[\"transcription\"] and chunk[\"transcription\"][\"segments\"]:\n",
    "                current_text += \" \".join(seg[\"text\"] for seg in chunk[\"transcription\"][\"segments\"])\n",
    "        if current_speaker:\n",
    "            cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "        return [line for line in cleaned if line.strip() != \"\"]\n",
    "\n",
    "    def clean_whole_transcription(self, whole_transcription: Dict) -> List[str]:\n",
    "        \"\"\"Cleans the whole transcription output to a readable format (without diarization).\"\"\"\n",
    "        cleaned = []\n",
    "        if whole_transcription and whole_transcription.get(\"segments\"):\n",
    "            for segment in whole_transcription[\"segments\"]:\n",
    "                cleaned.append(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "        return cleaned\n",
    "\n",
    "    def save_transcription_to_file(self, cleaned_transcriptions: List[str], filename=\"transcription.txt\"):\n",
    "        \"\"\"Saves the cleaned transcription to a text file.\"\"\"\n",
    "        output_file = os.path.join(self.output_dir, f\"{os.path.splitext(os.path.basename(self.audio_file))[0]}_{filename}\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(cleaned_transcriptions))\n",
    "\n",
    "    def process_audio(self, language: Optional[str] = None, min_speakers: Optional[int] = None, max_speakers: Optional[int] = None):\n",
    "        \"\"\"Orchestrates the audio processing pipeline.\"\"\"\n",
    "        try:\n",
    "            if self.skip_diarization:\n",
    "                print(\"Skipping diarization and transcribing the whole audio file.\")\n",
    "                whole_transcription = self.transcribe_whole_audio(language=language)\n",
    "                if whole_transcription:\n",
    "                    cleaned_transcriptions = self.clean_whole_transcription(whole_transcription)\n",
    "                    print(\"\\nTranscription:\")\n",
    "                    for line in cleaned_transcriptions:\n",
    "                        print(line)\n",
    "                    self.save_transcription_to_file(cleaned_transcriptions, filename=\"whole_transcription.txt\")\n",
    "                else:\n",
    "                    print(\"Transcription failed.\")\n",
    "            else:\n",
    "                pipeline, diarization_result, speaker_names = self.run_diarization(max_speakers=max_speakers, min_speakers=min_speakers)\n",
    "                chunk_info_list = self.chunk_audio(diarization_result)\n",
    "                print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "                num_speakers = len(speaker_names)\n",
    "                print(f\"\\nDetected {num_speakers} speakers:\")\n",
    "                for i, (turn, _, speaker) in enumerate(diarization_result.itertracks(yield_label=True), 1):\n",
    "                    print(f\"Chunk {i}: Speaker '{speaker}' [{turn.start:.2f} - {turn.end:.2f}]\")\n",
    "\n",
    "                rename_speakers = input(\"\\nDo you want to rename the speakers? (yes/no): \").lower()\n",
    "                if rename_speakers == \"yes\":\n",
    "                    new_speaker_names = {}\n",
    "                    for label in sorted(speaker_names.keys()):\n",
    "                        new_name = input(f\"Enter a new name for '{speaker_names[label]}' (default: {speaker_names[label]}): \").strip()\n",
    "                        if new_name:\n",
    "                            new_speaker_names[label] = new_name\n",
    "                        else:\n",
    "                            new_speaker_names[label] = speaker_names[label]\n",
    "                    speaker_names.update(new_speaker_names)\n",
    "\n",
    "                speaker_names_path = os.path.join(self.output_dir, \"speaker_names.json\")\n",
    "                with open(speaker_names_path, \"w\") as f:\n",
    "                    json.dump(speaker_names, f, indent=4)\n",
    "                print(f\"Speaker names saved to {speaker_names_path}\")\n",
    "\n",
    "                self.process_and_transcribe_chunks(chunk_info_list, language=language)\n",
    "\n",
    "                transcriptions_json_path = os.path.join(self.output_dir, \"transcriptions.json\")\n",
    "                cleaned_transcriptions = self.clean_transcription(transcriptions_json_path, speaker_names)\n",
    "                print(\"Cleaned Transcriptions:\")\n",
    "                for paragraph in cleaned_transcriptions:\n",
    "                    print(paragraph)\n",
    "\n",
    "                self.save_transcription_to_file(cleaned_transcriptions, filename=\"diarized_transcription.txt\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred: {e}\")\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "    output_dir = \"chunks\"\n",
    "\n",
    "    # Load HF_TOKEN if config.py exists\n",
    "    hf_token = None\n",
    "    try:\n",
    "        from config import HF_TOKEN as config_token\n",
    "        hf_token = config_token\n",
    "    except ImportError:\n",
    "        logging.warning(\n",
    "            \"config.py not found or HF_TOKEN not defined. Diarization may not work.\"\n",
    "            \" Please create a config.py file with HF_TOKEN = 'YOUR_HUGGINGFACE_TOKEN'\"\n",
    "        )\n",
    "\n",
    "    # Ask the user if they want to skip diarization\n",
    "    skip_diarization_input = input(\"Do you want to skip speaker diarization and transcribe the whole audio directly? (yes/no): \").lower()\n",
    "    skip_diarization = skip_diarization_input == \"yes\"\n",
    "\n",
    "    whisper_model_choice = \"base\"\n",
    "    if not skip_diarization:\n",
    "        # Ask for min/max speakers\n",
    "        specify_speakers = input(\"Do you want to specify the minimum and maximum number of speakers? (yes/no): \").lower()\n",
    "        min_speakers = None\n",
    "        max_speakers = None\n",
    "        if specify_speakers == \"yes\":\n",
    "            try:\n",
    "                min_speakers = int(input(\"Enter the minimum number of speakers (optional, leave blank for auto): \") or None)\n",
    "                max_speakers = int(input(\"Enter the maximum number of speakers (optional, leave blank for auto): \") or None)\n",
    "            except ValueError:\n",
    "                print(\"Invalid input for the number of speakers. Using default settings.\")\n",
    "\n",
    "        # Ask for Whisper model\n",
    "        print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "        chosen_model = input(\"Choose a Whisper model for transcription (default: base): \").lower().strip()\n",
    "        if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "            whisper_model_choice = chosen_model\n",
    "        elif chosen_model:\n",
    "            print(f\"Invalid Whisper model '{chosen_model}'. Using default model 'base'.\")\n",
    "        else:\n",
    "            print(\"Using default Whisper model 'base'.\")\n",
    "    else:\n",
    "        # Ask for Whisper model even if diarization is skipped\n",
    "        print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "        chosen_model = input(\"Choose a Whisper model for transcription (default: base): \").lower().strip()\n",
    "        if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "            whisper_model_choice = chosen_model\n",
    "        elif chosen_model:\n",
    "            print(f\"Invalid Whisper model '{chosen_model}'. Using default model 'base'.\")\n",
    "        else:\n",
    "            print(\"Using default Whisper model 'base'.\")\n",
    "\n",
    "    transcriber = AudioTranscriber(audio_file, output_dir, hf_token, skip_diarization=skip_diarization, whisper_model=whisper_model_choice)\n",
    "\n",
    "    # Ask the user if they want to specify a language\n",
    "    specify_language = input(\"Do you want to specify a language for transcription? (yes/no): \").lower()\n",
    "    transcription_language = None\n",
    "    if specify_language == \"yes\":\n",
    "        transcription_language = input(\"Enter the language code (e.g., en, fr, es), or leave blank for auto-detection: \").strip()\n",
    "        if not transcription_language:\n",
    "            transcription_language = None # Explicitly set to None if user leaves it blank\n",
    "    elif specify_language == \"no\":\n",
    "        print(\"Using automatic language detection.\")\n",
    "    else:\n",
    "        print(\"Invalid input. Using automatic language detection.\")\n",
    "\n",
    "    if not skip_diarization:\n",
    "        transcriber.process_audio(language=transcription_language, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "    else:\n",
    "        transcriber.process_audio(language=transcription_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n",
      "\n",
      "Available Whisper models: tiny, base, small, medium, large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:38:18,232 - INFO - Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using automatic language detection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:38:20,309 - INFO - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "2025-03-23 21:38:20,311 - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1007: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "\n",
      "Detected 2 speakers:\n",
      "Chunk 1: Speaker 'SPEAKER_00' [0.03 - 10.00]\n",
      "Chunk 2: Speaker 'SPEAKER_01' [11.42 - 12.82]\n",
      "Chunk 3: Speaker 'SPEAKER_00' [12.82 - 14.91]\n",
      "Chunk 4: Speaker 'SPEAKER_00' [16.94 - 25.63]\n",
      "Chunk 5: Speaker 'SPEAKER_01' [25.58 - 27.37]\n",
      "Chunk 6: Speaker 'SPEAKER_00' [27.37 - 28.97]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:39:59,541 - INFO - Transcribing chunks\\chunk_1.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker names saved to chunks\\speaker_names.json\n",
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:03<00:00, 277.17frames/s]\n",
      "2025-03-23 21:40:06,096 - INFO - Transcribing chunks\\chunk_2.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:01<00:00, 115.37frames/s]\n",
      "2025-03-23 21:40:09,457 - INFO - Transcribing chunks\\chunk_3.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:01<00:00, 184.38frames/s]\n",
      "2025-03-23 21:40:13,201 - INFO - Transcribing chunks\\chunk_4.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:02<00:00, 400.53frames/s]\n",
      "2025-03-23 21:40:17,509 - INFO - Transcribing chunks\\chunk_5.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 148.95frames/s]\n",
      "2025-03-23 21:40:20,942 - INFO - Transcribing chunks\\chunk_6.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:01<00:00, 132.83frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Transcriptions:\n",
      "Detected Language: EN\n",
      "Cicero [0.03 - 10.00]:  Anyway, look where we're digressing the rules.  Oh, simple, Emma, you're about to face five questions  of increasing difficulty.  You must answer as quickly as possible.  If you get it correct, you move onto the next round.  Do you know what happens if you get it wrong?\n",
      "ragazza [11.42 - 12.82]:  and correction and embarrassment.\n",
      "Cicero [12.82 - 14.91]:  Do indeed round one. Round 1 astronomers are saying that Saturn's rings are slowly disappearing.  They estimate we only have a few hundred million years left of them.\n",
      "ragazza [25.58 - 27.37]:  I'll earn you a few hundred million.\n",
      "Cicero [27.37 - 28.97]:  But what I want to know?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Annotation\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import whisper_timestamped as whisper\n",
    "\n",
    "class AudioTranscriber:\n",
    "    def __init__(self, audio_file: str, output_dir: str = \"chunks\", hf_token: Optional[str] = None, skip_diarization: bool = False, whisper_model: str = \"base\"):\n",
    "        self.audio_file = audio_file\n",
    "        self.output_dir = output_dir\n",
    "        self.hf_token = hf_token\n",
    "        self.whisper_model = whisper_model\n",
    "        self.debug_mode = False\n",
    "        self.skip_diarization = skip_diarization\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def load_audio(self, audio_file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Loads an audio file using torchaudio.\"\"\"\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "            return waveform, sample_rate\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading audio file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_diarization(\n",
    "        self,\n",
    "        max_speakers: Optional[int] = None,\n",
    "        min_speakers: Optional[int] = None,\n",
    "    ) -> Tuple[Pipeline, Annotation, Dict[str, str]]:\n",
    "        \"\"\"Performs speaker diarization.\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(f\"Using device: {device}\")\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\", use_auth_token=self.hf_token\n",
    "        ).to(device)\n",
    "        waveform, sample_rate = self.load_audio(self.audio_file)\n",
    "        input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "        if max_speakers:\n",
    "            input_data[\"max_speakers\"] = max_speakers\n",
    "        if min_speakers:\n",
    "            input_data[\"min_speakers\"] = min_speakers\n",
    "        diarization = pipeline(input_data)\n",
    "        speaker_labels = set()\n",
    "        for segment, _, label in diarization.itertracks(yield_label=True):\n",
    "            speaker_labels.add(label)\n",
    "        speaker_names = {\n",
    "            label: f\"Speaker {i + 1}\" for i, label in enumerate(sorted(speaker_labels))\n",
    "        }\n",
    "        return pipeline, diarization, speaker_names\n",
    "\n",
    "    def chunk_audio(self, diarization: Annotation) -> List[Dict]:\n",
    "        \"\"\"Chunks audio based on diarization.\"\"\"\n",
    "        audio = AudioSegment.from_file(self.audio_file)\n",
    "        chunks = []\n",
    "        for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True), 1):\n",
    "            start_ms, end_ms = int(turn.start * 1000), int(turn.end * 1000)\n",
    "            chunk_path = os.path.join(self.output_dir, f\"chunk_{i}.mp3\")\n",
    "            audio[start_ms:end_ms].export(chunk_path, format=\"mp3\")\n",
    "            chunks.append({\"file_path\": chunk_path, \"speaker\": speaker, \"start_time\": turn.start, \"end_time\": turn.end})\n",
    "        return chunks\n",
    "\n",
    "    def transcribe_chunk(self, audio_file_path: str, language: Optional[str] = None) -> Tuple[Dict, Optional[str]]:\n",
    "        \"\"\"Transcribes an audio chunk and returns the transcription and detected language.\"\"\"\n",
    "        try:\n",
    "            model = whisper.load_model(self.whisper_model)\n",
    "            audio = whisper.load_audio(audio_file_path)\n",
    "            result = whisper.transcribe(model, audio, language=language)\n",
    "            detected_language = result.get(\"language\") if language is None else language\n",
    "            return result, detected_language\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Transcription error: {e}\")\n",
    "            return {}, None\n",
    "\n",
    "    def process_and_transcribe_chunks(self, chunks: List[Dict], language: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"Processes and saves transcriptions for individual chunks.\"\"\"\n",
    "        transcriptions = []\n",
    "        detected_language = None\n",
    "        for chunk in chunks:\n",
    "            logging.info(f\"Transcribing {chunk['file_path']}\")\n",
    "            transcription, lang = self.transcribe_chunk(chunk[\"file_path\"], language=language)\n",
    "            if transcription:\n",
    "                if detected_language is None and lang is not None:\n",
    "                    detected_language = lang\n",
    "                transcriptions.append({**chunk, \"transcription\": transcription, \"language\": lang})\n",
    "                if self.debug_mode: # only print if debug mode is on\n",
    "                    print(f\"Transcription for {chunk['file_path']} (Language: {lang}):\")\n",
    "                    for segment in transcription[\"segments\"]:\n",
    "                        print(f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "        transcriptions_json_path = os.path.join(self.output_dir, \"transcriptions.json\")\n",
    "        with open(transcriptions_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        return transcriptions, detected_language\n",
    "\n",
    "    def transcribe_whole_audio(self, language: Optional[str] = None) -> Tuple[Dict, Optional[str]]:\n",
    "        \"\"\"Transcribes the entire audio file without diarization.\"\"\"\n",
    "        logging.info(f\"Transcribing the entire audio file: {self.audio_file}\")\n",
    "        return self.transcribe_chunk(self.audio_file, language=language)\n",
    "\n",
    "    def clean_transcription(self, transcriptions_json: str, speaker_names: Dict[str, str]) -> List[str]:\n",
    "        \"\"\"Cleans the transcription JSON to a readable format (for diarized audio).\"\"\"\n",
    "        with open(transcriptions_json, 'r') as f:\n",
    "            transcriptions = json.load(f)\n",
    "\n",
    "        cleaned = []\n",
    "        detected_language = transcriptions[0].get(\"language\") if transcriptions else None\n",
    "        if detected_language:\n",
    "            cleaned.append(f\"Detected Language: {detected_language.upper()}\")\n",
    "            cleaned.append(\"\")\n",
    "\n",
    "        current_speaker, current_text, current_start, current_end = None, \"\", None, None\n",
    "        for i, chunk in enumerate(transcriptions):\n",
    "            if current_speaker != chunk[\"speaker\"]:\n",
    "                if current_speaker is not None:\n",
    "                    cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "                current_speaker, current_text = chunk[\"speaker\"], \"\"\n",
    "                current_start, current_end = chunk[\"start_time\"], chunk[\"end_time\"]\n",
    "                if i > 0 and transcriptions[i][\"speaker\"] != transcriptions[i-1][\"speaker\"]:\n",
    "                    cleaned.append(\"\") # Add a blank line before a new speaker (after the first)\n",
    "            if chunk[\"transcription\"] and chunk[\"transcription\"][\"segments\"]:\n",
    "                current_text += \" \".join(seg[\"text\"] for seg in chunk[\"transcription\"][\"segments\"])\n",
    "        if current_speaker:\n",
    "            cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "        return [line for line in cleaned if line.strip() != \"\"]\n",
    "\n",
    "    def clean_whole_transcription(self, whole_transcription: Dict, language: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"Cleans the whole transcription output to a readable format (without diarization).\"\"\"\n",
    "        cleaned = []\n",
    "        detected_language = whole_transcription.get(\"language\") if whole_transcription else language\n",
    "        if detected_language:\n",
    "            cleaned.append(f\"Detected Language: {detected_language.upper()}\")\n",
    "            cleaned.append(\"\")\n",
    "        if whole_transcription and whole_transcription.get(\"segments\"):\n",
    "            for segment in whole_transcription[\"segments\"]:\n",
    "                cleaned.append(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "        return cleaned\n",
    "\n",
    "    def save_transcription_to_file(self, cleaned_transcriptions: List[str], filename=\"transcription.txt\"):\n",
    "        \"\"\"Saves the cleaned transcription to a text file.\"\"\"\n",
    "        output_file = os.path.join(self.output_dir, f\"{os.path.splitext(os.path.basename(self.audio_file))[0]}_{filename}\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(cleaned_transcriptions))\n",
    "\n",
    "    def process_audio(self, language: Optional[str] = None, min_speakers: Optional[int] = None, max_speakers: Optional[int] = None):\n",
    "        \"\"\"Orchestrates the audio processing pipeline.\"\"\"\n",
    "        try:\n",
    "            if self.skip_diarization:\n",
    "                print(\"Skipping diarization and transcribing the whole audio file.\")\n",
    "                whole_transcription, detected_language = self.transcribe_whole_audio(language=language)\n",
    "                if whole_transcription:\n",
    "                    cleaned_transcriptions = self.clean_whole_transcription(whole_transcription, detected_language)\n",
    "                    print(\"\\nTranscription:\")\n",
    "                    for line in cleaned_transcriptions:\n",
    "                        print(line)\n",
    "                    self.save_transcription_to_file(cleaned_transcriptions, filename=\"whole_transcription.txt\")\n",
    "                else:\n",
    "                    print(\"Transcription failed.\")\n",
    "            else:\n",
    "                pipeline, diarization_result, speaker_names = self.run_diarization(max_speakers=max_speakers, min_speakers=min_speakers)\n",
    "                chunk_info_list = self.chunk_audio(diarization_result)\n",
    "                print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "                num_speakers = len(speaker_names)\n",
    "                print(f\"\\nDetected {num_speakers} speakers:\")\n",
    "                for i, (turn, _, speaker) in enumerate(diarization_result.itertracks(yield_label=True), 1):\n",
    "                    print(f\"Chunk {i}: Speaker '{speaker}' [{turn.start:.2f} - {turn.end:.2f}]\")\n",
    "\n",
    "                rename_speakers = input(\"\\nDo you want to rename the speakers? (yes/no): \").lower()\n",
    "                if rename_speakers == \"yes\":\n",
    "                    new_speaker_names = {}\n",
    "                    for label in sorted(speaker_names.keys()):\n",
    "                        new_name = input(f\"Enter a new name for '{speaker_names[label]}' (default: {speaker_names[label]}): \").strip()\n",
    "                        if new_name:\n",
    "                            new_speaker_names[label] = new_name\n",
    "                        else:\n",
    "                            new_speaker_names[label] = speaker_names[label]\n",
    "                    speaker_names.update(new_speaker_names)\n",
    "\n",
    "                speaker_names_path = os.path.join(self.output_dir, \"speaker_names.json\")\n",
    "                with open(speaker_names_path, \"w\") as f:\n",
    "                    json.dump(speaker_names, f, indent=4)\n",
    "                print(f\"Speaker names saved to {speaker_names_path}\")\n",
    "\n",
    "                transcriptions, detected_language = self.process_and_transcribe_chunks(chunk_info_list, language=language)\n",
    "\n",
    "                transcriptions_json_path = os.path.join(self.output_dir, \"transcriptions.json\")\n",
    "                cleaned_transcriptions = self.clean_transcription(transcriptions_json_path, speaker_names)\n",
    "                print(\"Cleaned Transcriptions:\")\n",
    "                for paragraph in cleaned_transcriptions:\n",
    "                    print(paragraph)\n",
    "\n",
    "                self.save_transcription_to_file(cleaned_transcriptions, filename=\"diarized_transcription.txt\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred: {e}\")\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "    output_dir = \"chunks\"\n",
    "\n",
    "    # Load HF_TOKEN if config.py exists\n",
    "    hf_token = None\n",
    "    try:\n",
    "        from config import HF_TOKEN as config_token\n",
    "        hf_token = config_token\n",
    "    except ImportError:\n",
    "        logging.warning(\n",
    "            \"config.py not found or HF_TOKEN not defined. Diarization may not work.\"\n",
    "            \" Please create a config.py file with HF_TOKEN = 'YOUR_HUGGINGFACE_TOKEN'\"\n",
    "        )\n",
    "\n",
    "    # Ask the user if they want to skip diarization\n",
    "    skip_diarization_input = input(\"Do you want to skip speaker diarization and transcribe the whole audio directly? (yes/no): \").lower()\n",
    "    skip_diarization = skip_diarization_input == \"yes\"\n",
    "\n",
    "    whisper_model_choice = \"base\"\n",
    "    if not skip_diarization:\n",
    "        # Ask for min/max speakers\n",
    "        specify_speakers = input(\"Do you want to specify the minimum and maximum number of speakers? (yes/no): \").lower()\n",
    "        min_speakers = None\n",
    "        max_speakers = None\n",
    "        if specify_speakers == \"yes\":\n",
    "            try:\n",
    "                min_speakers = int(input(\"Enter the minimum number of speakers (optional, leave blank for auto): \") or None)\n",
    "                max_speakers = int(input(\"Enter the maximum number of speakers (optional, leave blank for auto): \") or None)\n",
    "            except ValueError:\n",
    "                print(\"Invalid input for the number of speakers. Using default settings.\")\n",
    "\n",
    "        # Ask for Whisper model\n",
    "        print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "        chosen_model = input(\"Choose a Whisper model for transcription (default: base): \").lower().strip()\n",
    "        if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "            whisper_model_choice = chosen_model\n",
    "        elif chosen_model:\n",
    "            print(f\"Invalid Whisper model '{chosen_model}'. Using default model 'base'.\")\n",
    "        else:\n",
    "            print(\"Using default Whisper model 'base'.\")\n",
    "    else:\n",
    "        # Ask for Whisper model even if diarization is skipped\n",
    "        print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "        chosen_model = input(\"Choose a Whisper model for transcription (default: base): \").lower().strip()\n",
    "        if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "            whisper_model_choice = chosen_model\n",
    "        elif chosen_model:\n",
    "            print(f\"Invalid Whisper model '{chosen_model}'. Using default model 'base'.\")\n",
    "        else:\n",
    "            print(\"Using default Whisper model 'base'.\")\n",
    "\n",
    "    transcriber = AudioTranscriber(audio_file, output_dir, hf_token, skip_diarization=skip_diarization, whisper_model=whisper_model_choice)\n",
    "\n",
    "    # Ask the user if they want to specify a language\n",
    "    specify_language = input(\"Do you want to specify a language for transcription? (yes/no): \").lower()\n",
    "    transcription_language = None\n",
    "    if specify_language == \"yes\":\n",
    "        transcription_language = input(\"Enter the language code (e.g., en, fr, es), or leave blank for auto-detection: \").strip()\n",
    "        if not transcription_language:\n",
    "            transcription_language = None # Explicitly set to None if user leaves it blank\n",
    "    elif specify_language == \"no\":\n",
    "        print(\"Using automatic language detection.\")\n",
    "    else:\n",
    "        print(\"Invalid input. Using automatic language detection.\")\n",
    "\n",
    "    if not skip_diarization:\n",
    "        transcriber.process_audio(language=transcription_language, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "    else:\n",
    "        transcriber.process_audio(language=transcription_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n",
      "\n",
      "Available Whisper models: tiny, base, small, medium, large\n",
      "Using default Whisper model 'base'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:47:19,480 - INFO - Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using automatic language detection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:47:21,419 - INFO - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "2025-03-23 21:47:21,421 - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1007: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "\n",
      "Detected 2 speakers:\n",
      "Chunk 1: Speaker 'SPEAKER_00' [0.03 - 10.00]\n",
      "Chunk 2: Speaker 'SPEAKER_01' [11.42 - 12.82]\n",
      "Chunk 3: Speaker 'SPEAKER_00' [12.82 - 14.91]\n",
      "Chunk 4: Speaker 'SPEAKER_00' [16.94 - 25.63]\n",
      "Chunk 5: Speaker 'SPEAKER_01' [25.58 - 27.37]\n",
      "Chunk 6: Speaker 'SPEAKER_00' [27.37 - 28.97]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:47:56,473 - INFO - Transcribing out\\test\\chunk_1.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker names saved to out\\test\\test_speaker_names.json\n",
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:04<00:00, 233.92frames/s]\n",
      "2025-03-23 21:48:04,421 - INFO - Transcribing out\\test\\chunk_2.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:01<00:00, 124.37frames/s]\n",
      "2025-03-23 21:48:07,627 - INFO - Transcribing out\\test\\chunk_3.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:03<00:00, 64.33frames/s]\n",
      "2025-03-23 21:48:12,939 - INFO - Transcribing out\\test\\chunk_4.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:02<00:00, 413.80frames/s]\n",
      "2025-03-23 21:48:17,297 - INFO - Transcribing out\\test\\chunk_5.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 145.94frames/s]\n",
      "2025-03-23 21:48:20,690 - INFO - Transcribing out\\test\\chunk_6.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:01<00:00, 102.43frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Transcriptions:\n",
      "Detected Language: EN\n",
      "Audio File: test\n",
      "Speaker 1 [0.03 - 10.00]:  Anyway, look where we're digressing the rules.  Oh, simple, Emma, you're about to face five questions  of increasing difficulty.  You must answer as quickly as possible.  If you get it correct, you move onto the next round.  Do you know what happens if you get it wrong?\n",
      "Speaker 2 [11.42 - 12.82]:  and correction and embarrassment.\n",
      "Speaker 1 [12.82 - 14.91]:  Do indeed round one. Round 1 astronomers are saying that Saturn's rings are slowly disappearing.  They estimate we only have a few hundred million years left of them.\n",
      "Speaker 2 [25.58 - 27.37]:  I'll earn you a few hundred million.\n",
      "Speaker 1 [27.37 - 28.97]:  But what I want to know?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Annotation\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import whisper_timestamped as whisper\n",
    "\n",
    "class AudioTranscriber:\n",
    "    def __init__(self, audio_file: str, output_dir: str = \"chunks\", hf_token: Optional[str] = None, skip_diarization: bool = False, whisper_model: str = \"base\"):\n",
    "        self.audio_file = audio_file\n",
    "        self.audio_base_name = os.path.splitext(os.path.basename(self.audio_file))[0]\n",
    "        self.output_dir_base = output_dir\n",
    "        self.output_dir = os.path.join(self.output_dir_base, self.audio_base_name)\n",
    "        self.hf_token = hf_token\n",
    "        self.whisper_model = whisper_model\n",
    "        self.debug_mode = False\n",
    "        self.skip_diarization = skip_diarization\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def load_audio(self, audio_file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Loads an audio file using torchaudio.\"\"\"\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "            return waveform, sample_rate\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading audio file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_diarization(\n",
    "        self,\n",
    "        max_speakers: Optional[int] = None,\n",
    "        min_speakers: Optional[int] = None,\n",
    "    ) -> Tuple[Pipeline, Annotation, Dict[str, str]]:\n",
    "        \"\"\"Performs speaker diarization.\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(f\"Using device: {device}\")\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\", use_auth_token=self.hf_token\n",
    "        ).to(device)\n",
    "        waveform, sample_rate = self.load_audio(self.audio_file)\n",
    "        input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "        if max_speakers:\n",
    "            input_data[\"max_speakers\"] = max_speakers\n",
    "        if min_speakers:\n",
    "            input_data[\"min_speakers\"] = min_speakers\n",
    "        diarization = pipeline(input_data)\n",
    "        speaker_labels = set()\n",
    "        for segment, _, label in diarization.itertracks(yield_label=True):\n",
    "            speaker_labels.add(label)\n",
    "        speaker_names = {\n",
    "            label: f\"Speaker {i + 1}\" for i, label in enumerate(sorted(speaker_labels))\n",
    "        }\n",
    "        return pipeline, diarization, speaker_names\n",
    "\n",
    "    def chunk_audio(self, diarization: Annotation) -> List[Dict]:\n",
    "        \"\"\"Chunks audio based on diarization.\"\"\"\n",
    "        audio = AudioSegment.from_file(self.audio_file)\n",
    "        chunks = []\n",
    "        for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True), 1):\n",
    "            start_ms, end_ms = int(turn.start * 1000), int(turn.end * 1000)\n",
    "            chunk_path = os.path.join(self.output_dir, f\"chunk_{i}.mp3\")\n",
    "            audio[start_ms:end_ms].export(chunk_path, format=\"mp3\")\n",
    "            chunks.append({\"file_path\": chunk_path, \"speaker\": speaker, \"start_time\": turn.start, \"end_time\": turn.end})\n",
    "        return chunks\n",
    "\n",
    "    def transcribe_chunk(self, audio_file_path: str, language: Optional[str] = None) -> Tuple[Dict, Optional[str]]:\n",
    "        \"\"\"Transcribes an audio chunk and returns the transcription and detected language.\"\"\"\n",
    "        try:\n",
    "            model = whisper.load_model(self.whisper_model)\n",
    "            audio = whisper.load_audio(audio_file_path)\n",
    "            result = whisper.transcribe(model, audio, language=language)\n",
    "            detected_language = result.get(\"language\") if language is None else language\n",
    "            return result, detected_language\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Transcription error: {e}\")\n",
    "            return {}, None\n",
    "\n",
    "    def process_and_transcribe_chunks(self, chunks: List[Dict], language: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"Processes and saves transcriptions for individual chunks.\"\"\"\n",
    "        transcriptions = []\n",
    "        detected_language = None\n",
    "        for chunk in chunks:\n",
    "            logging.info(f\"Transcribing {chunk['file_path']}\")\n",
    "            transcription, lang = self.transcribe_chunk(chunk[\"file_path\"], language=language)\n",
    "            if transcription:\n",
    "                if detected_language is None and lang is not None:\n",
    "                    detected_language = lang\n",
    "                transcriptions.append({**chunk, \"transcription\": transcription, \"language\": lang})\n",
    "                if self.debug_mode: # only print if debug mode is on\n",
    "                    print(f\"Transcription for {chunk['file_path']} (Language: {lang}):\")\n",
    "                    for segment in transcription[\"segments\"]:\n",
    "                        print(f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "        transcriptions_json_path = os.path.join(self.output_dir, f\"{self.audio_base_name}_transcriptions.json\")\n",
    "        with open(transcriptions_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        return transcriptions, detected_language\n",
    "\n",
    "    def transcribe_whole_audio(self, language: Optional[str] = None) -> Tuple[Dict, Optional[str]]:\n",
    "        \"\"\"Transcribes the entire audio file without diarization.\"\"\"\n",
    "        logging.info(f\"Transcribing the entire audio file: {self.audio_file}\")\n",
    "        return self.transcribe_chunk(self.audio_file, language=language)\n",
    "\n",
    "    def clean_transcription(self, transcriptions_json: str, speaker_names: Dict[str, str]) -> List[str]:\n",
    "        \"\"\"Cleans the transcription JSON to a readable format (for diarized audio).\"\"\"\n",
    "        with open(transcriptions_json, 'r') as f:\n",
    "            transcriptions = json.load(f)\n",
    "\n",
    "        cleaned = []\n",
    "        detected_language = transcriptions[0].get(\"language\") if transcriptions else None\n",
    "        if detected_language:\n",
    "            cleaned.append(f\"Detected Language: {detected_language.upper()}\")\n",
    "            cleaned.append(\"\")\n",
    "        cleaned.append(f\"Audio File: {self.audio_base_name}\")\n",
    "        cleaned.append(\"\")\n",
    "\n",
    "        current_speaker, current_text, current_start, current_end = None, \"\", None, None\n",
    "        for i, chunk in enumerate(transcriptions):\n",
    "            if current_speaker != chunk[\"speaker\"]:\n",
    "                if current_speaker is not None:\n",
    "                    cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "                current_speaker, current_text = chunk[\"speaker\"], \"\"\n",
    "                current_start, current_end = chunk[\"start_time\"], chunk[\"end_time\"]\n",
    "                if i > 0 and transcriptions[i][\"speaker\"] != transcriptions[i-1][\"speaker\"]:\n",
    "                    cleaned.append(\"\") # Add a blank line before a new speaker (after the first)\n",
    "            if chunk[\"transcription\"] and chunk[\"transcription\"][\"segments\"]:\n",
    "                current_text += \" \".join(seg[\"text\"] for seg in chunk[\"transcription\"][\"segments\"])\n",
    "        if current_speaker:\n",
    "            cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "        return [line for line in cleaned if line.strip() != \"\"]\n",
    "\n",
    "    def clean_whole_transcription(self, whole_transcription: Dict, language: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"Cleans the whole transcription output to a readable format (without diarization).\"\"\"\n",
    "        cleaned = []\n",
    "        detected_language = whole_transcription.get(\"language\") if whole_transcription else language\n",
    "        if detected_language:\n",
    "            cleaned.append(f\"Detected Language: {detected_language.upper()}\")\n",
    "            cleaned.append(\"\")\n",
    "        cleaned.append(f\"Audio File: {self.audio_base_name}\")\n",
    "        cleaned.append(\"\")\n",
    "        if whole_transcription and whole_transcription.get(\"segments\"):\n",
    "            for segment in whole_transcription[\"segments\"]:\n",
    "                cleaned.append(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "        return cleaned\n",
    "\n",
    "    def save_transcription_to_file(self, cleaned_transcriptions: List[str], filename=\"transcription.txt\"):\n",
    "        \"\"\"Saves the cleaned transcription to a text file.\"\"\"\n",
    "        output_file = os.path.join(self.output_dir, f\"{self.audio_base_name}_{filename}\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(cleaned_transcriptions))\n",
    "\n",
    "    def process_audio(self, language: Optional[str] = None, min_speakers: Optional[int] = None, max_speakers: Optional[int] = None):\n",
    "        \"\"\"Orchestrates the audio processing pipeline.\"\"\"\n",
    "        try:\n",
    "            if self.skip_diarization:\n",
    "                print(\"Skipping diarization and transcribing the whole audio file.\")\n",
    "                whole_transcription, detected_language = self.transcribe_whole_audio(language=language)\n",
    "                if whole_transcription:\n",
    "                    cleaned_transcriptions = self.clean_whole_transcription(whole_transcription, detected_language)\n",
    "                    print(\"\\nTranscription:\")\n",
    "                    for line in cleaned_transcriptions:\n",
    "                        print(line)\n",
    "                    self.save_transcription_to_file(cleaned_transcriptions, filename=\"whole_transcription.txt\")\n",
    "                else:\n",
    "                    print(\"Transcription failed.\")\n",
    "            else:\n",
    "                pipeline, diarization_result, speaker_names = self.run_diarization(max_speakers=max_speakers, min_speakers=min_speakers)\n",
    "                chunk_info_list = self.chunk_audio(diarization_result)\n",
    "                print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "                num_speakers = len(speaker_names)\n",
    "                print(f\"\\nDetected {num_speakers} speakers:\")\n",
    "                for i, (turn, _, speaker) in enumerate(diarization_result.itertracks(yield_label=True), 1):\n",
    "                    print(f\"Chunk {i}: Speaker '{speaker}' [{turn.start:.2f} - {turn.end:.2f}]\")\n",
    "\n",
    "                rename_speakers = input(\"\\nDo you want to rename the speakers? (yes/no): \").lower()\n",
    "                if rename_speakers == \"yes\":\n",
    "                    new_speaker_names = {}\n",
    "                    for label in sorted(speaker_names.keys()):\n",
    "                        new_name = input(f\"Enter a new name for '{speaker_names[label]}' (default: {speaker_names[label]}): \").strip()\n",
    "                        if new_name:\n",
    "                            new_speaker_names[label] = new_name\n",
    "                        else:\n",
    "                            new_speaker_names[label] = speaker_names[label]\n",
    "                    speaker_names.update(new_speaker_names)\n",
    "\n",
    "                speaker_names_path = os.path.join(self.output_dir, f\"{self.audio_base_name}_speaker_names.json\")\n",
    "                with open(speaker_names_path, \"w\") as f:\n",
    "                    json.dump(speaker_names, f, indent=4)\n",
    "                print(f\"Speaker names saved to {speaker_names_path}\")\n",
    "\n",
    "                transcriptions, detected_language = self.process_and_transcribe_chunks(chunk_info_list, language=language)\n",
    "\n",
    "                transcriptions_json_path = os.path.join(self.output_dir, f\"{self.audio_base_name}_transcriptions.json\")\n",
    "                cleaned_transcriptions = self.clean_transcription(transcriptions_json_path, speaker_names)\n",
    "                print(\"Cleaned Transcriptions:\")\n",
    "                for paragraph in cleaned_transcriptions:\n",
    "                    print(paragraph)\n",
    "\n",
    "                self.save_transcription_to_file(cleaned_transcriptions, filename=\"diarized_transcription.txt\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred: {e}\")\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = r\"C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\"\n",
    "    output_dir = \"out\"\n",
    "\n",
    "    # Load HF_TOKEN if config.py exists\n",
    "    hf_token = None\n",
    "    try:\n",
    "        from config import HF_TOKEN as config_token\n",
    "        hf_token = config_token\n",
    "    except ImportError:\n",
    "        logging.warning(\n",
    "            \"config.py not found or HF_TOKEN not defined. Diarization may not work.\"\n",
    "            \" Please create a config.py file with HF_TOKEN = 'YOUR_HUGGINGFACE_TOKEN'\"\n",
    "        )\n",
    "\n",
    "    # Ask the user if they want to skip diarization\n",
    "    skip_diarization_input = input(\"Do you want to skip speaker diarization and transcribe the whole audio directly? (yes/no): \").lower()\n",
    "    skip_diarization = skip_diarization_input == \"yes\"\n",
    "\n",
    "    whisper_model_choice = \"base\"\n",
    "    if not skip_diarization:\n",
    "        # Ask for min/max speakers\n",
    "        specify_speakers = input(\"Do you want to specify the minimum and maximum number of speakers? (yes/no): \").lower()\n",
    "        min_speakers = None\n",
    "        max_speakers = None\n",
    "        if specify_speakers == \"yes\":\n",
    "            try:\n",
    "                min_speakers = int(input(\"Enter the minimum number of speakers (optional, leave blank for auto): \") or None)\n",
    "                max_speakers = int(input(\"Enter the maximum number of speakers (optional, leave blank for auto): \") or None)\n",
    "            except ValueError:\n",
    "                print(\"Invalid input for the number of speakers. Using default settings.\")\n",
    "\n",
    "        # Ask for Whisper model\n",
    "        print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "        chosen_model = input(\"Choose a Whisper model for transcription (default: base): \").lower().strip()\n",
    "        if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "            whisper_model_choice = chosen_model\n",
    "        elif chosen_model:\n",
    "            print(f\"Invalid Whisper model '{chosen_model}'. Using default model 'base'.\")\n",
    "        else:\n",
    "            print(\"Using default Whisper model 'base'.\")\n",
    "    else:\n",
    "        # Ask for Whisper model even if diarization is skipped\n",
    "        print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "        chosen_model = input(\"Choose a Whisper model for transcription (default: base): \").lower().strip()\n",
    "        if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "            whisper_model_choice = chosen_model\n",
    "        elif chosen_model:\n",
    "            print(f\"Invalid Whisper model '{chosen_model}'. Using default model 'base'.\")\n",
    "        else:\n",
    "            print(\"Using default Whisper model 'base'.\")\n",
    "\n",
    "    transcriber = AudioTranscriber(audio_file, output_dir, hf_token, skip_diarization=skip_diarization, whisper_model=whisper_model_choice)\n",
    "\n",
    "    # Ask the user if they want to specify a language\n",
    "    specify_language = input(\"Do you want to specify a language for transcription? (yes/no): \").lower()\n",
    "    transcription_language = None\n",
    "    if specify_language == \"yes\":\n",
    "        transcription_language = input(\"Enter the language code (e.g., en, fr, es), or leave blank for auto-detection: \").strip()\n",
    "        if not transcription_language:\n",
    "            transcription_language = None # Explicitly set to None if user leaves it blank\n",
    "    elif specify_language == \"no\":\n",
    "        print(\"Using automatic language detection.\")\n",
    "    else:\n",
    "        print(\"Invalid input. Using automatic language detection.\")\n",
    "\n",
    "    if not skip_diarization:\n",
    "        transcriber.process_audio(language=transcription_language, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "    else:\n",
    "        transcriber.process_audio(language=transcription_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n",
      "\n",
      "Available audio files:\n",
      "1. 11 lug, 11.20​ prova 2.aac\n",
      "2. 4547.mp3\n",
      "3. 6313.mp3\n",
      "4. Botanicario - Ribes Nero.wav\n",
      "5. Come STUDIARE allUNIVERSITÀ.mp3\n",
      "6. Come STUDIARE allUNIVERSITÀ.wav\n",
      "7. test.mp3\n",
      "\n",
      "--- Processing: C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3 ---\n",
      "\n",
      "Available Whisper models: tiny, base, small, medium, large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 21:56:18,671 - INFO - Using device: cpu\n",
      "2025-03-23 21:56:20,589 - INFO - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "2025-03-23 21:56:20,590 - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1007: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "\n",
      "Detected 2 speakers:\n",
      "Chunk 1: Speaker 'SPEAKER_00' [0.03 - 10.00]\n",
      "Chunk 2: Speaker 'SPEAKER_01' [11.42 - 12.82]\n",
      "Chunk 3: Speaker 'SPEAKER_00' [12.82 - 14.91]\n",
      "Chunk 4: Speaker 'SPEAKER_00' [16.94 - 25.63]\n",
      "Chunk 5: Speaker 'SPEAKER_01' [25.58 - 27.37]\n",
      "Chunk 6: Speaker 'SPEAKER_00' [27.37 - 28.97]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 22:00:23,942 - INFO - Transcribing out\\test\\chunk_1.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker names saved to out\\test\\test_speaker_names.json\n",
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:04<00:00, 241.98frames/s]\n",
      "2025-03-23 22:00:30,949 - INFO - Transcribing out\\test\\chunk_2.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:03<00:00, 36.35frames/s]\n",
      "2025-03-23 22:00:36,965 - INFO - Transcribing out\\test\\chunk_3.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:01<00:00, 201.42frames/s]\n",
      "2025-03-23 22:00:40,090 - INFO - Transcribing out\\test\\chunk_4.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:02<00:00, 378.97frames/s]\n",
      "2025-03-23 22:00:44,712 - INFO - Transcribing out\\test\\chunk_5.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 125.47frames/s]\n",
      "2025-03-23 22:00:48,236 - INFO - Transcribing out\\test\\chunk_6.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:01<00:00, 96.36frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Transcriptions:\n",
      "Detected Language: EN\n",
      "Audio File: test\n",
      "Speaker 1 [0.03 - 10.00]:  Anyway, look where we're digressing the rules.  Oh, simple, Emma, you're about to face five questions  of increasing difficulty.  You must answer as quickly as possible.  If you get it correct, you move onto the next round.  Do you know what happens if you get it wrong?\n",
      "Speaker 2 [11.42 - 12.82]:  and correction and embarrassment.\n",
      "Speaker 1 [12.82 - 14.91]:  Do indeed round one. Round 1 astronomers are saying that Saturn's rings are slowly disappearing.  They estimate we only have a few hundred million years left of them.\n",
      "Speaker 2 [25.58 - 27.37]:  I'll earn you a few hundred million.\n",
      "Speaker 1 [27.37 - 28.97]:  But what I want to know?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Annotation\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import whisper_timestamped as whisper\n",
    "\n",
    "class AudioTranscriber:\n",
    "    def __init__(self, audio_file: str, output_dir: str = \"chunks\", hf_token: Optional[str] = None, skip_diarization: bool = False, whisper_model: str = \"base\"):\n",
    "        self.audio_file = audio_file\n",
    "        self.audio_base_name = os.path.splitext(os.path.basename(self.audio_file))[0]\n",
    "        self.output_dir_base = output_dir\n",
    "        self.output_dir = os.path.join(self.output_dir_base, self.audio_base_name)\n",
    "        self.hf_token = hf_token\n",
    "        self.whisper_model = whisper_model\n",
    "        self.debug_mode = False\n",
    "        self.skip_diarization = skip_diarization\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def load_audio(self, audio_file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Loads an audio file using torchaudio.\"\"\"\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "            return waveform, sample_rate\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading audio file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_diarization(\n",
    "        self,\n",
    "        max_speakers: Optional[int] = None,\n",
    "        min_speakers: Optional[int] = None,\n",
    "    ) -> Tuple[Pipeline, Annotation, Dict[str, str]]:\n",
    "        \"\"\"Performs speaker diarization.\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(f\"Using device: {device}\")\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\", use_auth_token=self.hf_token\n",
    "        ).to(device)\n",
    "        waveform, sample_rate = self.load_audio(self.audio_file)\n",
    "        input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "        if max_speakers:\n",
    "            input_data[\"max_speakers\"] = max_speakers\n",
    "        if min_speakers:\n",
    "            input_data[\"min_speakers\"] = min_speakers\n",
    "        diarization = pipeline(input_data)\n",
    "        speaker_labels = set()\n",
    "        for segment, _, label in diarization.itertracks(yield_label=True):\n",
    "            speaker_labels.add(label)\n",
    "        speaker_names = {\n",
    "            label: f\"Speaker {i + 1}\" for i, label in enumerate(sorted(speaker_labels))\n",
    "        }\n",
    "        return pipeline, diarization, speaker_names\n",
    "\n",
    "    def chunk_audio(self, diarization: Annotation) -> List[Dict]:\n",
    "        \"\"\"Chunks audio based on diarization.\"\"\"\n",
    "        audio = AudioSegment.from_file(self.audio_file)\n",
    "        chunks = []\n",
    "        for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True), 1):\n",
    "            start_ms, end_ms = int(turn.start * 1000), int(turn.end * 1000)\n",
    "            chunk_path = os.path.join(self.output_dir, f\"chunk_{i}.mp3\")\n",
    "            audio[start_ms:end_ms].export(chunk_path, format=\"mp3\")\n",
    "            chunks.append({\"file_path\": chunk_path, \"speaker\": speaker, \"start_time\": turn.start, \"end_time\": turn.end})\n",
    "        return chunks\n",
    "\n",
    "    def transcribe_chunk(self, audio_file_path: str, language: Optional[str] = None) -> Tuple[Dict, Optional[str]]:\n",
    "        \"\"\"Transcribes an audio chunk and returns the transcription and detected language.\"\"\"\n",
    "        try:\n",
    "            model = whisper.load_model(self.whisper_model)\n",
    "            audio = whisper.load_audio(audio_file_path)\n",
    "            result = whisper.transcribe(model, audio, language=language)\n",
    "            detected_language = result.get(\"language\") if language is None else language\n",
    "            return result, detected_language\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Transcription error: {e}\")\n",
    "            return {}, None\n",
    "\n",
    "    def process_and_transcribe_chunks(self, chunks: List[Dict], language: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"Processes and saves transcriptions for individual chunks.\"\"\"\n",
    "        transcriptions = []\n",
    "        detected_language = None\n",
    "        for chunk in chunks:\n",
    "            logging.info(f\"Transcribing {chunk['file_path']}\")\n",
    "            transcription, lang = self.transcribe_chunk(chunk[\"file_path\"], language=language)\n",
    "            if transcription:\n",
    "                if detected_language is None and lang is not None:\n",
    "                    detected_language = lang\n",
    "                transcriptions.append({**chunk, \"transcription\": transcription, \"language\": lang})\n",
    "                if self.debug_mode: # only print if debug mode is on\n",
    "                    print(f\"Transcription for {chunk['file_path']} (Language: {lang}):\")\n",
    "                    for segment in transcription[\"segments\"]:\n",
    "                        print(f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "        transcriptions_json_path = os.path.join(self.output_dir, f\"{self.audio_base_name}_transcriptions.json\")\n",
    "        with open(transcriptions_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        return transcriptions, detected_language\n",
    "\n",
    "    def transcribe_whole_audio(self, language: Optional[str] = None) -> Tuple[Dict, Optional[str]]:\n",
    "        \"\"\"Transcribes the entire audio file without diarization.\"\"\"\n",
    "        logging.info(f\"Transcribing the entire audio file: {self.audio_file}\")\n",
    "        return self.transcribe_chunk(self.audio_file, language=language)\n",
    "\n",
    "    def clean_transcription(self, transcriptions_json: str, speaker_names: Dict[str, str]) -> List[str]:\n",
    "        \"\"\"Cleans the transcription JSON to a readable format (for diarized audio).\"\"\"\n",
    "        with open(transcriptions_json, 'r') as f:\n",
    "            transcriptions = json.load(f)\n",
    "\n",
    "        cleaned = []\n",
    "        detected_language = transcriptions[0].get(\"language\") if transcriptions else None\n",
    "        if detected_language:\n",
    "            cleaned.append(f\"Detected Language: {detected_language.upper()}\")\n",
    "            cleaned.append(\"\")\n",
    "        cleaned.append(f\"Audio File: {self.audio_base_name}\")\n",
    "        cleaned.append(\"\")\n",
    "\n",
    "        current_speaker, current_text, current_start, current_end = None, \"\", None, None\n",
    "        for i, chunk in enumerate(transcriptions):\n",
    "            if current_speaker != chunk[\"speaker\"]:\n",
    "                if current_speaker is not None:\n",
    "                    cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "                current_speaker, current_text = chunk[\"speaker\"], \"\"\n",
    "                current_start, current_end = chunk[\"start_time\"], chunk[\"end_time\"]\n",
    "                if i > 0 and transcriptions[i][\"speaker\"] != transcriptions[i-1][\"speaker\"]:\n",
    "                    cleaned.append(\"\") # Add a blank line before a new speaker (after the first)\n",
    "            if chunk[\"transcription\"] and chunk[\"transcription\"][\"segments\"]:\n",
    "                current_text += \" \".join(seg[\"text\"] for seg in chunk[\"transcription\"][\"segments\"])\n",
    "        if current_speaker:\n",
    "            cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "        return [line for line in cleaned if line.strip() != \"\"]\n",
    "\n",
    "    def clean_whole_transcription(self, whole_transcription: Dict, language: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"Cleans the whole transcription output to a readable format (without diarization).\"\"\"\n",
    "        cleaned = []\n",
    "        detected_language = whole_transcription.get(\"language\") if whole_transcription else language\n",
    "        if detected_language:\n",
    "            cleaned.append(f\"Detected Language: {detected_language.upper()}\")\n",
    "            cleaned.append(\"\")\n",
    "        cleaned.append(f\"Audio File: {self.audio_base_name}\")\n",
    "        cleaned.append(\"\")\n",
    "        if whole_transcription and whole_transcription.get(\"segments\"):\n",
    "            for segment in whole_transcription[\"segments\"]:\n",
    "                cleaned.append(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "        return cleaned\n",
    "\n",
    "    def save_transcription_to_file(self, cleaned_transcriptions: List[str], filename=\"transcription.txt\"):\n",
    "        \"\"\"Saves the cleaned transcription to a text file.\"\"\"\n",
    "        output_file = os.path.join(self.output_dir, f\"{self.audio_base_name}_{filename}\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(cleaned_transcriptions))\n",
    "\n",
    "    def process_audio(self, language: Optional[str] = None, min_speakers: Optional[int] = None, max_speakers: Optional[int] = None):\n",
    "        \"\"\"Orchestrates the audio processing pipeline.\"\"\"\n",
    "        try:\n",
    "            if self.skip_diarization:\n",
    "                print(\"Skipping diarization and transcribing the whole audio file.\")\n",
    "                whole_transcription, detected_language = self.transcribe_whole_audio(language=language)\n",
    "                if whole_transcription:\n",
    "                    cleaned_transcriptions = self.clean_whole_transcription(whole_transcription, detected_language)\n",
    "                    print(\"\\nTranscription:\")\n",
    "                    for line in cleaned_transcriptions:\n",
    "                        print(line)\n",
    "                    self.save_transcription_to_file(cleaned_transcriptions, filename=\"whole_transcription.txt\")\n",
    "                else:\n",
    "                    print(\"Transcription failed.\")\n",
    "            else:\n",
    "                pipeline, diarization_result, speaker_names = self.run_diarization(max_speakers=max_speakers, min_speakers=min_speakers)\n",
    "                chunk_info_list = self.chunk_audio(diarization_result)\n",
    "                print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "                num_speakers = len(speaker_names)\n",
    "                print(f\"\\nDetected {num_speakers} speakers:\")\n",
    "                for i, (turn, _, speaker) in enumerate(diarization_result.itertracks(yield_label=True), 1):\n",
    "                    print(f\"Chunk {i}: Speaker '{speaker}' [{turn.start:.2f} - {turn.end:.2f}]\")\n",
    "\n",
    "                rename_speakers = input(\"\\nDo you want to rename the speakers? (yes/no): \").lower()\n",
    "                if rename_speakers == \"yes\":\n",
    "                    new_speaker_names = {}\n",
    "                    for label in sorted(speaker_names.keys()):\n",
    "                        new_name = input(f\"Enter a new name for '{speaker_names[label]}' (default: {speaker_names[label]}): \").strip()\n",
    "                        if new_name:\n",
    "                            new_speaker_names[label] = new_name\n",
    "                        else:\n",
    "                            new_speaker_names[label] = speaker_names[label]\n",
    "                    speaker_names.update(new_speaker_names)\n",
    "\n",
    "                speaker_names_path = os.path.join(self.output_dir, f\"{self.audio_base_name}_speaker_names.json\")\n",
    "                with open(speaker_names_path, \"w\") as f:\n",
    "                    json.dump(speaker_names, f, indent=4)\n",
    "                print(f\"Speaker names saved to {speaker_names_path}\")\n",
    "\n",
    "                transcriptions, detected_language = self.process_and_transcribe_chunks(chunk_info_list, language=language)\n",
    "\n",
    "                transcriptions_json_path = os.path.join(self.output_dir, f\"{self.audio_base_name}_transcriptions.json\")\n",
    "                cleaned_transcriptions = self.clean_transcription(transcriptions_json_path, speaker_names)\n",
    "                print(\"Cleaned Transcriptions:\")\n",
    "                for paragraph in cleaned_transcriptions:\n",
    "                    print(paragraph)\n",
    "\n",
    "                self.save_transcription_to_file(cleaned_transcriptions, filename=\"diarized_transcription.txt\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred: {e}\")\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "def process_single_audio(audio_file_path, output_dir=\"chunks\", hf_token=None, skip_diarization=False, whisper_model=\"base\", language=None, min_speakers=None, max_speakers=None):\n",
    "    transcriber = AudioTranscriber(audio_file_path, output_dir, hf_token, skip_diarization=skip_diarization, whisper_model=whisper_model)\n",
    "    transcriber.process_audio(language=language, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_dir = \"out\"\n",
    "\n",
    "    # Load HF_TOKEN if config.py exists\n",
    "    hf_token = None\n",
    "    try:\n",
    "        from config import HF_TOKEN as config_token\n",
    "        hf_token = config_token\n",
    "    except ImportError:\n",
    "        logging.warning(\n",
    "            \"config.py not found or HF_TOKEN not defined. Diarization may not work.\"\n",
    "            \" Please create a config.py file with HF_TOKEN = 'YOUR_HUGGINGFACE_TOKEN'\"\n",
    "        )\n",
    "\n",
    "    audio_input = input(\"Enter the path to an audio file or a directory containing audio files: \").strip()\n",
    "\n",
    "    if os.path.isdir(audio_input):\n",
    "        audio_files = [f for f in os.listdir(audio_input) if f.endswith(('.mp3', '.wav', '.aac', '.m4a'))]\n",
    "        if not audio_files:\n",
    "            print(\"No audio files found in the specified directory.\")\n",
    "        else:\n",
    "            print(\"\\nAvailable audio files:\")\n",
    "            for i, filename in enumerate(audio_files):\n",
    "                print(f\"{i + 1}. {filename}\")\n",
    "\n",
    "            while True:\n",
    "                selection = input(\"\\nChoose files to process (e.g., 'all', '1', '2,3,4', '1-3'): \").lower().strip()\n",
    "                files_to_process = []\n",
    "\n",
    "                if selection == 'all':\n",
    "                    files_to_process = [os.path.join(audio_input, f) for f in audio_files]\n",
    "                    break\n",
    "                elif ',' in selection:\n",
    "                    indices = [s.strip() for s in selection.split(',')]\n",
    "                    valid_indices = True\n",
    "                    selected_indices = set()\n",
    "                    for index_str in indices:\n",
    "                        if index_str.isdigit():\n",
    "                            index = int(index_str)\n",
    "                            if 1 <= index <= len(audio_files):\n",
    "                                selected_indices.add(index - 1)\n",
    "                            else:\n",
    "                                print(f\"Invalid file number: {index_str}\")\n",
    "                                valid_indices = False\n",
    "                                break\n",
    "                        else:\n",
    "                            print(f\"Invalid input: {index_str}\")\n",
    "                            valid_indices = False\n",
    "                            break\n",
    "                    if valid_indices:\n",
    "                        files_to_process = [os.path.join(audio_input, audio_files[i]) for i in sorted(list(selected_indices))]\n",
    "                        break\n",
    "                elif '-' in selection:\n",
    "                    try:\n",
    "                        start_str, end_str = selection.split('-')\n",
    "                        start_index = int(start_str.strip())\n",
    "                        end_index = int(end_str.strip())\n",
    "                        if 1 <= start_index <= len(audio_files) and 1 <= end_index <= len(audio_files) and start_index <= end_index:\n",
    "                            files_to_process = [os.path.join(audio_input, audio_files[i]) for i in range(start_index - 1, end_index)]\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"Invalid range of file numbers.\")\n",
    "                    except ValueError:\n",
    "                        print(\"Invalid range format.\")\n",
    "                elif selection.isdigit():\n",
    "                    index = int(selection)\n",
    "                    if 1 <= index <= len(audio_files):\n",
    "                        files_to_process = [os.path.join(audio_input, audio_files[index - 1])]\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Invalid file number.\")\n",
    "                else:\n",
    "                    print(\"Invalid selection format. Please use 'all', a single number, comma-separated numbers, or a range (e.g., '1-3').\")\n",
    "\n",
    "            for audio_file_path in files_to_process:\n",
    "                print(f\"\\n--- Processing: {audio_file_path} ---\")\n",
    "                # Ask for processing options for each file\n",
    "                skip_diarization_input = input(\"Skip speaker diarization for this file? (yes/no): \").lower()\n",
    "                skip_diarization = skip_diarization_input == \"yes\"\n",
    "\n",
    "                whisper_model_choice = \"base\"\n",
    "                if not skip_diarization:\n",
    "                    specify_speakers = input(\"Specify min/max speakers for this file? (yes/no): \").lower()\n",
    "                    min_speakers = None\n",
    "                    max_speakers = None\n",
    "                    if specify_speakers == \"yes\":\n",
    "                        try:\n",
    "                            min_speakers = int(input(\"Enter the minimum number of speakers (optional): \") or None)\n",
    "                            max_speakers = int(input(\"Enter the maximum number of speakers (optional): \") or None)\n",
    "                        except ValueError:\n",
    "                            print(\"Invalid input for the number of speakers.\")\n",
    "\n",
    "                    print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "                    chosen_model = input(\"Choose a Whisper model (default: base): \").lower().strip()\n",
    "                    if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "                        whisper_model_choice = chosen_model\n",
    "                    elif chosen_model:\n",
    "                        print(f\"Invalid model '{chosen_model}'. Using default 'base'.\")\n",
    "                else:\n",
    "                    print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "                    chosen_model = input(\"Choose a Whisper model (default: base): \").lower().strip()\n",
    "                    if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "                        whisper_model_choice = chosen_model\n",
    "                    elif chosen_model:\n",
    "                        print(f\"Invalid model '{chosen_model}'. Using default 'base'.\")\n",
    "\n",
    "                specify_language = input(\"Specify a language for this file? (yes/no): \").lower()\n",
    "                transcription_language = None\n",
    "                if specify_language == \"yes\":\n",
    "                    transcription_language = input(\"Enter the language code (e.g., en, fr, es): \").strip()\n",
    "\n",
    "                process_single_audio(audio_file_path, output_dir, hf_token, skip_diarization, whisper_model_choice, transcription_language, min_speakers, max_speakers)\n",
    "\n",
    "    elif os.path.isfile(audio_input):\n",
    "        audio_file = audio_input\n",
    "        # Ask for processing options for the single file\n",
    "        skip_diarization_input = input(\"Skip speaker diarization? (yes/no): \").lower()\n",
    "        skip_diarization = skip_diarization_input == \"yes\"\n",
    "\n",
    "        whisper_model_choice = \"base\"\n",
    "        if not skip_diarization:\n",
    "            specify_speakers = input(\"Specify min/max speakers? (yes/no): \").lower()\n",
    "            min_speakers = None\n",
    "            max_speakers = None\n",
    "            if specify_speakers == \"yes\":\n",
    "                try:\n",
    "                    min_speakers = int(input(\"Enter the minimum number of speakers (optional): \") or None)\n",
    "                    max_speakers = int(input(\"Enter the maximum number of speakers (optional): \") or None)\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input for the number of speakers.\")\n",
    "\n",
    "            print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "            chosen_model = input(\"Choose a Whisper model (default: base): \").lower().strip()\n",
    "            if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "                whisper_model_choice = chosen_model\n",
    "            elif chosen_model:\n",
    "                print(f\"Invalid model '{chosen_model}'. Using default 'base'.\")\n",
    "        else:\n",
    "            print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "            chosen_model = input(\"Choose a Whisper model (default: base): \").lower().strip()\n",
    "            if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "                whisper_model_choice = chosen_model\n",
    "            elif chosen_model:\n",
    "                print(f\"Invalid model '{chosen_model}'. Using default 'base'.\")\n",
    "\n",
    "        specify_language = input(\"Specify a language? (yes/no): \").lower()\n",
    "        transcription_language = None\n",
    "        if specify_language == \"yes\":\n",
    "            transcription_language = input(\"Enter the language code (e.g., en, fr, es): \").strip()\n",
    "\n",
    "        process_single_audio(audio_file, output_dir, hf_token, skip_diarization, whisper_model_choice, transcription_language, min_speakers, max_speakers)\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid input. Please provide a valid file path or directory path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sopra funziona, sotto rprovo a rimuovere i silenzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available audio files:\n",
      "1. 11 lug, 11.20​ prova 2.aac\n",
      "2. 4547.mp3\n",
      "3. 6313.mp3\n",
      "4. Botanicario - Ribes Nero.wav\n",
      "5. Come STUDIARE allUNIVERSITÀ.mp3\n",
      "6. Come STUDIARE allUNIVERSITÀ.wav\n",
      "7. test.mp3\n",
      "\n",
      "--- Processing: C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3 ---\n",
      "\n",
      "Available Whisper models: tiny, base, small, medium, large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 22:31:50,445 - INFO - Using device: cpu\n",
      "2025-03-23 22:31:51,580 - INFO - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "2025-03-23 22:31:51,581 - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1007: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file diarized and chunked.\n",
      "\n",
      "Detected 2 speakers:\n",
      "Chunk 1: Speaker 'SPEAKER_00' [0.03 - 10.00]\n",
      "Chunk 2: Speaker 'SPEAKER_01' [11.42 - 12.82]\n",
      "Chunk 3: Speaker 'SPEAKER_00' [12.82 - 14.91]\n",
      "Chunk 4: Speaker 'SPEAKER_00' [16.94 - 25.63]\n",
      "Chunk 5: Speaker 'SPEAKER_01' [25.58 - 27.37]\n",
      "Chunk 6: Speaker 'SPEAKER_00' [27.37 - 28.97]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 22:32:27,732 - INFO - Transcribing out\\test\\chunk_1.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker names saved to out\\test\\test_speaker_names.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 22:32:29,240 - INFO - Performing voice activity detection with settings: True\n",
      "2025-03-23 22:32:29,251 - WARNING - Please install onnxruntime to use more efficiently silero VAD\n",
      "c:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\.venv\\Lib\\site-packages\\torch\\hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to C:\\Users\\Admin/.cache\\torch\\hub\\master.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [00:04<00:00, 208.87frames/s]\n",
      "2025-03-23 22:32:39,925 - INFO - Transcribing out\\test\\chunk_2.mp3\n",
      "2025-03-23 22:32:41,198 - INFO - Performing voice activity detection with settings: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:01<00:00, 71.86frames/s]\n",
      "2025-03-23 22:32:44,887 - INFO - Transcribing out\\test\\chunk_3.mp3\n",
      "2025-03-23 22:32:46,103 - INFO - Performing voice activity detection with settings: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145/145 [00:02<00:00, 71.26frames/s]\n",
      "2025-03-23 22:32:49,890 - INFO - Transcribing out\\test\\chunk_4.mp3\n",
      "2025-03-23 22:32:51,119 - INFO - Performing voice activity detection with settings: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:02<00:00, 295.27frames/s]\n",
      "2025-03-23 22:32:55,958 - INFO - Transcribing out\\test\\chunk_5.mp3\n",
      "2025-03-23 22:32:57,177 - INFO - Performing voice activity detection with settings: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:02<00:00, 83.66frames/s]\n",
      "2025-03-23 22:33:01,118 - INFO - Transcribing out\\test\\chunk_6.mp3\n",
      "2025-03-23 22:33:02,366 - INFO - Performing voice activity detection with settings: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:02<00:00, 67.67frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Transcriptions:\n",
      "Detected Language: EN\n",
      "Audio File: test\n",
      "Speaker 1 [0.03 - 10.00]:  Anyway, look where we're digressing the rules.  Oh, simple, Emma, you're about to face five questions  of increasing difficulty.  You must answer as quickly as possible.  If you get it correct, you move onto the next round.  Do you know what happens if you get it wrong?\n",
      "Speaker 2 [11.42 - 12.82]:  and correction and embarrassment.\n",
      "Speaker 1 [12.82 - 14.91]:  to indeed.  Brown. Round 1 astronomers are saying that Saturn's rings are slowly disappearing.  They estimate we only have a few hundred million years left of them.\n",
      "Speaker 2 [25.58 - 27.37]:  I'll earn you a few hundred million.\n",
      "Speaker 1 [27.37 - 28.97]:  But what I want to know?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Annotation\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import whisper_timestamped as whisper\n",
    "\n",
    "class AudioTranscriber:\n",
    "    def __init__(self, audio_file: str, output_dir: str = \"chunks\", hf_token: Optional[str] = None, skip_diarization: bool = False, whisper_model: str = \"base\"):\n",
    "        self.audio_file = audio_file\n",
    "        self.audio_base_name = os.path.splitext(os.path.basename(self.audio_file))[0]\n",
    "        self.output_dir_base = output_dir\n",
    "        self.output_dir = os.path.join(self.output_dir_base, self.audio_base_name)\n",
    "        self.hf_token = hf_token\n",
    "        self.whisper_model = whisper_model\n",
    "        self.debug_mode = False\n",
    "        self.skip_diarization = skip_diarization\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def load_audio(self, audio_file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Loads an audio file using torchaudio.\"\"\"\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "            return waveform, sample_rate\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading audio file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_diarization(\n",
    "        self,\n",
    "        max_speakers: Optional[int] = None,\n",
    "        min_speakers: Optional[int] = None,\n",
    "    ) -> Tuple[Pipeline, Annotation, Dict[str, str]]:\n",
    "        \"\"\"Performs speaker diarization.\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(f\"Using device: {device}\")\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\", use_auth_token=self.hf_token\n",
    "        ).to(device)\n",
    "        waveform, sample_rate = self.load_audio(self.audio_file)\n",
    "        input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "        if max_speakers:\n",
    "            input_data[\"max_speakers\"] = max_speakers\n",
    "        if min_speakers:\n",
    "            input_data[\"min_speakers\"] = min_speakers\n",
    "        diarization = pipeline(input_data)\n",
    "        speaker_labels = set()\n",
    "        for segment, _, label in diarization.itertracks(yield_label=True):\n",
    "            speaker_labels.add(label)\n",
    "        speaker_names = {\n",
    "            label: f\"Speaker {i + 1}\" for i, label in enumerate(sorted(speaker_labels))\n",
    "        }\n",
    "        return pipeline, diarization, speaker_names\n",
    "\n",
    "    def chunk_audio(self, diarization: Annotation) -> List[Dict]:\n",
    "        \"\"\"Chunks audio based on diarization.\"\"\"\n",
    "        audio = AudioSegment.from_file(self.audio_file)\n",
    "        chunks = []\n",
    "        for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True), 1):\n",
    "            start_ms, end_ms = int(turn.start * 1000), int(turn.end * 1000)\n",
    "            chunk_path = os.path.join(self.output_dir, f\"chunk_{i}.mp3\")\n",
    "            audio[start_ms:end_ms].export(chunk_path, format=\"mp3\")\n",
    "            chunks.append({\"file_path\": chunk_path, \"speaker\": speaker, \"start_time\": turn.start, \"end_time\": turn.end})\n",
    "        return chunks\n",
    "\n",
    "    def transcribe_chunk(self, audio_file_path: str, language: Optional[str] = None, vad: Optional[bool or str or List[Tuple[float, float]]] = None) -> Tuple[Dict, Optional[str]]:\n",
    "            \"\"\"Transcribes an audio chunk and returns the transcription and detected language.\"\"\"\n",
    "            try:\n",
    "                model = whisper.load_model(self.whisper_model)\n",
    "                audio = whisper.load_audio(audio_file_path)\n",
    "\n",
    "                if vad is not None and vad is not False:\n",
    "                    logging.info(f\"Performing voice activity detection with settings: {vad}\")\n",
    "                    if vad is True or vad == \"silero\":\n",
    "                        result = whisper.transcribe(model, audio, language=language, vad=\"silero\")\n",
    "                    elif vad == \"silero:3.1\":\n",
    "                        result = whisper.transcribe(model, audio, language=language, vad=\"silero:3.1\")\n",
    "                    elif vad == \"auditok\":\n",
    "                        result = whisper.transcribe(model, audio, language=language, vad=\"auditok\")\n",
    "                    elif isinstance(vad, list):\n",
    "                        speech_segments = []\n",
    "                        for start, end in vad:\n",
    "                            speech_segments.append(audio[int(start * whisper.audio.SAMPLE_RATE):int(end * whisper.audio.SAMPLE_RATE)])\n",
    "                        if speech_segments:\n",
    "                            full_transcription = {\"segments\":\"\"}\n",
    "                            for segment in speech_segments:\n",
    "                                segment_result = whisper.transcribe(model, segment, language=language)\n",
    "                                full_transcription[\"segments\"].extend(segment_result.get(\"segments\",))\n",
    "                            result = full_transcription\n",
    "                        else:\n",
    "                            result = {\"segments\":\"\"} # No speech segments provided\n",
    "                    else:\n",
    "                        logging.warning(f\"Invalid VAD setting: {vad}. Transcribing without VAD.\")\n",
    "                        result = whisper.transcribe(model, audio, language=language)\n",
    "                else:\n",
    "                    result = whisper.transcribe(model, audio, language=language)\n",
    "\n",
    "                detected_language = result.get(\"language\") if language is None else language\n",
    "                return result, detected_language\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Transcription error: {e}\")\n",
    "                return {}, None\n",
    "\n",
    "    def process_and_transcribe_chunks(self, chunks: List[Dict], language: Optional[str] = None, use_vad: bool = False, vad_method: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"Processes and saves transcriptions for individual chunks.\"\"\"\n",
    "        transcriptions = []\n",
    "        detected_language = None\n",
    "        for chunk in chunks:\n",
    "            logging.info(f\"Transcribing {chunk['file_path']}\")\n",
    "            vad_option = None\n",
    "            if use_vad:\n",
    "                vad_option = vad_method if vad_method else True # Use default if no method specified\n",
    "            transcription, lang = self.transcribe_chunk(chunk[\"file_path\"], language=language, vad=vad_option)\n",
    "            if transcription:\n",
    "                if detected_language is None and lang is not None:\n",
    "                    detected_language = lang\n",
    "                transcriptions.append({**chunk, \"transcription\": transcription, \"language\": lang})\n",
    "                if self.debug_mode: # only print if debug mode is on\n",
    "                    print(f\"Transcription for {chunk['file_path']} (Language: {lang}):\")\n",
    "                    for segment in transcription[\"segments\"]:\n",
    "                        print(f\"     [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "        transcriptions_json_path = os.path.join(self.output_dir, f\"{self.audio_base_name}_transcriptions.json\")\n",
    "        with open(transcriptions_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        return transcriptions, detected_language\n",
    "\n",
    "    def transcribe_whole_audio(self, language: Optional[str] = None, use_vad: bool = False, vad_method: Optional[str] = None) -> Tuple[Dict, Optional[str]]:\n",
    "        \"\"\"Transcribes the entire audio file without diarization.\"\"\"\n",
    "        logging.info(f\"Transcribing the entire audio file: {self.audio_file}\")\n",
    "        vad_option = None\n",
    "        if use_vad:\n",
    "            vad_option = vad_method if vad_method else True\n",
    "        return self.transcribe_chunk(self.audio_file, language=language, vad=vad_option)\n",
    "\n",
    "    def clean_transcription(self, transcriptions_json: str, speaker_names: Dict[str, str]) -> List[str]:\n",
    "        \"\"\"Cleans the transcription JSON to a readable format (for diarized audio).\"\"\"\n",
    "        with open(transcriptions_json, 'r') as f:\n",
    "            transcriptions = json.load(f)\n",
    "\n",
    "        cleaned = []\n",
    "        detected_language = transcriptions[0].get(\"language\") if transcriptions else None\n",
    "        if detected_language:\n",
    "            cleaned.append(f\"Detected Language: {detected_language.upper()}\")\n",
    "            cleaned.append(\"\")\n",
    "        cleaned.append(f\"Audio File: {self.audio_base_name}\")\n",
    "        cleaned.append(\"\")\n",
    "\n",
    "        current_speaker, current_text, current_start, current_end = None, \"\", None, None\n",
    "        for i, chunk in enumerate(transcriptions):\n",
    "            if current_speaker != chunk[\"speaker\"]:\n",
    "                if current_speaker is not None:\n",
    "                    cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "                current_speaker, current_text = chunk[\"speaker\"], \"\"\n",
    "                current_start, current_end = chunk[\"start_time\"], chunk[\"end_time\"]\n",
    "                if i > 0 and transcriptions[i][\"speaker\"] != transcriptions[i-1][\"speaker\"]:\n",
    "                    cleaned.append(\"\") # Add a blank line before a new speaker (after the first)\n",
    "            if chunk[\"transcription\"] and chunk[\"transcription\"][\"segments\"]:\n",
    "                current_text += \" \".join(seg[\"text\"] for seg in chunk[\"transcription\"][\"segments\"])\n",
    "        if current_speaker:\n",
    "            cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "        return [line for line in cleaned if line.strip() != \"\"]\n",
    "\n",
    "    def clean_whole_transcription(self, whole_transcription: Dict, language: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"Cleans the whole transcription output to a readable format (without diarization).\"\"\"\n",
    "        cleaned = []\n",
    "        detected_language = whole_transcription.get(\"language\") if whole_transcription else language\n",
    "        if detected_language:\n",
    "            cleaned.append(f\"Detected Language: {detected_language.upper()}\")\n",
    "            cleaned.append(\"\")\n",
    "        cleaned.append(f\"Audio File: {self.audio_base_name}\")\n",
    "        cleaned.append(\"\")\n",
    "        if whole_transcription and whole_transcription.get(\"segments\"):\n",
    "            for segment in whole_transcription[\"segments\"]:\n",
    "                cleaned.append(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "        return cleaned\n",
    "\n",
    "    def save_transcription_to_file(self, cleaned_transcriptions: List[str], filename=\"transcription.txt\"):\n",
    "        \"\"\"Saves the cleaned transcription to a text file.\"\"\"\n",
    "        output_file = os.path.join(self.output_dir, f\"{self.audio_base_name}_{filename}\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(cleaned_transcriptions))\n",
    "\n",
    "    def process_audio(self, language: Optional[str] = None, min_speakers: Optional[int] = None, max_speakers: Optional[int] = None):\n",
    "        \"\"\"Orchestrates the audio processing pipeline.\"\"\"\n",
    "        try:\n",
    "            use_vad = input(\"Do you want to remove silent parts before transcription? (yes/no): \").lower() == \"yes\"\n",
    "            vad_method = None\n",
    "            if use_vad:\n",
    "                vad_choice = input(\"Choose VAD method (silero, silero:3.1, auditok, or leave empty for default silero): \").lower().strip()\n",
    "                if vad_choice in [\"silero\", \"silero:3.1\", \"auditok\"]:\n",
    "                    vad_method = vad_choice\n",
    "                elif vad_choice:\n",
    "                    print(f\"Invalid VAD method '{vad_choice}'. Using default silero.\")\n",
    "\n",
    "            if self.skip_diarization:\n",
    "                print(\"Skipping diarization and transcribing the whole audio file.\")\n",
    "                whole_transcription, detected_language = self.transcribe_whole_audio(language=language, use_vad=use_vad, vad_method=vad_method)\n",
    "                if whole_transcription:\n",
    "                    cleaned_transcriptions = self.clean_whole_transcription(whole_transcription, detected_language)\n",
    "                    print(\"\\nTranscription:\")\n",
    "                    for line in cleaned_transcriptions:\n",
    "                        print(line)\n",
    "                    self.save_transcription_to_file(cleaned_transcriptions, filename=\"whole_transcription.txt\")\n",
    "                else:\n",
    "                    print(\"Transcription failed.\")\n",
    "            else:\n",
    "                pipeline, diarization_result, speaker_names = self.run_diarization(max_speakers=max_speakers, min_speakers=min_speakers)\n",
    "                chunk_info_list = self.chunk_audio(diarization_result)\n",
    "                print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "                num_speakers = len(speaker_names)\n",
    "                print(f\"\\nDetected {num_speakers} speakers:\")\n",
    "                for i, (turn, _, speaker) in enumerate(diarization_result.itertracks(yield_label=True), 1):\n",
    "                    print(f\"Chunk {i}: Speaker '{speaker}' [{turn.start:.2f} - {turn.end:.2f}]\")\n",
    "\n",
    "                rename_speakers = input(\"\\nDo you want to rename the speakers? (yes/no): \").lower()\n",
    "                if rename_speakers == \"yes\":\n",
    "                    new_speaker_names = {}\n",
    "                    for label in sorted(speaker_names.keys()):\n",
    "                        new_name = input(f\"Enter a new name for '{speaker_names[label]}' (default: {speaker_names[label]}): \").strip()\n",
    "                        if new_name:\n",
    "                            new_speaker_names[label] = new_name\n",
    "                        else:\n",
    "                            new_speaker_names[label] = speaker_names[label]\n",
    "                    speaker_names.update(new_speaker_names)\n",
    "\n",
    "                speaker_names_path = os.path.join(self.output_dir, f\"{self.audio_base_name}_speaker_names.json\")\n",
    "                with open(speaker_names_path, \"w\") as f:\n",
    "                    json.dump(speaker_names, f, indent=4)\n",
    "                print(f\"Speaker names saved to {speaker_names_path}\")\n",
    "\n",
    "                transcriptions, detected_language = self.process_and_transcribe_chunks(chunk_info_list, language=language, use_vad=use_vad, vad_method=vad_method)\n",
    "\n",
    "                transcriptions_json_path = os.path.join(self.output_dir, f\"{self.audio_base_name}_transcriptions.json\")\n",
    "                cleaned_transcriptions = self.clean_transcription(transcriptions_json_path, speaker_names)\n",
    "                print(\"Cleaned Transcriptions:\")\n",
    "                for paragraph in cleaned_transcriptions:\n",
    "                    print(paragraph)\n",
    "\n",
    "                self.save_transcription_to_file(cleaned_transcriptions, filename=\"diarized_transcription.txt\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred: {e}\")\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "def process_single_audio(audio_file_path, output_dir=\"chunks\", hf_token=None, skip_diarization=False, whisper_model=\"base\", language=None, min_speakers=None, max_speakers=None):\n",
    "    transcriber = AudioTranscriber(audio_file_path, output_dir, hf_token, skip_diarization=skip_diarization, whisper_model=whisper_model)\n",
    "    transcriber.process_audio(language=language, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_dir = \"out\"\n",
    "\n",
    "    # Load HF_TOKEN if config.py exists\n",
    "    hf_token = None\n",
    "    try:\n",
    "        from config import HF_TOKEN as config_token\n",
    "        hf_token = config_token\n",
    "    except ImportError:\n",
    "        logging.warning(\n",
    "            \"config.py not found or HF_TOKEN not defined. Diarization may not work.\"\n",
    "            \" Please create a config.py file with HF_TOKEN = 'YOUR_HUGGINGFACE_TOKEN'\"\n",
    "        )\n",
    "\n",
    "    audio_input = input(\"Enter the path to an audio file or a directory containing audio files: \").strip()\n",
    "\n",
    "    if os.path.isdir(audio_input):\n",
    "        audio_files = [f for f in os.listdir(audio_input) if f.endswith(('.mp3', '.wav', '.aac', '.m4a'))]\n",
    "        if not audio_files:\n",
    "            print(\"No audio files found in the specified directory.\")\n",
    "        else:\n",
    "            print(\"\\nAvailable audio files:\")\n",
    "            for i, filename in enumerate(audio_files):\n",
    "                print(f\"{i + 1}. {filename}\")\n",
    "\n",
    "            while True:\n",
    "                selection = input(\"\\nChoose files to process (e.g., 'all', '1', '2,3,4', '1-3'): \").lower().strip()\n",
    "                files_to_process = []\n",
    "\n",
    "                if selection == 'all':\n",
    "                    files_to_process = [os.path.join(audio_input, f) for f in audio_files]\n",
    "                    break\n",
    "                elif ',' in selection:\n",
    "                    indices = [s.strip() for s in selection.split(',')]\n",
    "                    valid_indices = True\n",
    "                    selected_indices = set()\n",
    "                    for index_str in indices:\n",
    "                        if index_str.isdigit():\n",
    "                            index = int(index_str)\n",
    "                            if 1 <= index <= len(audio_files):\n",
    "                                selected_indices.add(index - 1)\n",
    "                            else:\n",
    "                                print(f\"Invalid file number: {index_str}\")\n",
    "                                valid_indices = False\n",
    "                                break\n",
    "                        else:\n",
    "                            print(f\"Invalid input: {index_str}\")\n",
    "                            valid_indices = False\n",
    "                            break\n",
    "                    if valid_indices:\n",
    "                        files_to_process = [os.path.join(audio_input, audio_files[i]) for i in sorted(list(selected_indices))]\n",
    "                        break\n",
    "                elif '-' in selection:\n",
    "                    try:\n",
    "                        start_str, end_str = selection.split('-')\n",
    "                        start_index = int(start_str.strip())\n",
    "                        end_index = int(end_str.strip())\n",
    "                        if 1 <= start_index <= len(audio_files) and 1 <= end_index <= len(audio_files) and start_index <= end_index:\n",
    "                            files_to_process = [os.path.join(audio_input, audio_files[i]) for i in range(start_index - 1, end_index)]\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"Invalid range of file numbers.\")\n",
    "                    except ValueError:\n",
    "                        print(\"Invalid range format.\")\n",
    "                elif selection.isdigit():\n",
    "                    index = int(selection)\n",
    "                    if 1 <= index <= len(audio_files):\n",
    "                        files_to_process = [os.path.join(audio_input, audio_files[index - 1])]\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Invalid file number.\")\n",
    "                else:\n",
    "                    print(\"Invalid selection format. Please use 'all', a single number, comma-separated numbers, or a range (e.g., '1-3').\")\n",
    "\n",
    "            for audio_file_path in files_to_process:\n",
    "                print(f\"\\n--- Processing: {audio_file_path} ---\")\n",
    "                # Ask for processing options for each file\n",
    "                skip_diarization_input = input(\"Skip speaker diarization for this file? (yes/no): \").lower()\n",
    "                skip_diarization = skip_diarization_input == \"yes\"\n",
    "\n",
    "                whisper_model_choice = \"base\"\n",
    "                if not skip_diarization:\n",
    "                    specify_speakers = input(\"Specify min/max speakers for this file? (yes/no): \").lower()\n",
    "                    min_speakers = None\n",
    "                    max_speakers = None\n",
    "                    if specify_speakers == \"yes\":\n",
    "                        try:\n",
    "                            min_speakers = int(input(\"Enter the minimum number of speakers (optional): \") or None)\n",
    "                            max_speakers = int(input(\"Enter the maximum number of speakers (optional): \") or None)\n",
    "                        except ValueError:\n",
    "                            print(\"Invalid input for the number of speakers.\")\n",
    "\n",
    "                    print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "                    chosen_model = input(\"Choose a Whisper model (default: base): \").lower().strip()\n",
    "                    if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "                        whisper_model_choice = chosen_model\n",
    "                    elif chosen_model:\n",
    "                        print(f\"Invalid model '{chosen_model}'. Using default 'base'.\")\n",
    "                else:\n",
    "                    print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "                    chosen_model = input(\"Choose a Whisper model (default: base): \").lower().strip()\n",
    "                    if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "                        whisper_model_choice = chosen_model\n",
    "                    elif chosen_model:\n",
    "                        print(f\"Invalid model '{chosen_model}'. Using default 'base'.\")\n",
    "\n",
    "                specify_language = input(\"Specify a language for this file? (yes/no): \").lower()\n",
    "                transcription_language = None\n",
    "                if specify_language == \"yes\":\n",
    "                    transcription_language = input(\"Enter the language code (e.g., en, fr, es): \").strip()\n",
    "\n",
    "                process_single_audio(audio_file_path, output_dir, hf_token, skip_diarization, whisper_model_choice, transcription_language, min_speakers, max_speakers)\n",
    "\n",
    "    elif os.path.isfile(audio_input):\n",
    "        audio_file = audio_input\n",
    "        # Ask for processing options for the single file\n",
    "        skip_diarization_input = input(\"Skip speaker diarization? (yes/no): \").lower()\n",
    "        skip_diarization = skip_diarization_input == \"yes\"\n",
    "\n",
    "        whisper_model_choice = \"base\"\n",
    "        if not skip_diarization:\n",
    "            specify_speakers = input(\"Specify min/max speakers? (yes/no): \").lower()\n",
    "            min_speakers = None\n",
    "            max_speakers = None\n",
    "            if specify_speakers == \"yes\":\n",
    "                try:\n",
    "                    min_speakers = int(input(\"Enter the minimum number of speakers (optional): \") or None)\n",
    "                    max_speakers = int(input(\"Enter the maximum number of speakers (optional): \") or None)\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input for the number of speakers.\")\n",
    "\n",
    "            print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "            chosen_model = input(\"Choose a Whisper model (default: base): \").lower().strip()\n",
    "            if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "                whisper_model_choice = chosen_model\n",
    "            elif chosen_model:\n",
    "                print(f\"Invalid model '{chosen_model}'. Using default 'base'.\")\n",
    "        else:\n",
    "            print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "            chosen_model = input(\"Choose a Whisper model (default: base): \").lower().strip()\n",
    "            if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "                whisper_model_choice = chosen_model\n",
    "            elif chosen_model:\n",
    "                print(f\"Invalid model '{chosen_model}'. Using default 'base'.\")\n",
    "\n",
    "        specify_language = input(\"Specify a language? (yes/no): \").lower()\n",
    "        transcription_language = None\n",
    "        if specify_language == \"yes\":\n",
    "            transcription_language = input(\"Enter the language code (e.g., en, fr, es): \").strip()\n",
    "\n",
    "        process_single_audio(audio_file, output_dir, hf_token, skip_diarization, whisper_model_choice, transcription_language, min_speakers, max_speakers)\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid input. Please provide a valid file path or directory path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available audio files:\n",
      "1. 11 lug, 11.20​ prova 2.aac\n",
      "2. 4547.mp3\n",
      "3. 6313.mp3\n",
      "4. Botanicario - Ribes Nero.wav\n",
      "5. Come STUDIARE allUNIVERSITÀ.mp3\n",
      "6. Come STUDIARE allUNIVERSITÀ.wav\n",
      "7. test.mp3\n",
      "\n",
      "--- Processing: C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3 ---\n",
      "\n",
      "Available Whisper models: tiny, base, small, medium, large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 20:53:19,705 - INFO - Transcribing the entire audio file: C:\\Users\\Admin\\Documents\\Coding\\Transcriptor\\audio\\test.mp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping diarization and transcribing the whole audio file.\n",
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2880/2880 [00:07<00:00, 376.79frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription:\n",
      "Detected Language: EN\n",
      "\n",
      "Audio File: test\n",
      "\n",
      "[0.00 - 1.62]  Anyway, look, we're digressing the rules.\n",
      "[1.74 - 4.34]  Oh, simple, Emma, you're about to face five questions.\n",
      "[4.36 - 5.34]  It's been increasing difficulty.\n",
      "[5.34 - 6.60]  You must answer as quickly as possible.\n",
      "[6.66 - 8.34]  If you get it correct, you move onto the next round.\n",
      "[8.62 - 9.78]  Do you know what happens if you get it wrong?\n",
      "[11.44 - 12.42]  I've got an embarrassment.\n",
      "[12.78 - 13.31]  You do indeed.\n",
      "[13.31 - 14.78]  Round one.\n",
      "[16.98 - 19.90]  Round one, astronomers are saying that Saturn's rings\n",
      "[20.02 - 21.44]  are slowly disappearing.\n",
      "[21.48 - 25.62]  They estimate we only have a few hundred million years left of them.\n",
      "[25.64 - 27.10]  I'll only have a few hundred million.\n",
      "[27.46 - 28.44]  But what I want to know?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Annotation\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import whisper_timestamped as whisper\n",
    "\n",
    "class AudioTranscriber:\n",
    "    def __init__(self, audio_file: str, output_dir: str = \"chunks\", hf_token: Optional[str] = None, skip_diarization: bool = False, whisper_model: str = \"base\"):\n",
    "        self.audio_file = audio_file\n",
    "        self.audio_base_name = os.path.splitext(os.path.basename(self.audio_file))[0]\n",
    "        self.output_dir_base = output_dir  # Base output directory\n",
    "        self.output_dir = os.path.join(self.output_dir_base, self.audio_base_name) # final output dir\n",
    "        self.hf_token = hf_token\n",
    "        self.whisper_model = whisper_model\n",
    "        self.debug_mode = False\n",
    "        self.skip_diarization = skip_diarization\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def load_audio(self, audio_file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Loads an audio file using torchaudio.\"\"\"\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "            return waveform, sample_rate\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading audio file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_diarization(\n",
    "        self,\n",
    "        max_speakers: Optional[int] = None,\n",
    "        min_speakers: Optional[int] = None,\n",
    "    ) -> Tuple[Pipeline, Annotation, Dict[str, str]]:\n",
    "        \"\"\"Performs speaker diarization.\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(f\"Using device: {device}\")\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\", use_auth_token=self.hf_token\n",
    "        ).to(device)\n",
    "        waveform, sample_rate = self.load_audio(self.audio_file)\n",
    "        input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "        if max_speakers:\n",
    "            input_data[\"max_speakers\"] = max_speakers\n",
    "        if min_speakers:\n",
    "            input_data[\"min_speakers\"] = min_speakers\n",
    "        diarization = pipeline(input_data)\n",
    "        speaker_labels = set()\n",
    "        for segment, _, label in diarization.itertracks(yield_label=True):\n",
    "            speaker_labels.add(label)\n",
    "        speaker_names = {\n",
    "            label: f\"Speaker {i + 1}\" for i, label in enumerate(sorted(speaker_labels))\n",
    "        }\n",
    "        return pipeline, diarization, speaker_names\n",
    "\n",
    "    def chunk_audio(self, diarization: Annotation) -> List[Dict]:\n",
    "        \"\"\"Chunks audio based on diarization.\"\"\"\n",
    "        audio = AudioSegment.from_file(self.audio_file)\n",
    "        chunks = []\n",
    "        for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True), 1):\n",
    "            start_ms, end_ms = int(turn.start * 1000), int(turn.end * 1000)\n",
    "            chunk_path = os.path.join(self.output_dir, f\"chunk_{i}.mp3\")\n",
    "            audio[start_ms:end_ms].export(chunk_path, format=\"mp3\")\n",
    "            chunks.append({\"file_path\": chunk_path, \"speaker\": speaker, \"start_time\": turn.start, \"end_time\": turn.end})\n",
    "        return chunks\n",
    "\n",
    "    def transcribe_chunk(self, audio_file_path: str, language: Optional[str] = None, vad: Optional[bool or str or List[Tuple[float, float]]] = None, verbose: bool = False, plot_word_alignment: bool = False, detect_disfluencies: bool = False) -> Tuple[Dict, Optional[str]]:\n",
    "        \"\"\"Transcribes an audio chunk and returns the transcription and detected language.\"\"\"\n",
    "        try:\n",
    "            model = whisper.load_model(self.whisper_model)\n",
    "            audio = whisper.load_audio(audio_file_path)\n",
    "\n",
    "            if vad is not None and vad is not False:\n",
    "                logging.info(f\"Performing voice activity detection with settings: {vad}\")\n",
    "                if vad is True or vad == \"silero\":\n",
    "                    result = whisper.transcribe(model, audio, language=language, vad=\"silero\", verbose=verbose, plot_word_alignment=plot_word_alignment, detect_disfluencies=detect_disfluencies)\n",
    "                elif vad == \"silero:3.1\":\n",
    "                    result = whisper.transcribe(model, audio, language=language, vad=\"silero:3.1\", verbose=verbose, plot_word_alignment=plot_word_alignment, detect_disfluencies=detect_disfluencies)\n",
    "                elif vad == \"auditok\":\n",
    "                    result = whisper.transcribe(model, audio, language=language, vad=\"auditok\", verbose=verbose, plot_word_alignment=plot_word_alignment, detect_disfluencies=detect_disfluencies)\n",
    "                elif isinstance(vad, list):\n",
    "                    speech_segments = []\n",
    "                    for start, end in vad:\n",
    "                        speech_segments.append(audio[int(start * whisper.audio.SAMPLE_RATE):int(end * whisper.audio.SAMPLE_RATE)])\n",
    "                    if speech_segments:\n",
    "                        full_transcription = {\"segments\":\"\"}\n",
    "                        for segment in speech_segments:\n",
    "                            segment_result = whisper.transcribe(model, segment, language=language, verbose=verbose, plot_word_alignment=plot_word_alignment, detect_disfluencies=detect_disfluencies)\n",
    "                            full_transcription[\"segments\"].extend(segment_result.get(\"segments\",))\n",
    "                        result = full_transcription\n",
    "                else:\n",
    "                    logging.warning(f\"Invalid VAD setting: {vad}. Transcribing without VAD.\")\n",
    "                    result = whisper.transcribe(model, audio, language=language, verbose=verbose, plot_word_alignment=plot_word_alignment, detect_disfluencies=detect_disfluencies)\n",
    "            else:\n",
    "                result = whisper.transcribe(model, audio, language=language, verbose=verbose, plot_word_alignment=plot_word_alignment, detect_disfluencies=detect_disfluencies)\n",
    "\n",
    "            detected_language = result.get(\"language\") if language is None else language\n",
    "            return result, detected_language\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Transcription error: {e}\")\n",
    "            return {}, None\n",
    "\n",
    "    def process_and_transcribe_chunks(self, chunks: List[Dict], language: Optional[str] = None, use_vad: bool = False, vad_method: Optional[str] = None, verbose: bool = False, plot_word_alignment: bool = False, detect_disfluencies: bool = False) -> List[Dict]:\n",
    "        \"\"\"Processes and saves transcriptions for individual chunks.\"\"\"\n",
    "        transcriptions = []\n",
    "        detected_language = None\n",
    "        for chunk in chunks:\n",
    "            logging.info(f\"Transcribing {chunk['file_path']}\")\n",
    "            vad_option = None\n",
    "            if use_vad:\n",
    "                vad_option = vad_method if vad_method else True # Use default if no method specified\n",
    "            transcription, lang = self.transcribe_chunk(chunk[\"file_path\"], language=language, vad=vad_option, verbose=verbose, plot_word_alignment=plot_word_alignment, detect_disfluencies=detect_disfluencies)\n",
    "            if transcription:\n",
    "                if detected_language is None and lang is not None:\n",
    "                    detected_language = lang\n",
    "                transcriptions.append({**chunk, \"transcription\": transcription, \"language\": lang})\n",
    "                if self.debug_mode: # only print if debug mode is on\n",
    "                    print(f\"Transcription for {chunk['file_path']} (Language: {lang}):\")\n",
    "                    for segment in transcription[\"segments\"]:\n",
    "                        print(f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "        transcriptions_json_path = os.path.join(self.output_dir, f\"{self.audio_base_name}_transcriptions.json\")\n",
    "        with open(transcriptions_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        return transcriptions, detected_language\n",
    "\n",
    "    def transcribe_whole_audio(self, language: Optional[str] = None, use_vad: bool = False, vad_method: Optional[str] = None, verbose: bool = False, plot_word_alignment: bool = False, detect_disfluencies: bool = False) -> Tuple[Dict, Optional[str]]:\n",
    "        \"\"\"Transcribes the entire audio file without diarization.\"\"\"\n",
    "        logging.info(f\"Transcribing the entire audio file: {self.audio_file}\")\n",
    "        vad_option = None\n",
    "        if use_vad:\n",
    "            vad_option = vad_method if vad_method else True\n",
    "        return self.transcribe_chunk(self.audio_file, language=language, vad=vad_option, verbose=verbose, plot_word_alignment=plot_word_alignment, detect_disfluencies=detect_disfluencies)\n",
    "\n",
    "    def clean_transcription(self, transcriptions_json: str, speaker_names: Dict[str, str]) -> List[str]:\n",
    "        \"\"\"Cleans the transcription JSON to a readable format (for diarized audio).\"\"\"\n",
    "        with open(transcriptions_json, 'r') as f:\n",
    "            transcriptions = json.load(f)\n",
    "\n",
    "        cleaned = []\n",
    "        detected_language = transcriptions[0].get(\"language\") if transcriptions else None\n",
    "        if detected_language:\n",
    "            cleaned.append(f\"Detected Language: {detected_language.upper()}\")\n",
    "            cleaned.append(\"\")\n",
    "        cleaned.append(f\"Audio File: {self.audio_base_name}\")\n",
    "        cleaned.append(\"\")\n",
    "\n",
    "        current_speaker, current_text, current_start, current_end = None, \"\", None, None\n",
    "        for i, chunk in enumerate(transcriptions):\n",
    "            if current_speaker != chunk[\"speaker\"]:\n",
    "                if current_speaker is not None:\n",
    "                    cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "                current_speaker, current_text = chunk[\"speaker\"], \"\"\n",
    "                current_start, current_end = chunk[\"start_time\"], chunk[\"end_time\"]\n",
    "                if i > 0 and transcriptions[i][\"speaker\"] != transcriptions[i-1][\"speaker\"]:\n",
    "                    cleaned.append(\"\") # Add a blank line before a new speaker (after the first)\n",
    "            if chunk[\"transcription\"] and chunk[\"transcription\"][\"segments\"]:\n",
    "                current_text += \" \".join(seg[\"text\"] for seg in chunk[\"transcription\"][\"segments\"])\n",
    "        if current_speaker:\n",
    "            cleaned.append(f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "        return [line for line in cleaned if line.strip() != \"\"]\n",
    "\n",
    "    def clean_whole_transcription(self, whole_transcription: Dict, language: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"Cleans the whole transcription output to a readable format (without diarization).\"\"\"\n",
    "        cleaned = []\n",
    "        detected_language = whole_transcription.get(\"language\") if whole_transcription else language\n",
    "        if detected_language:\n",
    "            cleaned.append(f\"Detected Language: {detected_language.upper()}\")\n",
    "            cleaned.append(\"\")\n",
    "        cleaned.append(f\"Audio File: {self.audio_base_name}\")\n",
    "        cleaned.append(\"\")\n",
    "        if whole_transcription and whole_transcription.get(\"segments\"):\n",
    "            for segment in whole_transcription[\"segments\"]:\n",
    "                cleaned.append(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "        return cleaned\n",
    "\n",
    "    def save_transcription_to_file(self, cleaned_transcriptions: List[str], filename=\"transcription.txt\"):\n",
    "        \"\"\"Saves the cleaned transcription to a text file.\"\"\"\n",
    "        output_file = os.path.join(self.output_dir, f\"{self.audio_base_name}_{filename}\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(cleaned_transcriptions))\n",
    "\n",
    "    def process_audio(self, language: Optional[str] = None, min_speakers: Optional[int] = None, max_speakers: Optional[int] = None):\n",
    "        \"\"\"Orchestrates the audio processing pipeline.\"\"\"\n",
    "        try:\n",
    "            use_vad = input(\"Do you want to remove silent parts before transcription? (yes/no): \").lower() == \"yes\"\n",
    "            vad_method = None\n",
    "            if use_vad:\n",
    "                vad_choice = input(\"Choose VAD method (silero, silero:3.1, auditok, or leave empty for default silero): \").lower().strip()\n",
    "                if vad_choice in [\"silero\", \"silero:3.1\", \"auditok\"]:\n",
    "                    vad_method = vad_choice\n",
    "                elif vad_choice:\n",
    "                    print(f\"Invalid VAD method '{vad_choice}'. Using default silero.\")\n",
    "\n",
    "            verbose_input = input(\"Enable verbose output for Whisper? (yes/no): \").lower() == \"yes\"\n",
    "            plot_alignment_input = input(\"Enable plotting word alignment (if supported by model)? (yes/no): \").lower() == \"yes\"\n",
    "            detect_disfluencies_input = input(\"Enable disfluency detection (if supported by model)? (yes/no): \").lower() == \"yes\"\n",
    "\n",
    "            if self.skip_diarization:\n",
    "                print(\"Skipping diarization and transcribing the whole audio file.\")\n",
    "                whole_transcription, detected_language = self.transcribe_whole_audio(language=language, use_vad=use_vad, vad_method=vad_method, verbose=verbose_input, plot_word_alignment=plot_alignment_input, detect_disfluencies=detect_disfluencies_input)\n",
    "                if whole_transcription:\n",
    "                    cleaned_transcriptions = self.clean_whole_transcription(whole_transcription, detected_language)\n",
    "                    print(\"\\nTranscription:\")\n",
    "                    for line in cleaned_transcriptions:\n",
    "                        print(line)\n",
    "                    self.save_transcription_to_file(cleaned_transcriptions, filename=\"whole_transcription.txt\")\n",
    "                else:\n",
    "                    print(\"Transcription failed.\")\n",
    "            else:\n",
    "                pipeline, diarization_result, speaker_names = self.run_diarization(max_speakers=max_speakers, min_speakers=min_speakers)\n",
    "                chunk_info_list = self.chunk_audio(diarization_result)\n",
    "                print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "                num_speakers = len(speaker_names)\n",
    "                print(f\"\\nDetected {num_speakers} speakers:\")\n",
    "                for i, (turn, _, speaker) in enumerate(diarization_result.itertracks(yield_label=True), 1):\n",
    "                    print(f\"Chunk {i}: Speaker '{speaker}' [{turn.start:.2f} - {turn.end:.2f}]\")\n",
    "\n",
    "                rename_speakers = input(\"\\nDo you want to rename the speakers? (yes/no): \").lower()\n",
    "                if rename_speakers == \"yes\":\n",
    "                    new_speaker_names = {}\n",
    "                    for label in sorted(speaker_names.keys()):\n",
    "                        new_name = input(f\"Enter a new name for '{speaker_names[label]}' (default: {speaker_names[label]}): \").strip()\n",
    "                        if new_name:\n",
    "                            new_speaker_names[label] = new_name\n",
    "                        else:\n",
    "                            new_speaker_names[label] = speaker_names[label]\n",
    "                    speaker_names.update(new_speaker_names)\n",
    "\n",
    "                speaker_names_path = os.path.join(self.output_dir, f\"{self.audio_base_name}_speaker_names.json\")\n",
    "                with open(speaker_names_path, \"w\") as f:\n",
    "                    json.dump(speaker_names, f, indent=4)\n",
    "                print(f\"Speaker names saved to {speaker_names_path}\")\n",
    "\n",
    "                transcriptions, detected_language = self.process_and_transcribe_chunks(chunk_info_list, language=language, use_vad=use_vad, vad_method=vad_method, verbose=verbose_input, plot_word_alignment=plot_alignment_input, detect_disfluencies=detect_disfluencies_input)\n",
    "\n",
    "                transcriptions_json_path = os.path.join(self.output_dir, f\"{self.audio_base_name}_transcriptions.json\")\n",
    "                cleaned_transcriptions = self.clean_transcription(transcriptions_json_path, speaker_names)\n",
    "                print(\"Cleaned Transcriptions:\")\n",
    "                for paragraph in cleaned_transcriptions:\n",
    "                    print(paragraph)\n",
    "\n",
    "                self.save_transcription_to_file(cleaned_transcriptions, filename=\"diarized_transcription.txt\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred: {e}\")\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "def process_single_audio(audio_file_path, output_dir=\"chunks\", hf_token=None, skip_diarization=False, whisper_model=\"base\", language=None, min_speakers=None, max_speakers=None, verbose=False, plot_word_alignment=False, detect_disfluencies=False):\n",
    "    transcriber = AudioTranscriber(audio_file_path, output_dir, hf_token, skip_diarization=skip_diarization, whisper_model=whisper_model)\n",
    "    transcriber.process_audio(language=language, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_dir_base = input(\"Enter the main output directory (default: output): \").strip() or \"output\"\n",
    "\n",
    "    # Load HF_TOKEN if config.py exists\n",
    "    hf_token = None\n",
    "    try:\n",
    "        from config import HF_TOKEN as config_token\n",
    "        hf_token = config_token\n",
    "    except ImportError:\n",
    "        logging.warning(\n",
    "            \"config.py not found or HF_TOKEN not defined. Diarization may not work.\"\n",
    "            \" Please create a config.py file with HF_TOKEN = 'YOUR_HUGGINGFACE_TOKEN'\"\n",
    "        )\n",
    "\n",
    "    audio_input = input(\"Enter the path to an audio file or a directory containing audio files: \").strip()\n",
    "\n",
    "    if os.path.isdir(audio_input):\n",
    "        audio_files = [f for f in os.listdir(audio_input) if f.endswith(('.mp3', '.wav', '.aac', '.m4a'))]\n",
    "        if not audio_files:\n",
    "            print(\"No audio files found in the specified directory.\")\n",
    "        else:\n",
    "            print(\"\\nAvailable audio files:\")\n",
    "            for i, filename in enumerate(audio_files):\n",
    "                print(f\"{i + 1}. {filename}\")\n",
    "\n",
    "            while True:\n",
    "                selection = input(\"\\nChoose files to process (e.g., 'all', '1', '2,3,4', '1-3'): \").lower().strip()\n",
    "                files_to_process = []\n",
    "\n",
    "                if selection == 'all':\n",
    "                    files_to_process = [os.path.join(audio_input, f) for f in audio_files]\n",
    "                    break\n",
    "                elif ',' in selection:\n",
    "                    indices = [s.strip() for s in selection.split(',')]\n",
    "                    valid_indices = True\n",
    "                    selected_indices = set()\n",
    "                    for index_str in indices:\n",
    "                        if index_str.isdigit():\n",
    "                            index = int(index_str)\n",
    "                            if 1 <= index <= len(audio_files):\n",
    "                                selected_indices.add(index - 1)\n",
    "                            else:\n",
    "                                print(f\"Invalid file number: {index_str}\")\n",
    "                                valid_indices = False\n",
    "                                break\n",
    "                        else:\n",
    "                            print(f\"Invalid input: {index_str}\")\n",
    "                            valid_indices = False\n",
    "                            break\n",
    "                    if valid_indices:\n",
    "                        files_to_process = [os.path.join(audio_input, audio_files[i]) for i in sorted(list(selected_indices))]\n",
    "                        break\n",
    "                elif '-' in selection:\n",
    "                    try:\n",
    "                        start_str, end_str = selection.split('-')\n",
    "                        start_index = int(start_str.strip())\n",
    "                        end_index = int(end_str.strip())\n",
    "                        if 1 <= start_index <= len(audio_files) and 1 <= end_index <= len(audio_files) and start_index <= end_index:\n",
    "                            files_to_process = [os.path.join(audio_input, audio_files[i]) for i in range(start_index - 1, end_index)]\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"Invalid range of file numbers.\")\n",
    "                    except ValueError:\n",
    "                        print(\"Invalid range format.\")\n",
    "                elif selection.isdigit():\n",
    "                    index = int(selection)\n",
    "                    if 1 <= index <= len(audio_files):\n",
    "                        files_to_process = [os.path.join(audio_input, audio_files[index - 1])]\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Invalid file number.\")\n",
    "                else:\n",
    "                    print(\"Invalid selection format. Please use 'all', a single number, comma-separated numbers, or a range (e.g., '1-3').\")\n",
    "\n",
    "            for audio_file_path in files_to_process:\n",
    "                print(f\"\\n--- Processing: {audio_file_path} ---\")\n",
    "                # Ask for processing options for each file\n",
    "                skip_diarization_input = input(\"Skip speaker diarization for this file? (yes/no): \").lower()\n",
    "                skip_diarization = skip_diarization_input == \"yes\"\n",
    "\n",
    "                whisper_model_choice = \"base\"\n",
    "                min_speakers = None #set default values\n",
    "                max_speakers = None\n",
    "                if not skip_diarization:\n",
    "                    specify_speakers = input(\"Specify min/max speakers for this file? (yes/no): \").lower()\n",
    "\n",
    "                    if specify_speakers == \"yes\":\n",
    "                        try:\n",
    "                            min_speakers = int(input(\"Enter the minimum number of speakers (optional): \") or None)\n",
    "                            max_speakers = int(input(\"Enter the maximum number of speakers (optional): \") or None)\n",
    "                        except ValueError:\n",
    "                            print(\"Invalid input for the number of speakers.\")\n",
    "\n",
    "                    print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "                    chosen_model = input(\"Choose a Whisper model (default: base): \").lower().strip()\n",
    "                    if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "                        whisper_model_choice = chosen_model\n",
    "                    elif chosen_model:\n",
    "                        print(f\"Invalid model '{chosen_model}'. Using default 'base'.\")\n",
    "                else:\n",
    "                    print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "                    chosen_model = input(\"Choose a Whisper model (default: base): \").lower().strip()\n",
    "                    if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "                        whisper_model_choice = chosen_model\n",
    "                    elif chosen_model:\n",
    "                        print(f\"Invalid model '{chosen_model}'. Using default 'base'.\")\n",
    "\n",
    "                specify_language = input(\"Specify a language for this file? (yes/no): \").lower()\n",
    "                transcription_language = None\n",
    "                if specify_language == \"yes\":\n",
    "                    transcription_language = input(\"Enter the language code (e.g., en, fr, es): \").strip()\n",
    "\n",
    "                process_single_audio(audio_file_path, os.path.join(output_dir_base, \"chunks\"), hf_token, skip_diarization, whisper_model_choice, transcription_language, min_speakers, max_speakers)\n",
    "\n",
    "    elif os.path.isfile(audio_input):\n",
    "        audio_file = audio_input\n",
    "        # Ask for processing options for the single file\n",
    "        skip_diarization_input = input(\"Skip speaker diarization? (yes/no): \").lower()\n",
    "        skip_diarization = skip_diarization_input == \"yes\"\n",
    "\n",
    "        whisper_model_choice = \"base\"\n",
    "        min_speakers = None #set default values\n",
    "        max_speakers = None\n",
    "        if not skip_diarization:\n",
    "            specify_speakers = input(\"Specify min/max speakers? (yes/no): \").lower()\n",
    "            if specify_speakers == \"yes\":\n",
    "                try:\n",
    "                    min_speakers = int(input(\"Enter the minimum number of speakers (optional): \") or None)\n",
    "                    max_speakers = int(input(\"Enter the maximum number of speakers (optional): \") or None)\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input for the number of speakers.\")\n",
    "\n",
    "            print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "            chosen_model = input(\"Choose a Whisper model (default: base): \").lower().strip()\n",
    "            if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "                whisper_model_choice = chosen_model\n",
    "            elif chosen_model:\n",
    "                print(f\"Invalid model '{chosen_model}'. Using default 'base'.\")\n",
    "        else:\n",
    "            print(\"\\nAvailable Whisper models: tiny, base, small, medium, large\")\n",
    "            chosen_model = input(\"Choose a Whisper model (default: base): \").lower().strip()\n",
    "            if chosen_model in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]:\n",
    "                whisper_model_choice = chosen_model\n",
    "            elif chosen_model:\n",
    "                print(f\"Invalid model '{chosen_model}'. Using default 'base'.\")\n",
    "\n",
    "        specify_language = input(\"Specify a language for this file? (yes/no): \").lower()\n",
    "        transcription_language = None\n",
    "        if specify_language == \"yes\":\n",
    "            transcription_language = input(\"Enter the language code (e.g., en, fr, es): \").strip()\n",
    "\n",
    "        process_single_audio(audio_file, os.path.join(output_dir_base, \"chunks\"), hf_token, skip_diarization, whisper_model_choice, transcription_language, min_speakers, max_speakers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--output_dir OUTPUT_DIR]\n",
      "                             [--hf_token HF_TOKEN] [--skip_diarization]\n",
      "                             [--whisper_model {tiny,base,small,medium,large}]\n",
      "                             [--language LANGUAGE]\n",
      "                             [--min_speakers MIN_SPEAKERS]\n",
      "                             [--max_speakers MAX_SPEAKERS] [--use_vad]\n",
      "                             [--vad_method {silero,silero:3.1,auditok}]\n",
      "                             [--verbose] [--plot_word_alignment]\n",
      "                             [--detect_disfluencies] [--no_rename_speakers]\n",
      "                             audio_input\n",
      "ipykernel_launcher.py: error: the following arguments are required: audio_input\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Annotation\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import whisper_timestamped as whisper\n",
    "import argparse\n",
    "\n",
    "class AudioTranscriber:\n",
    "    def __init__(self, audio_file: str, output_dir: str = \"chunks\", hf_token: Optional[str] = None, skip_diarization: bool = False, whisper_model: str = \"base\"):\n",
    "        self.audio_file = audio_file\n",
    "        self.audio_base_name = os.path.splitext(os.path.basename(self.audio_file))[0]\n",
    "        self.output_dir_base = output_dir  # Base output directory\n",
    "        self.output_dir = os.path.join(self.output_dir_base, self.audio_base_name)  # final output dir\n",
    "        self.hf_token = hf_token\n",
    "        self.whisper_model = whisper_model\n",
    "        self.debug_mode = False\n",
    "        self.skip_diarization = skip_diarization\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def load_audio(self, audio_file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Loads an audio file using torchaudio.\"\"\"\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "            return waveform, sample_rate\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading audio file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_diarization(\n",
    "        self,\n",
    "        max_speakers: Optional[int] = None,\n",
    "        min_speakers: Optional[int] = None,\n",
    "    ) -> Tuple[Pipeline, Annotation, Dict[str, str]]:\n",
    "        \"\"\"Performs speaker diarization.\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(f\"Using device: {device}\")\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\", use_auth_token=self.hf_token\n",
    "        ).to(device)\n",
    "        waveform, sample_rate = self.load_audio(self.audio_file)\n",
    "        input_data = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "        if max_speakers:\n",
    "            input_data[\"max_speakers\"] = max_speakers\n",
    "        if min_speakers:\n",
    "            input_data[\"min_speakers\"] = min_speakers\n",
    "        diarization = pipeline(input_data)\n",
    "        speaker_labels = set()\n",
    "        for segment, _, label in diarization.itertracks(yield_label=True):\n",
    "            speaker_labels.add(label)\n",
    "        speaker_names = {\n",
    "            label: f\"Speaker {i + 1}\" for i, label in enumerate(sorted(speaker_labels))\n",
    "        }\n",
    "        return pipeline, diarization, speaker_names\n",
    "\n",
    "    def chunk_audio(self, diarization: Annotation) -> List[Dict]:\n",
    "        \"\"\"Chunks audio based on diarization.\"\"\"\n",
    "        audio = AudioSegment.from_file(self.audio_file)\n",
    "        chunks = []\n",
    "        for i, (turn, _, speaker) in enumerate(diarization.itertracks(yield_label=True), 1):\n",
    "            start_ms, end_ms = int(turn.start * 1000), int(turn.end * 1000)\n",
    "            chunk_path = os.path.join(self.output_dir, f\"chunk_{i}.mp3\")\n",
    "            audio[start_ms:end_ms].export(chunk_path, format=\"mp3\")\n",
    "            chunks.append({\"file_path\": chunk_path, \"speaker\": speaker, \"start_time\": turn.start, \"end_time\": turn.end})\n",
    "        return chunks\n",
    "\n",
    "    def transcribe_chunk(self, audio_file_path: str, language: Optional[str] = None,\n",
    "                         vad: Optional[bool or str or List[Tuple[float, float]]] = None, verbose: bool = False,\n",
    "                         plot_word_alignment: bool = False,\n",
    "                         detect_disfluencies: bool = False) -> Tuple[Dict, Optional[str]]:\n",
    "        \"\"\"Transcribes an audio chunk and returns the transcription and detected language.\"\"\"\n",
    "        try:\n",
    "            model = whisper.load_model(self.whisper_model)\n",
    "            audio = whisper.load_audio(audio_file_path)\n",
    "\n",
    "            if vad is not None and vad is not False:\n",
    "                logging.info(f\"Performing voice activity detection with settings: {vad}\")\n",
    "                if vad is True or vad == \"silero\":\n",
    "                    result = whisper.transcribe(model, audio, language=language, vad=\"silero\", verbose=verbose,\n",
    "                                                plot_word_alignment=plot_word_alignment,\n",
    "                                                detect_disfluencies=detect_disfluencies)\n",
    "                elif vad == \"silero:3.1\":\n",
    "                    result = whisper.transcribe(model, audio, language=language, vad=\"silero:3.1\", verbose=verbose,\n",
    "                                                plot_word_alignment=plot_word_alignment,\n",
    "                                                detect_disfluencies=detect_disfluencies)\n",
    "                elif vad == \"auditok\":\n",
    "                    result = whisper.transcribe(model, audio, language=language, vad=\"auditok\", verbose=verbose,\n",
    "                                                plot_word_alignment=plot_word_alignment,\n",
    "                                                detect_disfluencies=detect_disfluencies)\n",
    "                elif isinstance(vad, list):\n",
    "                    speech_segments = []\n",
    "                    for start, end in vad:\n",
    "                        speech_segments.append(\n",
    "                            audio[int(start * whisper.audio.SAMPLE_RATE):int(end * whisper.audio.SAMPLE_RATE)])\n",
    "                    if speech_segments:\n",
    "                        full_transcription = {\"segments\": \"\"}\n",
    "                        for segment in speech_segments:\n",
    "                            segment_result = whisper.transcribe(model, segment, language=language, verbose=verbose,\n",
    "                                                                plot_word_alignment=plot_word_alignment,\n",
    "                                                                detect_disfluencies=detect_disfluencies)\n",
    "                            full_transcription[\"segments\"].extend(segment_result.get(\"segments\",))\n",
    "                        result = full_transcription\n",
    "                else:\n",
    "                    logging.warning(f\"Invalid VAD setting: {vad}. Transcribing without VAD.\")\n",
    "                    result = whisper.transcribe(model, audio, language=language, verbose=verbose,\n",
    "                                                plot_word_alignment=plot_word_alignment,\n",
    "                                                detect_disfluencies=detect_disfluencies)\n",
    "            else:\n",
    "                result = whisper.transcribe(model, audio, language=language, verbose=verbose,\n",
    "                                            plot_word_alignment=plot_word_alignment,\n",
    "                                            detect_disfluencies=detect_disfluencies)\n",
    "\n",
    "            detected_language = result.get(\"language\") if language is None else language\n",
    "            return result, detected_language\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Transcription error: {e}\")\n",
    "            return {}, None\n",
    "\n",
    "    def process_and_transcribe_chunks(self, chunks: List[Dict], language: Optional[str] = None, use_vad: bool = False,\n",
    "                                      vad_method: Optional[str] = None, verbose: bool = False,\n",
    "                                      plot_word_alignment: bool = False,\n",
    "                                      detect_disfluencies: bool = False) -> List[Dict]:\n",
    "        \"\"\"Processes and saves transcriptions for individual chunks.\"\"\"\n",
    "        transcriptions = []\n",
    "        detected_language = None\n",
    "        for chunk in chunks:\n",
    "            logging.info(f\"Transcribing {chunk['file_path']}\")\n",
    "            vad_option = None\n",
    "            if use_vad:\n",
    "                vad_option = vad_method if vad_method else True  # Use default if no method specified\n",
    "            transcription, lang = self.transcribe_chunk(chunk[\"file_path\"], language=language, vad=vad_option,\n",
    "                                                        verbose=verbose, plot_word_alignment=plot_word_alignment,\n",
    "                                                        detect_disfluencies=detect_disfluencies)\n",
    "            if transcription:\n",
    "                if detected_language is None and lang is not None:\n",
    "                    detected_language = lang\n",
    "                transcriptions.append({**chunk, \"transcription\": transcription, \"language\": lang})\n",
    "                if self.debug_mode:  # only print if debug mode is on\n",
    "                    print(f\"Transcription for {chunk['file_path']} (Language: {lang}):\")\n",
    "                    for segment in transcription[\"segments\"]:\n",
    "                        print(f\"    [{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "        transcriptions_json_path = os.path.join(self.output_dir, f\"{self.audio_base_name}_transcriptions.json\")\n",
    "        with open(transcriptions_json_path, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=4)\n",
    "        return transcriptions, detected_language\n",
    "\n",
    "    def transcribe_whole_audio(self, language: Optional[str] = None, use_vad: bool = False,\n",
    "                                vad_method: Optional[str] = None, verbose: bool = False,\n",
    "                                plot_word_alignment: bool = False,\n",
    "                                detect_disfluencies: bool = False) -> Tuple[Dict, Optional[str]]:\n",
    "        \"\"\"Transcribes the entire audio file without diarization.\"\"\"\n",
    "        logging.info(f\"Transcribing the entire audio file: {self.audio_file}\")\n",
    "        vad_option = None\n",
    "        if use_vad:\n",
    "            vad_option = vad_method if vad_method else True\n",
    "        return self.transcribe_chunk(self.audio_file, language=language, vad=vad_option, verbose=verbose,\n",
    "                                    plot_word_alignment=plot_word_alignment,\n",
    "                                    detect_disfluencies=detect_disfluencies)\n",
    "\n",
    "    def clean_transcription(self, transcriptions_json: str, speaker_names: Dict[str, str]) -> List[str]:\n",
    "        \"\"\"Cleans the transcription JSON to a readable format (for diarized audio).\"\"\"\n",
    "        with open(transcriptions_json, 'r') as f:\n",
    "            transcriptions = json.load(f)\n",
    "\n",
    "        cleaned = []\n",
    "        detected_language = transcriptions[0].get(\"language\") if transcriptions else None\n",
    "        if detected_language:\n",
    "            cleaned.append(f\"Detected Language: {detected_language.upper()}\")\n",
    "            cleaned.append(\"\")\n",
    "        cleaned.append(f\"Audio File: {self.audio_base_name}\")\n",
    "        cleaned.append(\"\")\n",
    "\n",
    "        current_speaker, current_text, current_start, current_end = None, \"\", None, None\n",
    "        for i, chunk in enumerate(transcriptions):\n",
    "            if current_speaker != chunk[\"speaker\"]:\n",
    "                if current_speaker is not None:\n",
    "                    cleaned.append(\n",
    "                        f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "                current_speaker, current_text = chunk[\"speaker\"], \"\"\n",
    "                current_start, current_end = chunk[\"start_time\"], chunk[\"end_time\"]\n",
    "                if i > 0 and transcriptions[i][\"speaker\"] != transcriptions[i - 1][\"speaker\"]:\n",
    "                    cleaned.append(\"\")  # Add a blank line before a new speaker (after the first)\n",
    "            if chunk[\"transcription\"] and chunk[\"transcription\"][\"segments\"]:\n",
    "                current_text += \" \".join(seg[\"text\"] for seg in chunk[\"transcription\"][\"segments\"])\n",
    "        if current_speaker:\n",
    "            cleaned.append(\n",
    "                f\"{speaker_names.get(current_speaker, current_speaker)} [{current_start:.2f} - {current_end:.2f}]: {current_text}\")\n",
    "        return [line for line in cleaned if line.strip() != \"\"]\n",
    "\n",
    "    def clean_whole_transcription(self, whole_transcription: Dict, language: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"Cleans the whole transcription output to a readable format (without diarization).\"\"\"\n",
    "        cleaned = []\n",
    "        detected_language = whole_transcription.get(\"language\") if whole_transcription else language\n",
    "        if detected_language:\n",
    "            cleaned.append(f\"Detected Language: {detected_language.upper()}\")\n",
    "            cleaned.append(\"\")\n",
    "        cleaned.append(f\"Audio File: {self.audio_base_name}\")\n",
    "        cleaned.append(\"\")\n",
    "        if whole_transcription and whole_transcription.get(\"segments\"):\n",
    "            for segment in whole_transcription[\"segments\"]:\n",
    "                cleaned.append(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "        return cleaned\n",
    "\n",
    "    def save_transcription_to_file(self, cleaned_transcriptions: List[str], filename=\"transcription.txt\"):\n",
    "        \"\"\"Saves the cleaned transcription to a text file.\"\"\"\n",
    "        output_file = os.path.join(self.output_dir, f\"{self.audio_base_name}_{filename}\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(cleaned_transcriptions))\n",
    "\n",
    "    def process_audio(self, language: Optional[str] = None, min_speakers: Optional[int] = None,\n",
    "                      max_speakers: Optional[int] = None, use_vad: bool = False,\n",
    "                      vad_method: Optional[str] = None, verbose: bool = False,\n",
    "                      plot_word_alignment: bool = False,\n",
    "                      detect_disfluencies: bool = False, no_rename_speakers: bool = False):\n",
    "        \"\"\"Orchestrates the audio processing pipeline.\"\"\"\n",
    "        try:\n",
    "            if self.skip_diarization:\n",
    "                print(\"Skipping diarization and transcribing the whole audio file.\")\n",
    "                whole_transcription, detected_language = self.transcribe_whole_audio(language=language, use_vad=use_vad,\n",
    "                                                                                    vad_method=vad_method, verbose=verbose,\n",
    "                                                                                    plot_word_alignment=plot_word_alignment,\n",
    "                                                                                    detect_disfluencies=detect_disfluencies)\n",
    "                if whole_transcription:\n",
    "                    cleaned_transcriptions = self.clean_whole_transcription(whole_transcription, detected_language)\n",
    "                    print(\"\\nTranscription:\")\n",
    "                    for line in cleaned_transcriptions:\n",
    "                        print(line)\n",
    "                    self.save_transcription_to_file(cleaned_transcriptions, filename=\"whole_transcription.txt\")\n",
    "                else:\n",
    "                    print(\"Transcription failed.\")\n",
    "            else:\n",
    "                pipeline, diarization_result, speaker_names = self.run_diarization(max_speakers=max_speakers,\n",
    "                                                                                    min_speakers=min_speakers)\n",
    "                chunk_info_list = self.chunk_audio(diarization_result)\n",
    "                print(\"Audio file diarized and chunked.\")\n",
    "\n",
    "                num_speakers = len(speaker_names)\n",
    "                print(f\"\\nDetected {num_speakers} speakers:\")\n",
    "                for i, (turn, _, speaker) in enumerate(diarization_result.itertracks(yield_label=True), 1):\n",
    "                    print(f\"Chunk {i}: Speaker '{speaker}' [{turn.start:.2f} - {turn.end:.2f}]\")\n",
    "\n",
    "                # Speaker renaming moved here\n",
    "                if not no_rename_speakers:\n",
    "                    new_speaker_names = {}\n",
    "                    for label in sorted(speaker_names.keys()):\n",
    "                        new_name = input(\n",
    "                            f\"Enter a new name for '{speaker_names[label]}' (default: {speaker_names[label]}): \").strip()\n",
    "                        if new_name:\n",
    "                            new_speaker_names[label] = new_name\n",
    "                        else:\n",
    "                            new_speaker_names[label] = speaker_names[label]\n",
    "                    speaker_names.update(new_speaker_names)\n",
    "\n",
    "                    speaker_names_path = os.path.join(self.output_dir, f\"{self.audio_base_name}_speaker_names.json\")\n",
    "                    with open(speaker_names_path, \"w\") as f:\n",
    "                        json.dump(speaker_names, f, indent=4)\n",
    "                    print(f\"Speaker names saved to {speaker_names_path}\")\n",
    "\n",
    "                transcriptions, detected_language = self.process_and_transcribe_chunks(chunk_info_list, language=language,\n",
    "                                                                                        use_vad=use_vad,\n",
    "                                                                                        vad_method=vad_method, verbose=verbose,\n",
    "                                                                                        plot_word_alignment=plot_word_alignment,\n",
    "                                                                                        detect_disfluencies=detect_disfluencies)\n",
    "\n",
    "                transcriptions_json_path = os.path.join(self.output_dir,\n",
    "                                                        f\"{self.audio_base_name}_transcriptions.json\")\n",
    "                cleaned_transcriptions = self.clean_transcription(transcriptions_json_path, speaker_names)\n",
    "                print(\"Cleaned Transcriptions:\")\n",
    "                for paragraph in cleaned_transcriptions:\n",
    "                    print(paragraph)\n",
    "\n",
    "                self.save_transcription_to_file(cleaned_transcriptions, filename=\"diarized_transcription.txt\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred: {e}\")\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "def process_single_audio(audio_file_path, output_dir=\"chunks\", hf_token=None, skip_diarization=False,\n",
    "                       whisper_model=\"base\", language=None, min_speakers=None, max_speakers=None,\n",
    "                       use_vad=False, vad_method=None, verbose=False, plot_word_alignment=False,\n",
    "                       detect_disfluencies=False, no_rename_speakers=False):\n",
    "    transcriber = AudioTranscriber(audio_file_path, output_dir, hf_token, skip_diarization=skip_diarization,\n",
    "                                    whisper_model=whisper_model)\n",
    "    transcriber.process_audio(language=language, min_speakers=min_speakers, max_speakers=max_speakers,\n",
    "                                use_vad=use_vad, vad_method=vad_method, verbose=verbose,\n",
    "                                plot_word_alignment=plot_word_alignment,\n",
    "                                detect_disfluencies=detect_disfluencies,\n",
    "                                no_rename_speakers=no_rename_speakers)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Transcribe audio files with speaker diarization and/or VAD.\")\n",
    "    parser.add_argument(\"audio_input\",\n",
    "                        help=\"Path to an audio file or a directory containing audio files.  If a directory is provided, the script will process all audio files in that directory.\")\n",
    "    parser.add_argument(\"--output_dir\", default=\"output\",\n",
    "                        help=\"The main directory where all output files will be saved.  Defaults to 'output'.  If the input is a directory, a subdirectory with the name of each audio file will be created inside this directory.\")\n",
    "    parser.add_argument(\"--hf_token\", default=None,\n",
    "                        help=\"Hugging Face API token.  Required for speaker diarization.  If not provided, the script will attempt to read it from a config.py file.\")\n",
    "    parser.add_argument(\"--skip_diarization\", action=\"store_true\",\n",
    "                        help=\"Skip speaker diarization and transcribe the entire audio file as a single speaker.\")\n",
    "    parser.add_argument(\"--whisper_model\", default=\"base\", choices=[\"tiny\", \"base\", \"small\", \"medium\", \"large\"],\n",
    "                        help=\"Choose a Whisper model size.  Defaults to 'base'.\")\n",
    "    parser.add_argument(\"--language\", default=None,\n",
    "                        help=\"Specify the language of the audio file (e.g., 'en', 'fr', 'es').  If not provided, Whisper will attempt to detect the language.\")\n",
    "    parser.add_argument(\"--min_speakers\", type=int, default=None,\n",
    "                        help=\"Minimum number of speakers expected in the audio. Used for diarization.\")\n",
    "    parser.add_argument(\"--max_speakers\", type=int, default=None,\n",
    "                        help=\"Maximum number of speakers expected in the audio. Used for diarization.\")\n",
    "    parser.add_argument(\"--use_vad\", action=\"store_true\",\n",
    "                        help=\"Enable voice activity detection to remove silent parts before transcription.\")\n",
    "    parser.add_argument(\"--vad_method\", default=None, choices=[\"silero\", \"silero:3.1\", \"auditok\"],\n",
    "                        help=\"Choose a VAD method: 'silero',  'silero:3.1', or 'auditok'.  If --use_vad is set and this is not provided, 'silero' is used as default.\")\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\",\n",
    "                        help=\"Enable verbose output for Whisper.\")\n",
    "    parser.add_argument(\"--plot_word_alignment\", action=\"store_true\",\n",
    "                        help=\"Enable plotting word alignment (if supported by the model).\")\n",
    "    parser.add_argument(\"--detect_disfluencies\", action=\"store_true\",\n",
    "                        help=\"Enable disfluency detection (if supported by the model).\")\n",
    "    parser.add_argument(\"--no_rename_speakers\", action=\"store_true\",\n",
    "                        help=\"Disable the interactive prompt to rename speakers.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    output_dir_base = args.output_dir\n",
    "\n",
    "    # Load HF_TOKEN\n",
    "    hf_token = args.hf_token\n",
    "    if hf_token is None:\n",
    "        try:\n",
    "            from config import HF_TOKEN as config_token\n",
    "            hf_token = config_token\n",
    "        except ImportError:\n",
    "            logging.warning(\n",
    "                \"config.py not found or HF_TOKEN not defined. Diarization may not work.\"\n",
    "                \" Please create a config.py file with HF_TOKEN = 'YOUR_HUGGINGFACE_TOKEN'\"\n",
    "            )\n",
    "\n",
    "    audio_input = args.audio_input\n",
    "\n",
    "    if os.path.isdir(audio_input):\n",
    "        audio_files = [f for f in os.listdir(audio_input) if f.endswith(('.mp3', '.wav', '.aac', '.m4a'))]\n",
    "        if not audio_files:\n",
    "            print(\"No audio files found in the specified directory.\")\n",
    "        else:\n",
    "            print(\"\\nAvailable audio files:\")\n",
    "            for i, filename in enumerate(audio_files):\n",
    "                print(f\"{i + 1}. {filename}\")\n",
    "            files_to_process = [os.path.join(audio_input, f) for f in audio_files]\n",
    "\n",
    "            for audio_file_path in files_to_process:\n",
    "                print(f\"\\n--- Processing: {audio_file_path} ---\")\n",
    "                process_single_audio(audio_file_path, os.path.join(output_dir_base, \"chunks\"), hf_token,\n",
    "                                     args.skip_diarization, args.whisper_model, args.language,\n",
    "                                     args.min_speakers, args.max_speakers, args.use_vad,\n",
    "                                     args.vad_method, args.verbose, args.plot_word_alignment,\n",
    "                                     args.detect_disfluencies, args.no_rename_speakers)\n",
    "\n",
    "    elif os.path.isfile(audio_input):\n",
    "        audio_file = audio_input\n",
    "        process_single_audio(audio_file, os.path.join(output_dir_base, \"chunks\"), hf_token,\n",
    "                             args.skip_diarization, args.whisper_model, args.language,\n",
    "                             args.min_speakers, args.max_speakers, args.use_vad,\n",
    "                             args.vad_method, args.verbose, args.plot_word_alignment,\n",
    "                             args.detect_disfluencies, args.no_rename_speakers)\n",
    "    else:\n",
    "        print(\"Invalid input path.  Please provide a valid audio file or directory.\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
